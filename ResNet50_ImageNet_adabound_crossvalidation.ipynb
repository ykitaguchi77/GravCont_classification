{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7bac407e479848c5a9c34143b7c302f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c1b4fa39f1a74146b976fedfb51b4b74",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_56a99effc7f1490497f308f2bd8c5cc6",
              "IPY_MODEL_be3daa305c014f789d65d69ce64a5dd9"
            ]
          }
        },
        "c1b4fa39f1a74146b976fedfb51b4b74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56a99effc7f1490497f308f2bd8c5cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d1db10a93b1a4d3685851e7ad3c1c40c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 77999237,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 77999237,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d54739bc7024bdc93921afc332f21a6"
          }
        },
        "be3daa305c014f789d65d69ce64a5dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_747af3433caa412693b71cab350b8c95",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 74.4M/74.4M [00:02&lt;00:00, 27.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11bc8e113e1047aab07073f34b7cf266"
          }
        },
        "d1db10a93b1a4d3685851e7ad3c1c40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d54739bc7024bdc93921afc332f21a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "747af3433caa412693b71cab350b8c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11bc8e113e1047aab07073f34b7cf266": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/ResNet50_ImageNet_adabound_crossvalidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a4ZBlqPNdU",
        "colab_type": "text"
      },
      "source": [
        "#**GravCont: EfficientNet_b4_ImageNet**\n",
        "ValidationとTestに分けて解析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgM-Y7SVPNkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "5b7345d3-7d4f-4f25-86cd-8a36e5ec82be"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch_optimizer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/48/f670cf4b47c315861d0547f0c2be579cd801304c86e55008492f1acebd01/torch_optimizer-0.0.1a15-py3-none-any.whl (41kB)\n",
            "\r\u001b[K     |███████▉                        | 10kB 20.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 20kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 30kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 40kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.9MB/s \n",
            "\u001b[?25hCollecting pytorch-ranger>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/70/12256257d861bbc3e176130d25be1de085ce7a9e60594064888a950f2154/pytorch_ranger-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.0.1a15\n",
            "Random Seed:  1234\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp36-none-any.whl size=16031 sha256=5078672c9e066faad091cef8299ff8fa78bbd147136f7f97513cd67f07512fc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.0\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzp_09fWPNoU",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sJV06qPNsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(data_dir):\n",
        "    # 入力画像の前処理をするクラス\n",
        "    # 訓練時と推論時で処理が異なる\n",
        "\n",
        "    \"\"\"\n",
        "        画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "        画像のサイズをリサイズし、色を標準化する。\n",
        "        訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        resize : int\n",
        "            リサイズ先の画像の大きさ。\n",
        "        mean : (R, G, B)\n",
        "            各色チャネルの平均値。\n",
        "        std : (R, G, B)\n",
        "            各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    data_dir = data_dir\n",
        "    n_samples = len(data_dir)\n",
        "\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in ['train', 'val']}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                                shuffle=True, num_workers=4)\n",
        "                  for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "    print(class_names)\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_train:\"+str(len(os.listdir(path=data_dir + '/train/'+class_names[k]))))\n",
        "        k+=1\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_val:\"+str(len(os.listdir(path=data_dir + '/val/'+class_names[k]))))\n",
        "        k+=1\n",
        "\n",
        "    print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "    print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "    \n",
        "    return image_datasets, dataloaders, dataset_sizes, class_names, device\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "def getBatch(dataloaders):    \n",
        "    # Get a batch of training data\n",
        "    inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "    # Make a grid from batch\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "    #imshow(out, title=[class_names[x] for x in classes])\n",
        "    return(inputs, classes)\n",
        "\n",
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "def convnet():\n",
        "    model_ft = models.resnet50(pretrained=False)\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    optimizer_ft = optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=0,\n",
        "        amsbound=False,\n",
        "    )\n",
        "    return (model_ft, criterion, optimizer_ft)\n",
        "\n",
        "\n",
        "def training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50):\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=patience, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "    \"\"\"\n",
        "    #probalilityを計算する\n",
        "    pred_prob = torch.topk(nn.Softmax(dim=1)(output), 1)[0]\n",
        "    pred_class = torch.topk(nn.Softmax(dim=1)(output), 1)[1]\n",
        "    if pred_class == 1:\n",
        "        pred_prob = pred_prob\n",
        "    elif pred_class == 0:\n",
        "        pred_prob = 1- pred_prob\n",
        "    return(model_pred, pred_prob)  #class_nameの番号で出力される\n",
        "    \"\"\"\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\"\"\"\n",
        "・True positive (TN)\n",
        "・False positive (FP)\n",
        "・True negative (TN)\n",
        "・False negative (FN)\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "Precision = TP/(FP + TP) ※positive predictive value\n",
        "Recall = TP/(TP + FN)　※sensitivity\n",
        "Specificity = TN/(FP + TN)\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)\n",
        "\"\"\"\n",
        "\n",
        "def evaluation(model_ft, testset_dir):\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #testデータセット内のファイル名を取得\n",
        "    image_path = glob.glob(testset_dir + \"/*/*\")\n",
        "    #random.shuffle(image_path)  #表示順をランダムにする\n",
        "    print('number of images: ' +str(len(image_path)))\n",
        "\n",
        "\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1     \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    return TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob\n",
        "\n",
        "\n",
        "def make_csv(roc_label_list):\n",
        "    #csvのdata tableを作成\n",
        "    pd.set_option('display.max_rows', 800)  # 省略なしで表示\n",
        "    #columns1 = [\"EfficientNet_32\", \"EfficientNet_64\", \"EfficientNet_128\", \"EfficientNet_256\", \"EfficientNet_512\", \"EfficientNet_558\"]\n",
        "    roc_label_list.extend([\"avg\", \"std\"])\n",
        "    index1 = [\"TP\",\"TN\",\"FP\",\"FN\",\"Accuracy\",\"Positive predictive value\",\"sensitity\",\"specificity\",\"F-value\",\"roc_auc\"]\n",
        "    df = pd.DataFrame(index=index1, columns=roc_label_list)\n",
        "    return df\n",
        "\n",
        "def write_csv(df, col, TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc):\n",
        "    df.iloc[0:10, col] = TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc \n",
        "    #print(df)\n",
        "\n",
        "    # CSVとして出力\n",
        "    #df2.to_csv(\"/content/drive/My Drive/Grav_bootcamp/Posttrain_model_eval_result.csv\",encoding=\"shift_jis\")\n",
        "    return df\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == 'cont':\n",
        "                  y_true.append(0)\n",
        "            elif i == 'grav':\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == 'cont':\n",
        "              y_true.append(0)\n",
        "        elif i == 'grav':\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "def calcurate_ave_std(df, fold):\n",
        "    for i in range(5):\n",
        "        df.iloc[i,fold] = df[i,0:5].mean \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGfUwQXv6Jc",
        "colab_type": "text"
      },
      "source": [
        "#**まとめて解析**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObCZwRvYCPTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "74822fe4-dfe2-4bc7-9143-744a9a0c2d08"
      },
      "source": [
        "#create data_dir_list\n",
        "data_dir = '/content/drive/My Drive/gravcont_crossvalidation'\n",
        "fold = len(os.listdir(data_dir))\n",
        "print(fold)\n",
        "\n",
        "data_dir_list = [0]*fold\n",
        "\n",
        "for i in range(fold):\n",
        "    data_dir_list[i] = data_dir + '/' + str(i)\n",
        "    print(data_dir_list[i])\n",
        "\n",
        "#create roc_label_list\n",
        "roc_label_list = [0]*fold\n",
        "roc_label_list = list(range(fold))\n",
        "print(roc_label_list)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "/content/drive/My Drive/gravcont_crossvalidation/0\n",
            "/content/drive/My Drive/gravcont_crossvalidation/1\n",
            "/content/drive/My Drive/gravcont_crossvalidation/2\n",
            "/content/drive/My Drive/gravcont_crossvalidation/3\n",
            "/content/drive/My Drive/gravcont_crossvalidation/4\n",
            "[0, 1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM2VMXltwBs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7bac407e479848c5a9c34143b7c302f7",
            "c1b4fa39f1a74146b976fedfb51b4b74",
            "56a99effc7f1490497f308f2bd8c5cc6",
            "be3daa305c014f789d65d69ce64a5dd9",
            "d1db10a93b1a4d3685851e7ad3c1c40c",
            "3d54739bc7024bdc93921afc332f21a6",
            "747af3433caa412693b71cab350b8c95",
            "11bc8e113e1047aab07073f34b7cf266"
          ]
        },
        "outputId": "645e62d3-2d91-4c2b-d343-5018b9677f44"
      },
      "source": [
        "df = make_csv(roc_label_list)\n",
        "\n",
        "label_list_list, model_pred_prob_list, Y_TRUE, Y_SCORE = [],[],[],[]\n",
        "\n",
        "print(data_dir_list)\n",
        "print(roc_label_list)\n",
        "\n",
        "for i, t in enumerate(zip(data_dir_list, roc_label_list)):\n",
        "\n",
        "    image_datasets, dataloaders, dataset_sizes, class_names, device = pre_process(t[0]) #path\n",
        "    inputs, classes = getBatch(dataloaders)\n",
        "    model_ft, criterion, optimizer_ft = convnet()\n",
        "    training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50)  \n",
        "    TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob = evaluation(model_ft, '/content/drive/My Drive/Grav_bootcamp/Posttrain_250px')\n",
        "    roc_auc, y_true, y_score = calculate_auc(label_list, model_pred_prob)\n",
        "    Y_TRUE.append(y_true)\n",
        "    Y_SCORE.append(y_score)\n",
        "    df = write_csv(df, i,TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, roc_auc) #numberをcsvの行として指定\n",
        "\n",
        "    label_list_list.append(label_list)\n",
        "    model_pred_prob_list.append(model_pred_prob)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#Draw ROC curve\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list))\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "#それぞれの項目の平均を計算しcsvに追記する\n",
        "df.iloc[0:4,fold], df.iloc[9,fold]   = df.mean(axis=1)[0:4], df.mean(axis=1)[9] \n",
        "df.iloc[0:10,fold+1] = df.std(axis=1)[0:10]\n",
        "TP,TN,FP,FN = df.mean(axis=1)[0:4]\n",
        "df.iloc[4:9,fold] = calculateAccuracy (TP, TN, FP, FN)\n",
        "print(df)\n",
        "\n",
        "# 出力名を記入\n",
        "out_name = \"EfficientNet_ImageNet_256\"\n",
        "\n",
        "# CSVとして出力\n",
        "df.to_csv(\"/content/drive/My Drive/Grav_bootcamp/crossvaridation_\" + out_name + \".csv\",encoding=\"shift_jis\")\n",
        "\n",
        "#ROC_curveを保存\n",
        "fig.savefig(\"/content/drive/My Drive/Grav_bootcamp/crossvaridation_\" + out_name +\".png\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/gravcont_crossvalidation/0', '/content/drive/My Drive/gravcont_crossvalidation/1', '/content/drive/My Drive/gravcont_crossvalidation/2', '/content/drive/My Drive/gravcont_crossvalidation/3', '/content/drive/My Drive/gravcont_crossvalidation/4']\n",
            "[0, 1, 2, 3, 4, 'avg', 'std']\n",
            "['cont', 'grav']\n",
            "cont_train:102\n",
            "grav_train:102\n",
            "cont_val:26\n",
            "grav_val:26\n",
            "training data set_total：204\n",
            "validating data set_total：52\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bac407e479848c5a9c34143b7c302f7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=77999237.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch_optimizer/adabound.py:142: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.6241 Acc: 0.6324\n",
            "val Loss: 0.7478 Acc: 0.5577\n",
            "Validation loss decreased (inf --> 0.747780).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.3769 Acc: 0.8480\n",
            "val Loss: 1.1506 Acc: 0.6731\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3833 Acc: 0.8284\n",
            "val Loss: 0.8993 Acc: 0.7115\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.3385 Acc: 0.8284\n",
            "val Loss: 1.2013 Acc: 0.7115\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2498 Acc: 0.8873\n",
            "val Loss: 1.1654 Acc: 0.7308\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.2158 Acc: 0.9265\n",
            "val Loss: 3.1565 Acc: 0.7115\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1981 Acc: 0.9020\n",
            "val Loss: 4.1765 Acc: 0.5962\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1625 Acc: 0.9559\n",
            "val Loss: 1.9360 Acc: 0.7308\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1039 Acc: 0.9657\n",
            "val Loss: 0.8337 Acc: 0.8269\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1570 Acc: 0.9363\n",
            "val Loss: 1.3995 Acc: 0.7500\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.1048 Acc: 0.9755\n",
            "val Loss: 2.3095 Acc: 0.7885\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.1533 Acc: 0.9706\n",
            "val Loss: 1.0031 Acc: 0.8269\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0765 Acc: 0.9853\n",
            "val Loss: 0.7806 Acc: 0.7692\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.2248 Acc: 0.9069\n",
            "val Loss: 0.6158 Acc: 0.7692\n",
            "Validation loss decreased (0.747780 --> 0.615752).  Saving model ...\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.1419 Acc: 0.9804\n",
            "val Loss: 0.5304 Acc: 0.8077\n",
            "Validation loss decreased (0.615752 --> 0.530418).  Saving model ...\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.1151 Acc: 0.9657\n",
            "val Loss: 0.5368 Acc: 0.8269\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0688 Acc: 0.9804\n",
            "val Loss: 0.6174 Acc: 0.8269\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0819 Acc: 0.9755\n",
            "val Loss: 0.9117 Acc: 0.7308\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0968 Acc: 0.9706\n",
            "val Loss: 1.3802 Acc: 0.7308\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0746 Acc: 0.9804\n",
            "val Loss: 1.2905 Acc: 0.7308\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0645 Acc: 0.9755\n",
            "val Loss: 1.2284 Acc: 0.7308\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.0411 Acc: 0.9804\n",
            "val Loss: 2.1774 Acc: 0.7115\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0466 Acc: 0.9755\n",
            "val Loss: 1.5358 Acc: 0.7500\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0321 Acc: 0.9902\n",
            "val Loss: 1.4720 Acc: 0.7500\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.0119 Acc: 1.0000\n",
            "val Loss: 1.3149 Acc: 0.7500\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.0676 Acc: 0.9706\n",
            "val Loss: 1.3525 Acc: 0.7885\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.1396 Acc: 0.9559\n",
            "val Loss: 0.5928 Acc: 0.8462\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.1787 Acc: 0.9216\n",
            "val Loss: 0.4137 Acc: 0.8462\n",
            "Validation loss decreased (0.530418 --> 0.413728).  Saving model ...\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.1421 Acc: 0.9461\n",
            "val Loss: 0.6091 Acc: 0.8077\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.1010 Acc: 0.9510\n",
            "val Loss: 0.5260 Acc: 0.8269\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.1069 Acc: 0.9755\n",
            "val Loss: 0.5207 Acc: 0.8269\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.1011 Acc: 0.9657\n",
            "val Loss: 0.4481 Acc: 0.8462\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.0992 Acc: 0.9755\n",
            "val Loss: 0.5304 Acc: 0.7885\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.0442 Acc: 0.9804\n",
            "val Loss: 0.5787 Acc: 0.8077\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.0094 Acc: 1.0000\n",
            "val Loss: 0.5704 Acc: 0.8077\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.0716 Acc: 0.9853\n",
            "val Loss: 0.5626 Acc: 0.8269\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.0166 Acc: 1.0000\n",
            "val Loss: 0.4690 Acc: 0.8462\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.0234 Acc: 0.9951\n",
            "val Loss: 0.4885 Acc: 0.8269\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.0299 Acc: 0.9853\n",
            "val Loss: 0.6709 Acc: 0.7500\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.0838 Acc: 0.9804\n",
            "val Loss: 0.5785 Acc: 0.8269\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.0279 Acc: 0.9853\n",
            "val Loss: 0.4785 Acc: 0.8269\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.0454 Acc: 0.9853\n",
            "val Loss: 0.4572 Acc: 0.8269\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.0270 Acc: 0.9902\n",
            "val Loss: 0.5459 Acc: 0.8269\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 4m 49s\n",
            "Best val Acc: 0.846154\n",
            "number of images: 108\n",
            "34 20 53 1\n",
            "Accuracy: 0.8055555555555556\n",
            "Precision (positive predictive value): 0.9714285714285714\n",
            "Recall (sensitivity): 0.6296296296296297\n",
            "Specificity: 0.9814814814814815\n",
            "F_value: 0.7640449438202247\n",
            "roc_auc: 0.8662551440329219\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:102\n",
            "grav_train:102\n",
            "cont_val:26\n",
            "grav_val:26\n",
            "training data set_total：204\n",
            "validating data set_total：52\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.5954 Acc: 0.6863\n",
            "val Loss: 0.6171 Acc: 0.5962\n",
            "Validation loss decreased (inf --> 0.617074).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.3989 Acc: 0.8480\n",
            "val Loss: 0.9490 Acc: 0.7500\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.4549 Acc: 0.7794\n",
            "val Loss: 5.1933 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.3059 Acc: 0.8627\n",
            "val Loss: 3.2530 Acc: 0.6538\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.1191 Acc: 0.9608\n",
            "val Loss: 4.7077 Acc: 0.6154\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1653 Acc: 0.9314\n",
            "val Loss: 1.8504 Acc: 0.7500\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.0916 Acc: 0.9755\n",
            "val Loss: 2.3256 Acc: 0.7692\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.2051 Acc: 0.9510\n",
            "val Loss: 1.3964 Acc: 0.6923\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1280 Acc: 0.9608\n",
            "val Loss: 1.3206 Acc: 0.7308\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.2282 Acc: 0.9216\n",
            "val Loss: 0.7426 Acc: 0.6923\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.1308 Acc: 0.9608\n",
            "val Loss: 1.2409 Acc: 0.6923\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0758 Acc: 0.9853\n",
            "val Loss: 1.0896 Acc: 0.7692\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.2292 Acc: 0.9167\n",
            "val Loss: 1.4627 Acc: 0.7115\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.1871 Acc: 0.9314\n",
            "val Loss: 0.7216 Acc: 0.8269\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.1913 Acc: 0.9265\n",
            "val Loss: 0.7059 Acc: 0.8077\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.1045 Acc: 0.9657\n",
            "val Loss: 0.7222 Acc: 0.8269\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 59s\n",
            "Best val Acc: 0.826923\n",
            "number of images: 108\n",
            "40 14 46 8\n",
            "Accuracy: 0.7962962962962963\n",
            "Precision (positive predictive value): 0.8333333333333334\n",
            "Recall (sensitivity): 0.7407407407407407\n",
            "Specificity: 0.8518518518518519\n",
            "F_value: 0.7843137254901961\n",
            "roc_auc: 0.8449931412894376\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:102\n",
            "grav_train:102\n",
            "cont_val:26\n",
            "grav_val:26\n",
            "training data set_total：204\n",
            "validating data set_total：52\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.6787 Acc: 0.6422\n",
            "val Loss: 0.5609 Acc: 0.7115\n",
            "Validation loss decreased (inf --> 0.560915).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4242 Acc: 0.8088\n",
            "val Loss: 0.3468 Acc: 0.8462\n",
            "Validation loss decreased (0.560915 --> 0.346840).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.2872 Acc: 0.8775\n",
            "val Loss: 0.6233 Acc: 0.7500\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2157 Acc: 0.9118\n",
            "val Loss: 1.5522 Acc: 0.6346\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2867 Acc: 0.8971\n",
            "val Loss: 0.8732 Acc: 0.7500\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1890 Acc: 0.9412\n",
            "val Loss: 1.1139 Acc: 0.7308\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1675 Acc: 0.9314\n",
            "val Loss: 1.1778 Acc: 0.7115\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1845 Acc: 0.9461\n",
            "val Loss: 1.1638 Acc: 0.8077\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1203 Acc: 0.9363\n",
            "val Loss: 1.7039 Acc: 0.7885\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.0727 Acc: 0.9706\n",
            "val Loss: 1.0175 Acc: 0.7885\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0579 Acc: 0.9853\n",
            "val Loss: 0.5914 Acc: 0.8846\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.1161 Acc: 0.9706\n",
            "val Loss: 1.7152 Acc: 0.6538\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.1460 Acc: 0.9608\n",
            "val Loss: 0.7771 Acc: 0.8654\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0798 Acc: 0.9755\n",
            "val Loss: 0.8022 Acc: 0.7500\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.1806 Acc: 0.9412\n",
            "val Loss: 0.8772 Acc: 0.7308\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0824 Acc: 0.9755\n",
            "val Loss: 1.0813 Acc: 0.7308\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0287 Acc: 0.9902\n",
            "val Loss: 0.5831 Acc: 0.8462\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 2m 3s\n",
            "Best val Acc: 0.884615\n",
            "number of images: 108\n",
            "32 22 48 6\n",
            "Accuracy: 0.7407407407407407\n",
            "Precision (positive predictive value): 0.8421052631578947\n",
            "Recall (sensitivity): 0.5925925925925926\n",
            "Specificity: 0.8888888888888888\n",
            "F_value: 0.6956521739130435\n",
            "roc_auc: 0.8762002743484224\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:103\n",
            "grav_train:103\n",
            "cont_val:25\n",
            "grav_val:25\n",
            "training data set_total：206\n",
            "validating data set_total：50\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.6332 Acc: 0.6311\n",
            "val Loss: 0.5739 Acc: 0.7400\n",
            "Validation loss decreased (inf --> 0.573937).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.3618 Acc: 0.8252\n",
            "val Loss: 0.6679 Acc: 0.6400\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3551 Acc: 0.8447\n",
            "val Loss: 1.3555 Acc: 0.7200\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.3217 Acc: 0.8689\n",
            "val Loss: 1.2122 Acc: 0.6800\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2378 Acc: 0.9223\n",
            "val Loss: 3.2215 Acc: 0.5200\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.2211 Acc: 0.9078\n",
            "val Loss: 1.3038 Acc: 0.7200\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1471 Acc: 0.9515\n",
            "val Loss: 0.6824 Acc: 0.7000\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.0984 Acc: 0.9709\n",
            "val Loss: 0.9652 Acc: 0.7400\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1986 Acc: 0.9272\n",
            "val Loss: 0.8756 Acc: 0.7000\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1859 Acc: 0.9272\n",
            "val Loss: 2.4100 Acc: 0.6800\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.1698 Acc: 0.9175\n",
            "val Loss: 1.2626 Acc: 0.6800\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0657 Acc: 0.9854\n",
            "val Loss: 1.6967 Acc: 0.6800\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.1477 Acc: 0.9466\n",
            "val Loss: 0.9066 Acc: 0.7200\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0776 Acc: 0.9709\n",
            "val Loss: 1.2687 Acc: 0.7600\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.1001 Acc: 0.9709\n",
            "val Loss: 1.4388 Acc: 0.7400\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0987 Acc: 0.9660\n",
            "val Loss: 2.1537 Acc: 0.6600\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 54s\n",
            "Best val Acc: 0.760000\n",
            "number of images: 108\n",
            "45 9 39 15\n",
            "Accuracy: 0.7777777777777778\n",
            "Precision (positive predictive value): 0.75\n",
            "Recall (sensitivity): 0.8333333333333334\n",
            "Specificity: 0.7222222222222222\n",
            "F_value: 0.7894736842105262\n",
            "roc_auc: 0.896604938271605\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:103\n",
            "grav_train:103\n",
            "cont_val:25\n",
            "grav_val:25\n",
            "training data set_total：206\n",
            "validating data set_total：50\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.6450 Acc: 0.5825\n",
            "val Loss: 0.5454 Acc: 0.7600\n",
            "Validation loss decreased (inf --> 0.545362).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4248 Acc: 0.7913\n",
            "val Loss: 0.4225 Acc: 0.7800\n",
            "Validation loss decreased (0.545362 --> 0.422464).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3247 Acc: 0.8786\n",
            "val Loss: 0.7563 Acc: 0.7800\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.1685 Acc: 0.9320\n",
            "val Loss: 0.9813 Acc: 0.7400\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.1832 Acc: 0.9272\n",
            "val Loss: 3.6137 Acc: 0.5200\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1702 Acc: 0.9223\n",
            "val Loss: 1.2639 Acc: 0.7800\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.2759 Acc: 0.9029\n",
            "val Loss: 0.7066 Acc: 0.8800\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.2181 Acc: 0.9175\n",
            "val Loss: 0.9783 Acc: 0.7400\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1206 Acc: 0.9563\n",
            "val Loss: 0.6992 Acc: 0.8600\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1149 Acc: 0.9612\n",
            "val Loss: 1.1335 Acc: 0.8200\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0800 Acc: 0.9757\n",
            "val Loss: 1.9071 Acc: 0.7600\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0797 Acc: 0.9709\n",
            "val Loss: 1.2117 Acc: 0.8200\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0716 Acc: 0.9806\n",
            "val Loss: 0.6109 Acc: 0.8800\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0544 Acc: 0.9854\n",
            "val Loss: 0.8376 Acc: 0.8400\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0267 Acc: 0.9903\n",
            "val Loss: 0.6255 Acc: 0.8400\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0445 Acc: 0.9854\n",
            "val Loss: 0.5373 Acc: 0.8800\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0240 Acc: 0.9951\n",
            "val Loss: 0.7179 Acc: 0.9000\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 58s\n",
            "Best val Acc: 0.900000\n",
            "number of images: 108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "40 14 49 5\n",
            "Accuracy: 0.8240740740740741\n",
            "Precision (positive predictive value): 0.8888888888888888\n",
            "Recall (sensitivity): 0.7407407407407407\n",
            "Specificity: 0.9074074074074074\n",
            "F_value: 0.808080808080808\n",
            "roc_auc: 0.8717421124828533\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                  0         1         2         3         4  \\\n",
            "TP                               34        40        32        45        40   \n",
            "TN                               53        46        48        39        49   \n",
            "FP                                1         8         6        15         5   \n",
            "FN                               20        14        22         9        14   \n",
            "Accuracy                   0.805556  0.796296  0.740741  0.777778  0.824074   \n",
            "Positive predictive value  0.971429  0.833333  0.842105      0.75  0.888889   \n",
            "sensitity                   0.62963  0.740741  0.592593  0.833333  0.740741   \n",
            "specificity                0.981481  0.851852  0.888889  0.722222  0.907407   \n",
            "F-value                    0.764045  0.784314  0.695652  0.789474  0.808081   \n",
            "roc_auc                    0.866255  0.844993    0.8762  0.896605  0.871742   \n",
            "\n",
            "                                avg        std  \n",
            "TP                             38.2    4.66476  \n",
            "TN                               47    4.60435  \n",
            "FP                                7    4.60435  \n",
            "FN                             15.8    4.66476  \n",
            "Accuracy                   0.780854  0.0316715  \n",
            "Positive predictive value  0.833834  0.0811506  \n",
            "sensitity                  0.701602  0.0965808  \n",
            "specificity                0.860135  0.0953299  \n",
            "F-value                    0.762024  0.0435405  \n",
            "roc_auc                    0.871159   0.016625  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-JpdiMAAfb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "#Save ROC data\n",
        "with open(\"/content/drive/My Drive/Grav_bootcamp/ROCdata_\"+our_name+\"efficientNet_ImageNet.csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for i, t in enumerate(zip(Y_TRUE, Y_SCORE)):\n",
        "        writer.writerow([t[0],t[1]])\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPO-b2cAT0yF",
        "colab_type": "text"
      },
      "source": [
        "#**ネットワークの保存と読み込み**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMYyFQ6ATzyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワークの保存\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c744XUtfT6xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73a923de-e875-4e71-b4b5-1ae266557981"
      },
      "source": [
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}