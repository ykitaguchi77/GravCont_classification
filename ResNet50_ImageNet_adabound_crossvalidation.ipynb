{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "619bb3191aa64db09b442ca86351150c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7b7a633901bd415d95272fd1ed9aff12",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_652c8793434547ce8ff26005793922d5",
              "IPY_MODEL_2ce6ae0ce0954dcb89f546d53d95f65a"
            ]
          }
        },
        "7b7a633901bd415d95272fd1ed9aff12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "652c8793434547ce8ff26005793922d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_34b81b52b7324909858c50c4973e8571",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea7133ba910c47febeecc36a84c56443"
          }
        },
        "2ce6ae0ce0954dcb89f546d53d95f65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9b69e4325f7b4fb78ab8fc2bbb6c49e6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:30&lt;00:00, 3.37MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_be0af691738d4992a8236d0455c03732"
          }
        },
        "34b81b52b7324909858c50c4973e8571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea7133ba910c47febeecc36a84c56443": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b69e4325f7b4fb78ab8fc2bbb6c49e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "be0af691738d4992a8236d0455c03732": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/ResNet50_ImageNet_adabound_crossvalidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a4ZBlqPNdU",
        "colab_type": "text"
      },
      "source": [
        "#**GravCont: EfficientNet_b4_ImageNet**\n",
        "ValidationとTestに分けて解析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgM-Y7SVPNkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "342a4eb3-081d-44cb-9aa4-1c864bc46121"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch_optimizer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/48/f670cf4b47c315861d0547f0c2be579cd801304c86e55008492f1acebd01/torch_optimizer-0.0.1a15-py3-none-any.whl (41kB)\n",
            "\r\u001b[K     |███████▉                        | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 30kB 3.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 40kB 4.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.6MB/s \n",
            "\u001b[?25hCollecting pytorch-ranger>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/70/12256257d861bbc3e176130d25be1de085ce7a9e60594064888a950f2154/pytorch_ranger-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.0.1a15\n",
            "Random Seed:  1234\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp36-none-any.whl size=16031 sha256=e2b22d513a2613a477f0a76ea4c1a784100c5c6fa71410e1ae6c7ddb55d14e5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.0\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzp_09fWPNoU",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sJV06qPNsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(data_dir):\n",
        "    # 入力画像の前処理をするクラス\n",
        "    # 訓練時と推論時で処理が異なる\n",
        "\n",
        "    \"\"\"\n",
        "        画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "        画像のサイズをリサイズし、色を標準化する。\n",
        "        訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        resize : int\n",
        "            リサイズ先の画像の大きさ。\n",
        "        mean : (R, G, B)\n",
        "            各色チャネルの平均値。\n",
        "        std : (R, G, B)\n",
        "            各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    data_dir = data_dir\n",
        "    n_samples = len(data_dir)\n",
        "\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in ['train', 'val']}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                                shuffle=True, num_workers=4)\n",
        "                  for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "    print(class_names)\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_train:\"+str(len(os.listdir(path=data_dir + '/train/'+class_names[k]))))\n",
        "        k+=1\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_val:\"+str(len(os.listdir(path=data_dir + '/val/'+class_names[k]))))\n",
        "        k+=1\n",
        "\n",
        "    print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "    print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "    \n",
        "    return image_datasets, dataloaders, dataset_sizes, class_names, device\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "def getBatch(dataloaders):    \n",
        "    # Get a batch of training data\n",
        "    inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "    # Make a grid from batch\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "    #imshow(out, title=[class_names[x] for x in classes])\n",
        "    return(inputs, classes)\n",
        "\n",
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "def training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50):\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=patience, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "    \"\"\"\n",
        "    #probalilityを計算する\n",
        "    pred_prob = torch.topk(nn.Softmax(dim=1)(output), 1)[0]\n",
        "    pred_class = torch.topk(nn.Softmax(dim=1)(output), 1)[1]\n",
        "    if pred_class == 1:\n",
        "        pred_prob = pred_prob\n",
        "    elif pred_class == 0:\n",
        "        pred_prob = 1- pred_prob\n",
        "    return(model_pred, pred_prob)  #class_nameの番号で出力される\n",
        "    \"\"\"\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\"\"\"\n",
        "・True positive (TN)\n",
        "・False positive (FP)\n",
        "・True negative (TN)\n",
        "・False negative (FN)\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "Precision = TP/(FP + TP) ※positive predictive value\n",
        "Recall = TP/(TP + FN)　※sensitivity\n",
        "Specificity = TN/(FP + TN)\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)\n",
        "\"\"\"\n",
        "\n",
        "def evaluation(model_ft, testset_dir):\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #testデータセット内のファイル名を取得\n",
        "    image_path = glob.glob(testset_dir + \"/*/*\")\n",
        "    #random.shuffle(image_path)  #表示順をランダムにする\n",
        "    print('number of images: ' +str(len(image_path)))\n",
        "\n",
        "\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1     \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    return TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob\n",
        "\n",
        "\n",
        "def make_csv(roc_label_list):\n",
        "    #csvのdata tableを作成\n",
        "    pd.set_option('display.max_rows', 800)  # 省略なしで表示\n",
        "    #columns1 = [\"EfficientNet_32\", \"EfficientNet_64\", \"EfficientNet_128\", \"EfficientNet_256\", \"EfficientNet_512\", \"EfficientNet_558\"]\n",
        "    roc_label_list.extend([\"avg\", \"std\"])\n",
        "    index1 = [\"TP\",\"TN\",\"FP\",\"FN\",\"Accuracy\",\"Positive predictive value\",\"sensitity\",\"specificity\",\"F-value\",\"roc_auc\"]\n",
        "    df = pd.DataFrame(index=index1, columns=roc_label_list)\n",
        "    return df\n",
        "\n",
        "def write_csv(df, col, TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc):\n",
        "    df.iloc[0:10, col] = TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc \n",
        "    #print(df)\n",
        "\n",
        "    # CSVとして出力\n",
        "    #df2.to_csv(\"/content/drive/My Drive/Grav_bootcamp/Posttrain_model_eval_result.csv\",encoding=\"shift_jis\")\n",
        "    return df\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == 'cont':\n",
        "                  y_true.append(0)\n",
        "            elif i == 'grav':\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == 'cont':\n",
        "              y_true.append(0)\n",
        "        elif i == 'grav':\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "def calcurate_ave_std(df, fold):\n",
        "    for i in range(5):\n",
        "        df.iloc[i,fold] = df[i,0:5].mean \n",
        "\n",
        "def convnet():\n",
        "    model_ft = models.resnet50(pretrained=True)\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    optimizer_ft = optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=0,\n",
        "        amsbound=False,\n",
        "    )\n",
        "    return (model_ft, criterion, optimizer_ft)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGfUwQXv6Jc",
        "colab_type": "text"
      },
      "source": [
        "#**まとめて解析**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObCZwRvYCPTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "219709d0-ffb1-4b89-c071-f86a13bb3771"
      },
      "source": [
        "#create data_dir_list\n",
        "data_dir = '/content/drive/My Drive/gravcont_crossvalidation'\n",
        "fold = len(os.listdir(data_dir))\n",
        "print(fold)\n",
        "\n",
        "data_dir_list = [0]*fold\n",
        "\n",
        "for i in range(fold):\n",
        "    data_dir_list[i] = data_dir + '/' + str(i)\n",
        "    print(data_dir_list[i])\n",
        "\n",
        "#create roc_label_list\n",
        "roc_label_list = [0]*fold\n",
        "roc_label_list = list(range(fold))\n",
        "print(roc_label_list)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "/content/drive/My Drive/gravcont_crossvalidation/0\n",
            "/content/drive/My Drive/gravcont_crossvalidation/1\n",
            "/content/drive/My Drive/gravcont_crossvalidation/2\n",
            "/content/drive/My Drive/gravcont_crossvalidation/3\n",
            "/content/drive/My Drive/gravcont_crossvalidation/4\n",
            "[0, 1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM2VMXltwBs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "619bb3191aa64db09b442ca86351150c",
            "7b7a633901bd415d95272fd1ed9aff12",
            "652c8793434547ce8ff26005793922d5",
            "2ce6ae0ce0954dcb89f546d53d95f65a",
            "34b81b52b7324909858c50c4973e8571",
            "ea7133ba910c47febeecc36a84c56443",
            "9b69e4325f7b4fb78ab8fc2bbb6c49e6",
            "be0af691738d4992a8236d0455c03732"
          ]
        },
        "outputId": "f397abb0-3b0a-4aa7-8381-5f06b3315169"
      },
      "source": [
        "df = make_csv(roc_label_list)\n",
        "\n",
        "label_list_list, model_pred_prob_list, Y_TRUE, Y_SCORE = [],[],[],[]\n",
        "\n",
        "print(data_dir_list)\n",
        "print(roc_label_list)\n",
        "\n",
        "for i, t in enumerate(zip(data_dir_list, roc_label_list)):\n",
        "\n",
        "    image_datasets, dataloaders, dataset_sizes, class_names, device = pre_process(t[0]) #path\n",
        "    inputs, classes = getBatch(dataloaders)\n",
        "    model_ft, criterion, optimizer_ft = convnet()\n",
        "    training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50)  \n",
        "    TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob = evaluation(model_ft, '/content/drive/My Drive/Grav_bootcamp/Posttrain_250px')\n",
        "    roc_auc, y_true, y_score = calculate_auc(label_list, model_pred_prob)\n",
        "    Y_TRUE.append(y_true)\n",
        "    Y_SCORE.append(y_score)\n",
        "    df = write_csv(df, i,TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, roc_auc) #numberをcsvの行として指定\n",
        "\n",
        "    label_list_list.append(label_list)\n",
        "    model_pred_prob_list.append(model_pred_prob)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#Draw ROC curve\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list))\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "#それぞれの項目の平均を計算しcsvに追記する\n",
        "df.iloc[0:4,fold], df.iloc[9,fold]   = df.mean(axis=1)[0:4], df.mean(axis=1)[9] \n",
        "df.iloc[0:10,fold+1] = df.std(axis=1)[0:10]\n",
        "TP,TN,FP,FN = df.mean(axis=1)[0:4]\n",
        "df.iloc[4:9,fold] = calculateAccuracy (TP, TN, FP, FN)\n",
        "print(df)\n",
        "\n",
        "# 出力名を記入(Network + number of images)\n",
        "out_name = \"ResNet50_ImageNet_256\"\n",
        "\n",
        "# CSVとして出力\n",
        "df.to_csv(\"/content/drive/My Drive/Grav_bootcamp/crossvaridation_\" + out_name + \".csv\",encoding=\"shift_jis\")\n",
        "\n",
        "#ROC_curveを保存\n",
        "fig.savefig(\"/content/drive/My Drive/Grav_bootcamp/crossvaridation_\" + out_name +\".png\")\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/gravcont_crossvalidation/0', '/content/drive/My Drive/gravcont_crossvalidation/1', '/content/drive/My Drive/gravcont_crossvalidation/2', '/content/drive/My Drive/gravcont_crossvalidation/3', '/content/drive/My Drive/gravcont_crossvalidation/4']\n",
            "[0, 1, 2, 3, 4, 'avg', 'std']\n",
            "['cont', 'grav']\n",
            "cont_train:102\n",
            "grav_train:102\n",
            "cont_val:26\n",
            "grav_val:26\n",
            "training data set_total：204\n",
            "validating data set_total：52\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "619bb3191aa64db09b442ca86351150c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 0/49\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch_optimizer/adabound.py:142: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 1.0469 Acc: 0.5686\n",
            "val Loss: 21.8506 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 21.850619).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.7930 Acc: 0.5147\n",
            "val Loss: 1.9834 Acc: 0.5769\n",
            "Validation loss decreased (21.850619 --> 1.983374).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.7069 Acc: 0.5539\n",
            "val Loss: 0.8488 Acc: 0.5192\n",
            "Validation loss decreased (1.983374 --> 0.848755).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.6093 Acc: 0.7304\n",
            "val Loss: 0.7059 Acc: 0.7500\n",
            "Validation loss decreased (0.848755 --> 0.705852).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.5309 Acc: 0.7647\n",
            "val Loss: 0.7264 Acc: 0.6346\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.5055 Acc: 0.7500\n",
            "val Loss: 1.1148 Acc: 0.6923\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.5135 Acc: 0.7451\n",
            "val Loss: 24.6441 Acc: 0.5000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.6226 Acc: 0.6912\n",
            "val Loss: 9.5104 Acc: 0.4808\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.5565 Acc: 0.7255\n",
            "val Loss: 0.6002 Acc: 0.7885\n",
            "Validation loss decreased (0.705852 --> 0.600248).  Saving model ...\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.4799 Acc: 0.7696\n",
            "val Loss: 5.9463 Acc: 0.6731\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.6838 Acc: 0.7549\n",
            "val Loss: 6.9156 Acc: 0.5385\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.4846 Acc: 0.7745\n",
            "val Loss: 0.7937 Acc: 0.7500\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.4716 Acc: 0.7647\n",
            "val Loss: 0.6598 Acc: 0.6538\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.5768 Acc: 0.7451\n",
            "val Loss: 1.4900 Acc: 0.6154\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.5438 Acc: 0.7304\n",
            "val Loss: 1.2758 Acc: 0.5769\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.5326 Acc: 0.7500\n",
            "val Loss: 0.8128 Acc: 0.6538\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.4634 Acc: 0.8088\n",
            "val Loss: 0.7987 Acc: 0.6538\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.4191 Acc: 0.8382\n",
            "val Loss: 0.6543 Acc: 0.7308\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.4200 Acc: 0.8039\n",
            "val Loss: 0.4693 Acc: 0.7885\n",
            "Validation loss decreased (0.600248 --> 0.469296).  Saving model ...\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.3929 Acc: 0.8137\n",
            "val Loss: 0.4810 Acc: 0.7308\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.3561 Acc: 0.8529\n",
            "val Loss: 0.6875 Acc: 0.7115\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.3957 Acc: 0.8431\n",
            "val Loss: 0.4260 Acc: 0.8077\n",
            "Validation loss decreased (0.469296 --> 0.426035).  Saving model ...\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.3481 Acc: 0.8529\n",
            "val Loss: 0.6329 Acc: 0.6731\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.3848 Acc: 0.8480\n",
            "val Loss: 0.4235 Acc: 0.8462\n",
            "Validation loss decreased (0.426035 --> 0.423525).  Saving model ...\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.3411 Acc: 0.8578\n",
            "val Loss: 0.4575 Acc: 0.7692\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.3075 Acc: 0.8775\n",
            "val Loss: 0.3673 Acc: 0.8269\n",
            "Validation loss decreased (0.423525 --> 0.367276).  Saving model ...\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.3473 Acc: 0.8676\n",
            "val Loss: 0.3874 Acc: 0.8462\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.2969 Acc: 0.8725\n",
            "val Loss: 0.4643 Acc: 0.8462\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.3175 Acc: 0.8775\n",
            "val Loss: 0.7344 Acc: 0.6731\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.2311 Acc: 0.9314\n",
            "val Loss: 0.3634 Acc: 0.8462\n",
            "Validation loss decreased (0.367276 --> 0.363386).  Saving model ...\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.3338 Acc: 0.8431\n",
            "val Loss: 15.2150 Acc: 0.5769\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.3749 Acc: 0.8627\n",
            "val Loss: 2.5348 Acc: 0.5192\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.3865 Acc: 0.8088\n",
            "val Loss: 0.7219 Acc: 0.7692\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.3625 Acc: 0.8529\n",
            "val Loss: 0.3362 Acc: 0.8654\n",
            "Validation loss decreased (0.363386 --> 0.336197).  Saving model ...\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.3319 Acc: 0.8627\n",
            "val Loss: 0.4578 Acc: 0.7885\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.2326 Acc: 0.9510\n",
            "val Loss: 0.4433 Acc: 0.7885\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.2980 Acc: 0.8578\n",
            "val Loss: 0.4356 Acc: 0.8654\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.1959 Acc: 0.9167\n",
            "val Loss: 0.4243 Acc: 0.8077\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.2169 Acc: 0.9216\n",
            "val Loss: 0.6035 Acc: 0.8462\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.2594 Acc: 0.9069\n",
            "val Loss: 0.7691 Acc: 0.7885\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.2579 Acc: 0.8824\n",
            "val Loss: 1.4541 Acc: 0.7500\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.2329 Acc: 0.8971\n",
            "val Loss: 0.5575 Acc: 0.7115\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.2037 Acc: 0.9265\n",
            "val Loss: 1.0904 Acc: 0.6923\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.2274 Acc: 0.9118\n",
            "val Loss: 0.4919 Acc: 0.8077\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.1516 Acc: 0.9314\n",
            "val Loss: 0.6567 Acc: 0.8462\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.0976 Acc: 0.9657\n",
            "val Loss: 0.3740 Acc: 0.8846\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.0894 Acc: 0.9608\n",
            "val Loss: 0.5331 Acc: 0.8269\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.0666 Acc: 0.9804\n",
            "val Loss: 0.5183 Acc: 0.8269\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "train Loss: 0.0788 Acc: 0.9706\n",
            "val Loss: 0.6324 Acc: 0.8269\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 3m 16s\n",
            "Best val Acc: 0.884615\n",
            "number of images: 108\n",
            "38 16 46 8\n",
            "Accuracy: 0.7777777777777778\n",
            "Precision (positive predictive value): 0.8260869565217391\n",
            "Recall (sensitivity): 0.7037037037037037\n",
            "Specificity: 0.8518518518518519\n",
            "F_value: 0.76\n",
            "roc_auc: 0.8758573388203017\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:102\n",
            "grav_train:102\n",
            "cont_val:26\n",
            "grav_val:26\n",
            "training data set_total：204\n",
            "validating data set_total：52\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.8433 Acc: 0.6127\n",
            "val Loss: 4.3184 Acc: 0.6731\n",
            "Validation loss decreased (inf --> 4.318360).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.5796 Acc: 0.6912\n",
            "val Loss: 13.8920 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.6416 Acc: 0.6814\n",
            "val Loss: 2.5650 Acc: 0.7500\n",
            "Validation loss decreased (4.318360 --> 2.565016).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.4616 Acc: 0.7843\n",
            "val Loss: 0.6616 Acc: 0.6923\n",
            "Validation loss decreased (2.565016 --> 0.661605).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.4065 Acc: 0.8333\n",
            "val Loss: 3.5525 Acc: 0.6154\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.5386 Acc: 0.7451\n",
            "val Loss: 0.4894 Acc: 0.7115\n",
            "Validation loss decreased (0.661605 --> 0.489419).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.4481 Acc: 0.8088\n",
            "val Loss: 0.5112 Acc: 0.7692\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.3858 Acc: 0.8039\n",
            "val Loss: 1.0730 Acc: 0.5962\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.3554 Acc: 0.8627\n",
            "val Loss: 0.6548 Acc: 0.7115\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.3512 Acc: 0.8627\n",
            "val Loss: 0.5090 Acc: 0.8077\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.3524 Acc: 0.8676\n",
            "val Loss: 0.7651 Acc: 0.6731\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.4792 Acc: 0.7990\n",
            "val Loss: 0.8822 Acc: 0.6731\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.3477 Acc: 0.8775\n",
            "val Loss: 0.6247 Acc: 0.7115\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.4158 Acc: 0.7990\n",
            "val Loss: 1.1945 Acc: 0.6538\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.2772 Acc: 0.8922\n",
            "val Loss: 0.6122 Acc: 0.7692\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.2683 Acc: 0.8824\n",
            "val Loss: 0.8104 Acc: 0.7115\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.3437 Acc: 0.8627\n",
            "val Loss: 4.4346 Acc: 0.6346\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.3094 Acc: 0.8725\n",
            "val Loss: 0.4393 Acc: 0.7500\n",
            "Validation loss decreased (0.489419 --> 0.439259).  Saving model ...\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.3003 Acc: 0.8725\n",
            "val Loss: 0.8298 Acc: 0.7308\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.2511 Acc: 0.9020\n",
            "val Loss: 0.8324 Acc: 0.7692\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.2924 Acc: 0.8775\n",
            "val Loss: 0.4973 Acc: 0.7692\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.2250 Acc: 0.9069\n",
            "val Loss: 0.5513 Acc: 0.7115\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.2060 Acc: 0.9167\n",
            "val Loss: 0.6187 Acc: 0.8654\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.3862 Acc: 0.8676\n",
            "val Loss: 0.4983 Acc: 0.7500\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.2106 Acc: 0.9167\n",
            "val Loss: 0.4921 Acc: 0.7692\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.1543 Acc: 0.9314\n",
            "val Loss: 0.3706 Acc: 0.8462\n",
            "Validation loss decreased (0.439259 --> 0.370643).  Saving model ...\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.2395 Acc: 0.9069\n",
            "val Loss: 0.9297 Acc: 0.6731\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.1851 Acc: 0.9167\n",
            "val Loss: 0.5611 Acc: 0.7500\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.2115 Acc: 0.9167\n",
            "val Loss: 0.6174 Acc: 0.7692\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.1862 Acc: 0.9216\n",
            "val Loss: 0.6994 Acc: 0.8077\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.3399 Acc: 0.8529\n",
            "val Loss: 2.7554 Acc: 0.5962\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.2387 Acc: 0.9167\n",
            "val Loss: 0.8365 Acc: 0.7308\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.1904 Acc: 0.9216\n",
            "val Loss: 0.2930 Acc: 0.9038\n",
            "Validation loss decreased (0.370643 --> 0.293005).  Saving model ...\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.1848 Acc: 0.9167\n",
            "val Loss: 0.4386 Acc: 0.8269\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.0646 Acc: 0.9804\n",
            "val Loss: 0.5804 Acc: 0.7500\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.0671 Acc: 0.9804\n",
            "val Loss: 0.5874 Acc: 0.7308\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.0871 Acc: 0.9608\n",
            "val Loss: 0.9903 Acc: 0.7500\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.0941 Acc: 0.9608\n",
            "val Loss: 0.7053 Acc: 0.7885\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.1120 Acc: 0.9608\n",
            "val Loss: 0.8415 Acc: 0.6923\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.1640 Acc: 0.9363\n",
            "val Loss: 0.8311 Acc: 0.8269\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.3964 Acc: 0.8578\n",
            "val Loss: 1.4791 Acc: 0.7115\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.2575 Acc: 0.9020\n",
            "val Loss: 1.9893 Acc: 0.6731\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "train Loss: 0.1910 Acc: 0.9069\n",
            "val Loss: 0.5871 Acc: 0.8269\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "train Loss: 0.2770 Acc: 0.9020\n",
            "val Loss: 0.6095 Acc: 0.7500\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "train Loss: 0.1320 Acc: 0.9363\n",
            "val Loss: 0.3156 Acc: 0.8846\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "train Loss: 0.1561 Acc: 0.9461\n",
            "val Loss: 0.5294 Acc: 0.8077\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "train Loss: 0.1421 Acc: 0.9314\n",
            "val Loss: 0.4238 Acc: 0.8654\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "train Loss: 0.0823 Acc: 0.9804\n",
            "val Loss: 0.7142 Acc: 0.7308\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 3m 9s\n",
            "Best val Acc: 0.903846\n",
            "number of images: 108\n",
            "41 13 51 3\n",
            "Accuracy: 0.8518518518518519\n",
            "Precision (positive predictive value): 0.9318181818181818\n",
            "Recall (sensitivity): 0.7592592592592593\n",
            "Specificity: 0.9444444444444444\n",
            "F_value: 0.836734693877551\n",
            "roc_auc: 0.9029492455418382\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:102\n",
            "grav_train:102\n",
            "cont_val:26\n",
            "grav_val:26\n",
            "training data set_total：204\n",
            "validating data set_total：52\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.9010 Acc: 0.5098\n",
            "val Loss: 3.2513 Acc: 0.4615\n",
            "Validation loss decreased (inf --> 3.251258).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.5219 Acc: 0.7647\n",
            "val Loss: 14.4072 Acc: 0.5577\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.4993 Acc: 0.7892\n",
            "val Loss: 0.5089 Acc: 0.7692\n",
            "Validation loss decreased (3.251258 --> 0.508904).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.3564 Acc: 0.8627\n",
            "val Loss: 0.5584 Acc: 0.7885\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.4496 Acc: 0.8186\n",
            "val Loss: 1.7355 Acc: 0.7308\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.3811 Acc: 0.8186\n",
            "val Loss: 0.3404 Acc: 0.8077\n",
            "Validation loss decreased (0.508904 --> 0.340413).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.4224 Acc: 0.7794\n",
            "val Loss: 0.4708 Acc: 0.7500\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.3699 Acc: 0.8382\n",
            "val Loss: 3.4111 Acc: 0.5577\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.4257 Acc: 0.7941\n",
            "val Loss: 0.5754 Acc: 0.7885\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.4650 Acc: 0.7990\n",
            "val Loss: 0.7390 Acc: 0.6731\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.4052 Acc: 0.8186\n",
            "val Loss: 0.4191 Acc: 0.8077\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.4039 Acc: 0.8333\n",
            "val Loss: 0.4001 Acc: 0.8269\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.3492 Acc: 0.8578\n",
            "val Loss: 0.4222 Acc: 0.8077\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.3575 Acc: 0.8627\n",
            "val Loss: 0.5396 Acc: 0.7308\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.2618 Acc: 0.9118\n",
            "val Loss: 0.7023 Acc: 0.7115\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.2739 Acc: 0.8627\n",
            "val Loss: 1.0313 Acc: 0.5192\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.4203 Acc: 0.7941\n",
            "val Loss: 0.6616 Acc: 0.7115\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.3128 Acc: 0.8627\n",
            "val Loss: 0.4300 Acc: 0.7885\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.3365 Acc: 0.8480\n",
            "val Loss: 1.2217 Acc: 0.6346\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.2247 Acc: 0.9314\n",
            "val Loss: 0.5538 Acc: 0.7308\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.2455 Acc: 0.8873\n",
            "val Loss: 0.7176 Acc: 0.6538\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 28s\n",
            "Best val Acc: 0.826923\n",
            "number of images: 108\n",
            "42 12 40 14\n",
            "Accuracy: 0.7592592592592593\n",
            "Precision (positive predictive value): 0.75\n",
            "Recall (sensitivity): 0.7777777777777778\n",
            "Specificity: 0.7407407407407407\n",
            "F_value: 0.7636363636363638\n",
            "roc_auc: 0.8412208504801097\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:103\n",
            "grav_train:247\n",
            "cont_val:25\n",
            "grav_val:77\n",
            "training data set_total：350\n",
            "validating data set_total：102\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.7340 Acc: 0.6429\n",
            "val Loss: 28.3978 Acc: 0.7549\n",
            "Validation loss decreased (inf --> 28.397788).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4909 Acc: 0.7114\n",
            "val Loss: 0.4842 Acc: 0.7549\n",
            "Validation loss decreased (28.397788 --> 0.484234).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.4881 Acc: 0.7114\n",
            "val Loss: 2.1732 Acc: 0.7647\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.4641 Acc: 0.7486\n",
            "val Loss: 0.3504 Acc: 0.8235\n",
            "Validation loss decreased (0.484234 --> 0.350402).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.4726 Acc: 0.7229\n",
            "val Loss: 0.4179 Acc: 0.7549\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.4108 Acc: 0.7514\n",
            "val Loss: 0.4216 Acc: 0.7353\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.4231 Acc: 0.7886\n",
            "val Loss: 0.7340 Acc: 0.8431\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.3701 Acc: 0.8086\n",
            "val Loss: 0.3993 Acc: 0.7647\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.3485 Acc: 0.8400\n",
            "val Loss: 0.4619 Acc: 0.8627\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.3254 Acc: 0.8371\n",
            "val Loss: 0.6169 Acc: 0.6471\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.3354 Acc: 0.8400\n",
            "val Loss: 0.3763 Acc: 0.8333\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.3462 Acc: 0.8286\n",
            "val Loss: 0.3975 Acc: 0.8333\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.3236 Acc: 0.8343\n",
            "val Loss: 0.3948 Acc: 0.7745\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.3361 Acc: 0.8600\n",
            "val Loss: 0.6451 Acc: 0.8137\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.2780 Acc: 0.8657\n",
            "val Loss: 0.3799 Acc: 0.8137\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.3054 Acc: 0.8629\n",
            "val Loss: 0.4396 Acc: 0.8235\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.2977 Acc: 0.8600\n",
            "val Loss: 0.3023 Acc: 0.8627\n",
            "Validation loss decreased (0.350402 --> 0.302313).  Saving model ...\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.2930 Acc: 0.8686\n",
            "val Loss: 0.5010 Acc: 0.8627\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.2558 Acc: 0.8771\n",
            "val Loss: 0.3804 Acc: 0.8235\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.2578 Acc: 0.8971\n",
            "val Loss: 0.3342 Acc: 0.8627\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.2351 Acc: 0.9029\n",
            "val Loss: 0.3330 Acc: 0.8922\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.1786 Acc: 0.9171\n",
            "val Loss: 0.4787 Acc: 0.8824\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.1335 Acc: 0.9429\n",
            "val Loss: 0.7264 Acc: 0.8431\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.1663 Acc: 0.9543\n",
            "val Loss: 0.3892 Acc: 0.8039\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.1607 Acc: 0.9343\n",
            "val Loss: 0.4981 Acc: 0.8039\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.1240 Acc: 0.9543\n",
            "val Loss: 0.8339 Acc: 0.8824\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.1361 Acc: 0.9457\n",
            "val Loss: 0.6040 Acc: 0.8235\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.1323 Acc: 0.9429\n",
            "val Loss: 0.5236 Acc: 0.8431\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.0883 Acc: 0.9686\n",
            "val Loss: 0.3980 Acc: 0.8627\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.0786 Acc: 0.9686\n",
            "val Loss: 0.4508 Acc: 0.8333\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.1137 Acc: 0.9600\n",
            "val Loss: 1.0728 Acc: 0.8039\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.0923 Acc: 0.9543\n",
            "val Loss: 0.4463 Acc: 0.9118\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 3m 28s\n",
            "Best val Acc: 0.911765\n",
            "number of images: 108\n",
            "40 14 46 8\n",
            "Accuracy: 0.7962962962962963\n",
            "Precision (positive predictive value): 0.8333333333333334\n",
            "Recall (sensitivity): 0.7407407407407407\n",
            "Specificity: 0.8518518518518519\n",
            "F_value: 0.7843137254901961\n",
            "roc_auc: 0.9224965706447189\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:247\n",
            "grav_train:242\n",
            "cont_val:77\n",
            "grav_val:76\n",
            "training data set_total：489\n",
            "validating data set_total：153\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.9515 Acc: 0.4867\n",
            "val Loss: 1.4676 Acc: 0.5033\n",
            "Validation loss decreased (inf --> 1.467604).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.6107 Acc: 0.6564\n",
            "val Loss: 1.3924 Acc: 0.6601\n",
            "Validation loss decreased (1.467604 --> 1.392389).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.5177 Acc: 0.7628\n",
            "val Loss: 0.6656 Acc: 0.6471\n",
            "Validation loss decreased (1.392389 --> 0.665594).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.5413 Acc: 0.7464\n",
            "val Loss: 0.9057 Acc: 0.7190\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.5259 Acc: 0.7526\n",
            "val Loss: 0.4588 Acc: 0.8170\n",
            "Validation loss decreased (0.665594 --> 0.458835).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.4540 Acc: 0.7955\n",
            "val Loss: 0.4166 Acc: 0.8170\n",
            "Validation loss decreased (0.458835 --> 0.416646).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.4784 Acc: 0.7771\n",
            "val Loss: 0.6634 Acc: 0.7386\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.4365 Acc: 0.7935\n",
            "val Loss: 0.3893 Acc: 0.8366\n",
            "Validation loss decreased (0.416646 --> 0.389294).  Saving model ...\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.4175 Acc: 0.8221\n",
            "val Loss: 0.3545 Acc: 0.8431\n",
            "Validation loss decreased (0.389294 --> 0.354493).  Saving model ...\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.4374 Acc: 0.8119\n",
            "val Loss: 1.1899 Acc: 0.6471\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.4724 Acc: 0.7996\n",
            "val Loss: 0.4138 Acc: 0.8039\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.3664 Acc: 0.8364\n",
            "val Loss: 0.3492 Acc: 0.8497\n",
            "Validation loss decreased (0.354493 --> 0.349170).  Saving model ...\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.3779 Acc: 0.8425\n",
            "val Loss: 0.3045 Acc: 0.8627\n",
            "Validation loss decreased (0.349170 --> 0.304485).  Saving model ...\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.3625 Acc: 0.8528\n",
            "val Loss: 0.3159 Acc: 0.8497\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.2750 Acc: 0.8855\n",
            "val Loss: 0.4466 Acc: 0.7974\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.2852 Acc: 0.8793\n",
            "val Loss: 0.4906 Acc: 0.8105\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.3679 Acc: 0.8487\n",
            "val Loss: 0.3126 Acc: 0.8693\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.2982 Acc: 0.8609\n",
            "val Loss: 0.2243 Acc: 0.9020\n",
            "Validation loss decreased (0.304485 --> 0.224275).  Saving model ...\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.2932 Acc: 0.8834\n",
            "val Loss: 0.3384 Acc: 0.8497\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.2859 Acc: 0.8998\n",
            "val Loss: 0.2339 Acc: 0.9020\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.2266 Acc: 0.9223\n",
            "val Loss: 0.3250 Acc: 0.8693\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.2143 Acc: 0.9284\n",
            "val Loss: 0.7616 Acc: 0.7647\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.2274 Acc: 0.9182\n",
            "val Loss: 0.2237 Acc: 0.9020\n",
            "Validation loss decreased (0.224275 --> 0.223674).  Saving model ...\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.2589 Acc: 0.8834\n",
            "val Loss: 0.3187 Acc: 0.8497\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.2355 Acc: 0.9039\n",
            "val Loss: 0.3963 Acc: 0.8170\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.2299 Acc: 0.8896\n",
            "val Loss: 0.4687 Acc: 0.7908\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.2491 Acc: 0.8978\n",
            "val Loss: 0.2133 Acc: 0.9216\n",
            "Validation loss decreased (0.223674 --> 0.213289).  Saving model ...\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.1980 Acc: 0.9162\n",
            "val Loss: 0.3048 Acc: 0.8824\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.1510 Acc: 0.9325\n",
            "val Loss: 0.3477 Acc: 0.8431\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.1686 Acc: 0.9407\n",
            "val Loss: 0.2917 Acc: 0.8824\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.1431 Acc: 0.9387\n",
            "val Loss: 0.4376 Acc: 0.8235\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.1914 Acc: 0.9264\n",
            "val Loss: 0.5215 Acc: 0.8235\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.1190 Acc: 0.9571\n",
            "val Loss: 0.2391 Acc: 0.9150\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.1510 Acc: 0.9407\n",
            "val Loss: 0.3087 Acc: 0.8824\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.1625 Acc: 0.9305\n",
            "val Loss: 0.4158 Acc: 0.8497\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.1126 Acc: 0.9571\n",
            "val Loss: 0.2604 Acc: 0.9085\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.1550 Acc: 0.9346\n",
            "val Loss: 0.2153 Acc: 0.9150\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.2453 Acc: 0.8978\n",
            "val Loss: 0.9350 Acc: 0.7516\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.2553 Acc: 0.8957\n",
            "val Loss: 0.3722 Acc: 0.8235\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.1309 Acc: 0.9571\n",
            "val Loss: 0.2399 Acc: 0.8954\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "train Loss: 0.0897 Acc: 0.9734\n",
            "val Loss: 0.2700 Acc: 0.8954\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "train Loss: 0.1224 Acc: 0.9550\n",
            "val Loss: 1.1266 Acc: 0.7451\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 5m 56s\n",
            "Best val Acc: 0.921569\n",
            "number of images: 108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "43 11 52 2\n",
            "Accuracy: 0.8796296296296297\n",
            "Precision (positive predictive value): 0.9555555555555556\n",
            "Recall (sensitivity): 0.7962962962962963\n",
            "Specificity: 0.9629629629629629\n",
            "F_value: 0.8686868686868687\n",
            "roc_auc: 0.9053497942386831\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                  0         1         2         3         4  \\\n",
            "TP                               38        41        42        40        43   \n",
            "TN                               46        51        40        46        52   \n",
            "FP                                8         3        14         8         2   \n",
            "FN                               16        13        12        14        11   \n",
            "Accuracy                   0.777778  0.851852  0.759259  0.796296   0.87963   \n",
            "Positive predictive value  0.826087  0.931818      0.75  0.833333  0.955556   \n",
            "sensitity                  0.703704  0.759259  0.777778  0.740741  0.796296   \n",
            "specificity                0.851852  0.944444  0.740741  0.851852  0.962963   \n",
            "F-value                        0.76  0.836735  0.763636  0.784314  0.868687   \n",
            "roc_auc                    0.875857  0.902949  0.841221  0.922497   0.90535   \n",
            "\n",
            "                                avg        std  \n",
            "TP                             40.8    1.72047  \n",
            "TN                               47    4.28952  \n",
            "FP                                7    4.28952  \n",
            "FN                             13.2    1.72047  \n",
            "Accuracy                   0.807263  0.0508838  \n",
            "Positive predictive value  0.841913  0.0840355  \n",
            "sensitity                   0.75287  0.0356211  \n",
            "specificity                0.860816  0.0888117  \n",
            "F-value                    0.794906  0.0479582  \n",
            "roc_auc                    0.889575  0.0284177  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-JpdiMAAfb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "#Save ROC data\n",
        "with open(\"/content/drive/My Drive/Grav_bootcamp/ROCdata_\"+out_name+\".csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for i, t in enumerate(zip(Y_TRUE, Y_SCORE)):\n",
        "        writer.writerow([t[0],t[1]])\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPO-b2cAT0yF",
        "colab_type": "text"
      },
      "source": [
        "#**ネットワークの保存と読み込み**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMYyFQ6ATzyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワークの保存\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c744XUtfT6xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73a923de-e875-4e71-b4b5-1ae266557981"
      },
      "source": [
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}