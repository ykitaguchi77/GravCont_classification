{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/ResNet50_nonpretrained_adabound_crossvalidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a4ZBlqPNdU",
        "colab_type": "text"
      },
      "source": [
        "#**GravCont: EfficientNet_b4_ImageNet**\n",
        "ValidationとTestに分けて解析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgM-Y7SVPNkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "2be139a1-6475-4698-cacf-377a2cbc26c2"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.6/dist-packages (0.0.1a15)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Random Seed:  1234\n",
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzp_09fWPNoU",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sJV06qPNsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(data_dir):\n",
        "    # 入力画像の前処理をするクラス\n",
        "    # 訓練時と推論時で処理が異なる\n",
        "\n",
        "    \"\"\"\n",
        "        画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "        画像のサイズをリサイズし、色を標準化する。\n",
        "        訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        resize : int\n",
        "            リサイズ先の画像の大きさ。\n",
        "        mean : (R, G, B)\n",
        "            各色チャネルの平均値。\n",
        "        std : (R, G, B)\n",
        "            各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    data_dir = data_dir\n",
        "    n_samples = len(data_dir)\n",
        "\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in ['train', 'val']}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                                shuffle=True, num_workers=4)\n",
        "                  for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "    print(class_names)\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_train:\"+str(len(os.listdir(path=data_dir + '/train/'+class_names[k]))))\n",
        "        k+=1\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_val:\"+str(len(os.listdir(path=data_dir + '/val/'+class_names[k]))))\n",
        "        k+=1\n",
        "\n",
        "    print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "    print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "    \n",
        "    return image_datasets, dataloaders, dataset_sizes, class_names, device\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "def getBatch(dataloaders):    \n",
        "    # Get a batch of training data\n",
        "    inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "    # Make a grid from batch\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "    #imshow(out, title=[class_names[x] for x in classes])\n",
        "    return(inputs, classes)\n",
        "\n",
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "def training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50):\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=patience, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "    \"\"\"\n",
        "    #probalilityを計算する\n",
        "    pred_prob = torch.topk(nn.Softmax(dim=1)(output), 1)[0]\n",
        "    pred_class = torch.topk(nn.Softmax(dim=1)(output), 1)[1]\n",
        "    if pred_class == 1:\n",
        "        pred_prob = pred_prob\n",
        "    elif pred_class == 0:\n",
        "        pred_prob = 1- pred_prob\n",
        "    return(model_pred, pred_prob)  #class_nameの番号で出力される\n",
        "    \"\"\"\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\"\"\"\n",
        "・True positive (TN)\n",
        "・False positive (FP)\n",
        "・True negative (TN)\n",
        "・False negative (FN)\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "Precision = TP/(FP + TP) ※positive predictive value\n",
        "Recall = TP/(TP + FN)　※sensitivity\n",
        "Specificity = TN/(FP + TN)\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)\n",
        "\"\"\"\n",
        "\n",
        "def evaluation(model_ft, testset_dir):\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #testデータセット内のファイル名を取得\n",
        "    image_path = glob.glob(testset_dir + \"/*/*\")\n",
        "    #random.shuffle(image_path)  #表示順をランダムにする\n",
        "    print('number of images: ' +str(len(image_path)))\n",
        "\n",
        "\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1     \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    return TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob\n",
        "\n",
        "\n",
        "def make_csv(roc_label_list):\n",
        "    #csvのdata tableを作成\n",
        "    pd.set_option('display.max_rows', 800)  # 省略なしで表示\n",
        "    #columns1 = [\"EfficientNet_32\", \"EfficientNet_64\", \"EfficientNet_128\", \"EfficientNet_256\", \"EfficientNet_512\", \"EfficientNet_558\"]\n",
        "    roc_label_list.extend([\"avg\", \"std\"])\n",
        "    index1 = [\"TP\",\"TN\",\"FP\",\"FN\",\"Accuracy\",\"Positive predictive value\",\"sensitity\",\"specificity\",\"F-value\",\"roc_auc\"]\n",
        "    df = pd.DataFrame(index=index1, columns=roc_label_list)\n",
        "    return df\n",
        "\n",
        "def write_csv(df, col, TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc):\n",
        "    df.iloc[0:10, col] = TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc \n",
        "    #print(df)\n",
        "\n",
        "    # CSVとして出力\n",
        "    #df2.to_csv(\"/content/drive/My Drive/Grav_bootcamp/Posttrain_model_eval_result.csv\",encoding=\"shift_jis\")\n",
        "    return df\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == 'cont':\n",
        "                  y_true.append(0)\n",
        "            elif i == 'grav':\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == 'cont':\n",
        "              y_true.append(0)\n",
        "        elif i == 'grav':\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "def calcurate_ave_std(df, fold):\n",
        "    for i in range(5):\n",
        "        df.iloc[i,fold] = df[i,0:5].mean \n",
        "\n",
        "def convnet():\n",
        "    model_ft = models.resnet50(pretrained=False)\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    optimizer_ft = optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=0,\n",
        "        amsbound=False,\n",
        "    )\n",
        "    return (model_ft, criterion, optimizer_ft)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGfUwQXv6Jc",
        "colab_type": "text"
      },
      "source": [
        "#**まとめて解析**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM2VMXltwBs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e40c4356-456c-4bb0-d718-74cc08389eb7"
      },
      "source": [
        "# 出力名を記入\n",
        "out_name = \"ResNet50_nonpretrained_256\"\n",
        "\n",
        "#create data_dir_list\n",
        "data_dir = '/content/drive/My Drive/crossvalidation/256'\n",
        "fold = len(os.listdir(data_dir))\n",
        "print(str(fold)+'-fold cross validation')\n",
        "\n",
        "\n",
        "data_dir_list = [0]*fold\n",
        "\n",
        "for i in range(fold):\n",
        "    data_dir_list[i] = data_dir + '/' + str(i)\n",
        "    print(data_dir_list[i])\n",
        "\n",
        "#create roc_label_list\n",
        "roc_label_list = [0]*fold\n",
        "roc_label_list = list(range(fold))\n",
        "print(roc_label_list)\n",
        "\n",
        "\n",
        "\n",
        "df = make_csv(roc_label_list)\n",
        "\n",
        "label_list_list, model_pred_prob_list, Y_TRUE, Y_SCORE = [],[],[],[]\n",
        "\n",
        "print(data_dir_list)\n",
        "print(roc_label_list)\n",
        "\n",
        "for i, t in enumerate(zip(data_dir_list, roc_label_list)):\n",
        "\n",
        "    image_datasets, dataloaders, dataset_sizes, class_names, device = pre_process(t[0]) #path\n",
        "    inputs, classes = getBatch(dataloaders)\n",
        "    model_ft, criterion, optimizer_ft = convnet()\n",
        "    training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50)  \n",
        "    TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob = evaluation(model_ft, '/content/drive/My Drive/Grav_bootcamp/Posttrain_250px')\n",
        "    roc_auc, y_true, y_score = calculate_auc(label_list, model_pred_prob)\n",
        "    Y_TRUE.append(y_true)\n",
        "    Y_SCORE.append(y_score)\n",
        "    df = write_csv(df, i,TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, roc_auc) #numberをcsvの行として指定\n",
        "\n",
        "    label_list_list.append(label_list)\n",
        "    model_pred_prob_list.append(model_pred_prob)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#Draw ROC curve\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list))\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "#それぞれの項目の平均を計算しcsvに追記する\n",
        "df.iloc[0:4,fold], df.iloc[9,fold]   = df.mean(axis=1)[0:4], df.mean(axis=1)[9] \n",
        "df.iloc[0:10,fold+1] = df.std(axis=1)[0:10]\n",
        "TP,TN,FP,FN = df.mean(axis=1)[0:4]\n",
        "df.iloc[4:9,fold] = calculateAccuracy (TP, TN, FP, FN)\n",
        "print(df)\n",
        "\n",
        "# CSVとして出力\n",
        "df.to_csv(\"/content/drive/My Drive/crossvalidation/crossvalidation_csv/crossvalidation_\" + out_name + \".csv\",encoding=\"shift_jis\")\n",
        "\n",
        "#ROC_curveを保存\n",
        "fig.savefig(\"/content/drive/My Drive/crossvalidation/crossvalidation_ROCfigure/crossvalidation_\" + out_name +\".png\")\n",
        "\n",
        "#Save ROC data\n",
        "with open(\"/content/drive/My Drive/crossvalidation/crossvalidation_ROCdata/ROCdata_\"+out_name+\".csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for i, t in enumerate(zip(Y_TRUE, Y_SCORE)):\n",
        "        writer.writerow([t[0],t[1]])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5-fold cross validation\n",
            "/content/drive/My Drive/crossvalidation/256/0\n",
            "/content/drive/My Drive/crossvalidation/256/1\n",
            "/content/drive/My Drive/crossvalidation/256/2\n",
            "/content/drive/My Drive/crossvalidation/256/3\n",
            "/content/drive/My Drive/crossvalidation/256/4\n",
            "[0, 1, 2, 3, 4]\n",
            "['/content/drive/My Drive/crossvalidation/256/0', '/content/drive/My Drive/crossvalidation/256/1', '/content/drive/My Drive/crossvalidation/256/2', '/content/drive/My Drive/crossvalidation/256/3', '/content/drive/My Drive/crossvalidation/256/4']\n",
            "[0, 1, 2, 3, 4, 'avg', 'std']\n",
            "['cont', 'grav']\n",
            "cont_train:102\n",
            "grav_train:102\n",
            "cont_val:26\n",
            "grav_val:26\n",
            "training data set_total：204\n",
            "validating data set_total：52\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.7926 Acc: 0.5098\n",
            "val Loss: 0.6930 Acc: 0.5769\n",
            "Validation loss decreased (inf --> 0.692995).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 1.1424 Acc: 0.5196\n",
            "val Loss: 0.8570 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.9042 Acc: 0.5343\n",
            "val Loss: 0.7205 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.7572 Acc: 0.4951\n",
            "val Loss: 0.6727 Acc: 0.5577\n",
            "Validation loss decreased (0.692995 --> 0.672699).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.7123 Acc: 0.5490\n",
            "val Loss: 0.6774 Acc: 0.5962\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.7211 Acc: 0.5196\n",
            "val Loss: 0.6589 Acc: 0.6538\n",
            "Validation loss decreased (0.672699 --> 0.658880).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.7805 Acc: 0.5098\n",
            "val Loss: 0.6739 Acc: 0.5577\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.7734 Acc: 0.5539\n",
            "val Loss: 0.6969 Acc: 0.5385\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.8247 Acc: 0.5196\n",
            "val Loss: 0.6720 Acc: 0.5192\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.6530 Acc: 0.6520\n",
            "val Loss: 0.6617 Acc: 0.6154\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.8365 Acc: 0.5490\n",
            "val Loss: 0.6937 Acc: 0.5385\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.7416 Acc: 0.5245\n",
            "val Loss: 0.7197 Acc: 0.5385\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.7302 Acc: 0.5147\n",
            "val Loss: 0.7059 Acc: 0.5385\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.8449 Acc: 0.5441\n",
            "val Loss: 0.7039 Acc: 0.5769\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.7373 Acc: 0.5882\n",
            "val Loss: 0.7139 Acc: 0.5000\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.9998 Acc: 0.4804\n",
            "val Loss: 0.7353 Acc: 0.5385\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.8002 Acc: 0.4902\n",
            "val Loss: 22.2005 Acc: 0.5000\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.7325 Acc: 0.5147\n",
            "val Loss: 5.3188 Acc: 0.5000\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.7831 Acc: 0.5098\n",
            "val Loss: 0.7662 Acc: 0.5962\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.8113 Acc: 0.5441\n",
            "val Loss: 0.8664 Acc: 0.5385\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 1.0094 Acc: 0.5490\n",
            "val Loss: 0.6870 Acc: 0.5769\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 31s\n",
            "Best val Acc: 0.653846\n",
            "number of images: 108\n",
            "12 42 40 14\n",
            "Accuracy: 0.48148148148148145\n",
            "Precision (positive predictive value): 0.46153846153846156\n",
            "Recall (sensitivity): 0.2222222222222222\n",
            "Specificity: 0.7407407407407407\n",
            "F_value: 0.3\n",
            "roc_auc: 0.5504115226337449\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:102\n",
            "grav_train:102\n",
            "cont_val:26\n",
            "grav_val:26\n",
            "training data set_total：204\n",
            "validating data set_total：52\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.2024 Acc: 0.4657\n",
            "val Loss: 0.7964 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 0.796394).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.8104 Acc: 0.5588\n",
            "val Loss: 0.9657 Acc: 0.4038\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.7509 Acc: 0.5490\n",
            "val Loss: 0.7623 Acc: 0.4423\n",
            "Validation loss decreased (0.796394 --> 0.762322).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.6828 Acc: 0.6373\n",
            "val Loss: 0.8522 Acc: 0.3846\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.7486 Acc: 0.5588\n",
            "val Loss: 0.7276 Acc: 0.5000\n",
            "Validation loss decreased (0.762322 --> 0.727647).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.7371 Acc: 0.5539\n",
            "val Loss: 0.7080 Acc: 0.4808\n",
            "Validation loss decreased (0.727647 --> 0.708002).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.6946 Acc: 0.6127\n",
            "val Loss: 0.7697 Acc: 0.4423\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.7461 Acc: 0.6520\n",
            "val Loss: 0.7388 Acc: 0.4808\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.8409 Acc: 0.6127\n",
            "val Loss: 0.8249 Acc: 0.4423\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.6902 Acc: 0.6225\n",
            "val Loss: 0.7496 Acc: 0.4808\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.7300 Acc: 0.5686\n",
            "val Loss: 0.7918 Acc: 0.4038\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.6916 Acc: 0.6275\n",
            "val Loss: 2.1096 Acc: 0.4423\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.9230 Acc: 0.5882\n",
            "val Loss: 2.3410 Acc: 0.4615\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.8318 Acc: 0.5637\n",
            "val Loss: 0.7539 Acc: 0.5192\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.7007 Acc: 0.6078\n",
            "val Loss: 0.7503 Acc: 0.4615\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.6916 Acc: 0.5931\n",
            "val Loss: 0.7792 Acc: 0.4423\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.6850 Acc: 0.5588\n",
            "val Loss: 0.7409 Acc: 0.5385\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.6533 Acc: 0.6127\n",
            "val Loss: 0.8276 Acc: 0.5385\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.6584 Acc: 0.6324\n",
            "val Loss: 0.7825 Acc: 0.5000\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.6809 Acc: 0.5784\n",
            "val Loss: 0.8391 Acc: 0.4038\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.7750 Acc: 0.6176\n",
            "val Loss: 2.2421 Acc: 0.5192\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 30s\n",
            "Best val Acc: 0.538462\n",
            "number of images: 108\n",
            "23 31 34 20\n",
            "Accuracy: 0.5277777777777778\n",
            "Precision (positive predictive value): 0.5348837209302325\n",
            "Recall (sensitivity): 0.42592592592592593\n",
            "Specificity: 0.6296296296296297\n",
            "F_value: 0.4742268041237113\n",
            "roc_auc: 0.5805898491083678\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:102\n",
            "grav_train:102\n",
            "cont_val:26\n",
            "grav_val:26\n",
            "training data set_total：204\n",
            "validating data set_total：52\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.3415 Acc: 0.4706\n",
            "val Loss: 1.0646 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 1.064597).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.9120 Acc: 0.4363\n",
            "val Loss: 1.1879 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.8272 Acc: 0.4902\n",
            "val Loss: 1.6508 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.9296 Acc: 0.5049\n",
            "val Loss: 2.3077 Acc: 0.5192\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.7884 Acc: 0.5098\n",
            "val Loss: 0.7467 Acc: 0.4808\n",
            "Validation loss decreased (1.064597 --> 0.746661).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.8202 Acc: 0.5343\n",
            "val Loss: 0.6983 Acc: 0.5962\n",
            "Validation loss decreased (0.746661 --> 0.698292).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.7229 Acc: 0.5343\n",
            "val Loss: 2.0612 Acc: 0.4808\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.8387 Acc: 0.5441\n",
            "val Loss: 0.6526 Acc: 0.6154\n",
            "Validation loss decreased (0.698292 --> 0.652568).  Saving model ...\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.7531 Acc: 0.5343\n",
            "val Loss: 0.6835 Acc: 0.5577\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.8230 Acc: 0.4706\n",
            "val Loss: 0.7327 Acc: 0.5577\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.7540 Acc: 0.5735\n",
            "val Loss: 0.7432 Acc: 0.4615\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.7251 Acc: 0.5588\n",
            "val Loss: 0.7160 Acc: 0.5000\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.7461 Acc: 0.5539\n",
            "val Loss: 0.6681 Acc: 0.5962\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.6833 Acc: 0.6029\n",
            "val Loss: 1.1650 Acc: 0.5192\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.8234 Acc: 0.6078\n",
            "val Loss: 0.7495 Acc: 0.4231\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.6678 Acc: 0.6176\n",
            "val Loss: 0.7101 Acc: 0.5000\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.7164 Acc: 0.5637\n",
            "val Loss: 0.7562 Acc: 0.5192\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.6696 Acc: 0.5931\n",
            "val Loss: 0.6773 Acc: 0.6154\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.7072 Acc: 0.6176\n",
            "val Loss: 0.9115 Acc: 0.5000\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.7736 Acc: 0.5441\n",
            "val Loss: 1.2128 Acc: 0.4423\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.6985 Acc: 0.5588\n",
            "val Loss: 1.0672 Acc: 0.4808\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.6814 Acc: 0.5588\n",
            "val Loss: 0.6689 Acc: 0.6346\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.6595 Acc: 0.6324\n",
            "val Loss: 0.6768 Acc: 0.5769\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 37s\n",
            "Best val Acc: 0.634615\n",
            "number of images: 108\n",
            "22 32 26 28\n",
            "Accuracy: 0.4444444444444444\n",
            "Precision (positive predictive value): 0.44\n",
            "Recall (sensitivity): 0.4074074074074074\n",
            "Specificity: 0.48148148148148145\n",
            "F_value: 0.4230769230769231\n",
            "roc_auc: 0.483196159122085\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:103\n",
            "grav_train:103\n",
            "cont_val:25\n",
            "grav_val:25\n",
            "training data set_total：206\n",
            "validating data set_total：50\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.7278 Acc: 0.5146\n",
            "val Loss: 0.8814 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 0.881363).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 1.1925 Acc: 0.4709\n",
            "val Loss: 0.9344 Acc: 0.4200\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.9704 Acc: 0.5291\n",
            "val Loss: 0.7960 Acc: 0.5000\n",
            "Validation loss decreased (0.881363 --> 0.795955).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.7570 Acc: 0.5243\n",
            "val Loss: 0.6842 Acc: 0.5600\n",
            "Validation loss decreased (0.795955 --> 0.684180).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.7022 Acc: 0.5485\n",
            "val Loss: 0.6693 Acc: 0.6200\n",
            "Validation loss decreased (0.684180 --> 0.669297).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.6929 Acc: 0.5388\n",
            "val Loss: 0.7137 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.7349 Acc: 0.5340\n",
            "val Loss: 0.6688 Acc: 0.6200\n",
            "Validation loss decreased (0.669297 --> 0.668767).  Saving model ...\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.7631 Acc: 0.5388\n",
            "val Loss: 0.7303 Acc: 0.5200\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.6958 Acc: 0.6214\n",
            "val Loss: 0.6943 Acc: 0.5600\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.6574 Acc: 0.6311\n",
            "val Loss: 0.7187 Acc: 0.5400\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.7117 Acc: 0.6359\n",
            "val Loss: 0.7639 Acc: 0.5000\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.8082 Acc: 0.5680\n",
            "val Loss: 0.7929 Acc: 0.4600\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.9560 Acc: 0.5437\n",
            "val Loss: 0.7793 Acc: 0.4800\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.7756 Acc: 0.6165\n",
            "val Loss: 0.7059 Acc: 0.5400\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.6807 Acc: 0.6165\n",
            "val Loss: 0.7796 Acc: 0.5000\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.6815 Acc: 0.5971\n",
            "val Loss: 0.7280 Acc: 0.5600\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.7108 Acc: 0.6117\n",
            "val Loss: 0.7384 Acc: 0.5000\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.7195 Acc: 0.5728\n",
            "val Loss: 0.7351 Acc: 0.5600\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.6903 Acc: 0.6262\n",
            "val Loss: 2.6085 Acc: 0.5200\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.8302 Acc: 0.5680\n",
            "val Loss: 5.8850 Acc: 0.4800\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.7694 Acc: 0.5825\n",
            "val Loss: 0.8935 Acc: 0.5800\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.8128 Acc: 0.5485\n",
            "val Loss: 1.4088 Acc: 0.4800\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 33s\n",
            "Best val Acc: 0.620000\n",
            "number of images: 108\n",
            "26 28 34 20\n",
            "Accuracy: 0.5555555555555556\n",
            "Precision (positive predictive value): 0.5652173913043478\n",
            "Recall (sensitivity): 0.48148148148148145\n",
            "Specificity: 0.6296296296296297\n",
            "F_value: 0.52\n",
            "roc_auc: 0.5655006858710563\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:103\n",
            "grav_train:103\n",
            "cont_val:25\n",
            "grav_val:25\n",
            "training data set_total：206\n",
            "validating data set_total：50\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.8860 Acc: 0.4660\n",
            "val Loss: 1.3017 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 1.301723).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.7656 Acc: 0.5049\n",
            "val Loss: 0.9421 Acc: 0.5000\n",
            "Validation loss decreased (1.301723 --> 0.942077).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.8092 Acc: 0.5680\n",
            "val Loss: 0.6911 Acc: 0.5600\n",
            "Validation loss decreased (0.942077 --> 0.691064).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.8424 Acc: 0.5291\n",
            "val Loss: 1.3106 Acc: 0.6200\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.8442 Acc: 0.5291\n",
            "val Loss: 0.6618 Acc: 0.6400\n",
            "Validation loss decreased (0.691064 --> 0.661797).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.7820 Acc: 0.5825\n",
            "val Loss: 0.7790 Acc: 0.5200\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.6803 Acc: 0.5922\n",
            "val Loss: 0.7321 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.7201 Acc: 0.6214\n",
            "val Loss: 1.1044 Acc: 0.6000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.8392 Acc: 0.5388\n",
            "val Loss: 7.3702 Acc: 0.5200\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.9382 Acc: 0.5388\n",
            "val Loss: 0.7624 Acc: 0.5000\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.7441 Acc: 0.5874\n",
            "val Loss: 0.6790 Acc: 0.4800\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.7578 Acc: 0.5631\n",
            "val Loss: 0.6716 Acc: 0.5200\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.7389 Acc: 0.5485\n",
            "val Loss: 0.8282 Acc: 0.5400\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.7327 Acc: 0.5534\n",
            "val Loss: 0.6901 Acc: 0.6400\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.7454 Acc: 0.5777\n",
            "val Loss: 0.7076 Acc: 0.5600\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.7173 Acc: 0.5583\n",
            "val Loss: 0.7104 Acc: 0.5400\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.7249 Acc: 0.5437\n",
            "val Loss: 0.6757 Acc: 0.5600\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.7335 Acc: 0.5534\n",
            "val Loss: 0.7158 Acc: 0.5400\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.7298 Acc: 0.5437\n",
            "val Loss: 0.6677 Acc: 0.6000\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.6802 Acc: 0.5777\n",
            "val Loss: 0.6868 Acc: 0.5000\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 24s\n",
            "Best val Acc: 0.640000\n",
            "number of images: 108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "19 35 27 27\n",
            "Accuracy: 0.42592592592592593\n",
            "Precision (positive predictive value): 0.41304347826086957\n",
            "Recall (sensitivity): 0.35185185185185186\n",
            "Specificity: 0.5\n",
            "F_value: 0.38\n",
            "roc_auc: 0.45404663923182437\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                  0         1         2         3         4  \\\n",
            "TP                               12        23        22        26        19   \n",
            "TN                               40        34        26        34        27   \n",
            "FP                               14        20        28        20        27   \n",
            "FN                               42        31        32        28        35   \n",
            "Accuracy                   0.481481  0.527778  0.444444  0.555556  0.425926   \n",
            "Positive predictive value  0.461538  0.534884      0.44  0.565217  0.413043   \n",
            "sensitity                  0.222222  0.425926  0.407407  0.481481  0.351852   \n",
            "specificity                0.740741   0.62963  0.481481   0.62963       0.5   \n",
            "F-value                         0.3  0.474227  0.423077      0.52      0.38   \n",
            "roc_auc                    0.550412   0.58059  0.483196  0.565501  0.454047   \n",
            "\n",
            "                                avg        std  \n",
            "TP                             20.4    4.75815  \n",
            "TN                             32.2    5.15364  \n",
            "FP                             21.8    5.15364  \n",
            "FN                             33.6    4.75815  \n",
            "Accuracy                   0.487422  0.0546218  \n",
            "Positive predictive value  0.483286  0.0645273  \n",
            "sensitity                  0.381265  0.0985143  \n",
            "specificity                0.593327   0.106703  \n",
            "F-value                    0.426256  0.0850714  \n",
            "roc_auc                    0.526749  0.0492807  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPO-b2cAT0yF",
        "colab_type": "text"
      },
      "source": [
        "#**ネットワークの保存と読み込み**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMYyFQ6ATzyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワークの保存\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c744XUtfT6xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73a923de-e875-4e71-b4b5-1ae266557981"
      },
      "source": [
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}