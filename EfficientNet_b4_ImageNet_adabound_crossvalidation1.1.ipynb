{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8b935673153d4a0587ead305faff7385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_28b59b880b1148fe9308df138eeb160f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9b7a8cb775f74040b3854e4a83e29ddc",
              "IPY_MODEL_e1ac01a59b504cee8258f7157a530868"
            ]
          }
        },
        "28b59b880b1148fe9308df138eeb160f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9b7a8cb775f74040b3854e4a83e29ddc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0e4c1e4dfb6a4b6894ad231ea98cf6f0",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 77999237,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 77999237,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af14de6ec9b640849e85971560cb1a26"
          }
        },
        "e1ac01a59b504cee8258f7157a530868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_421220f72c204a3d808c5b1409e52c1e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 74.4M/74.4M [00:02&lt;00:00, 26.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7afa15a5dd5a4f3cb69eef9efb2b09b6"
          }
        },
        "0e4c1e4dfb6a4b6894ad231ea98cf6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af14de6ec9b640849e85971560cb1a26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "421220f72c204a3d808c5b1409e52c1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7afa15a5dd5a4f3cb69eef9efb2b09b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/EfficientNet_b4_ImageNet_adabound_crossvalidation1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a4ZBlqPNdU",
        "colab_type": "text"
      },
      "source": [
        "#**GravCont: EfficientNet_b4_ImageNet**\n",
        "ValidationとTestに分けて解析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgM-Y7SVPNkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "73256371-9d5e-4c0b-dd8e-5569a8c3895a"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.6/dist-packages (0.0.1a15)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Random Seed:  1234\n",
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzp_09fWPNoU",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sJV06qPNsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(data_dir):\n",
        "    # 入力画像の前処理をするクラス\n",
        "    # 訓練時と推論時で処理が異なる\n",
        "\n",
        "    \"\"\"\n",
        "        画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "        画像のサイズをリサイズし、色を標準化する。\n",
        "        訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        resize : int\n",
        "            リサイズ先の画像の大きさ。\n",
        "        mean : (R, G, B)\n",
        "            各色チャネルの平均値。\n",
        "        std : (R, G, B)\n",
        "            各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    data_dir = data_dir\n",
        "    n_samples = len(data_dir)\n",
        "\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in ['train', 'val']}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                                shuffle=True, num_workers=4)\n",
        "                  for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "    print(class_names)\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_train:\"+str(len(os.listdir(path=data_dir + '/train/'+class_names[k]))))\n",
        "        k+=1\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_val:\"+str(len(os.listdir(path=data_dir + '/val/'+class_names[k]))))\n",
        "        k+=1\n",
        "\n",
        "    print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "    print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "    \n",
        "    return image_datasets, dataloaders, dataset_sizes, class_names, device\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "def getBatch(dataloaders):    \n",
        "    # Get a batch of training data\n",
        "    inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "    # Make a grid from batch\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "    #imshow(out, title=[class_names[x] for x in classes])\n",
        "    return(inputs, classes)\n",
        "\n",
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "def convnet():\n",
        "    model_ft = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "    num_ftrs = model_ft._fc.in_features\n",
        "    model_ft._fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    optimizer_ft = optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=0,\n",
        "        amsbound=False,\n",
        "    )\n",
        "    return (model_ft, criterion, optimizer_ft)\n",
        "\n",
        "\n",
        "def training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50):\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=patience, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "    \"\"\"\n",
        "    #probalilityを計算する\n",
        "    pred_prob = torch.topk(nn.Softmax(dim=1)(output), 1)[0]\n",
        "    pred_class = torch.topk(nn.Softmax(dim=1)(output), 1)[1]\n",
        "    if pred_class == 1:\n",
        "        pred_prob = pred_prob\n",
        "    elif pred_class == 0:\n",
        "        pred_prob = 1- pred_prob\n",
        "    return(model_pred, pred_prob)  #class_nameの番号で出力される\n",
        "    \"\"\"\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\"\"\"\n",
        "・True positive (TN)\n",
        "・False positive (FP)\n",
        "・True negative (TN)\n",
        "・False negative (FN)\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "Precision = TP/(FP + TP) ※positive predictive value\n",
        "Recall = TP/(TP + FN)　※sensitivity\n",
        "Specificity = TN/(FP + TN)\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)\n",
        "\"\"\"\n",
        "\n",
        "def evaluation(model_ft, testset_dir):\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #testデータセット内のファイル名を取得\n",
        "    image_path = glob.glob(testset_dir + \"/*/*\")\n",
        "    #random.shuffle(image_path)  #表示順をランダムにする\n",
        "    print('number of images: ' +str(len(image_path)))\n",
        "\n",
        "\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1     \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    return TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob\n",
        "\n",
        "\n",
        "def make_csv(roc_label_list):\n",
        "    #csvのdata tableを作成\n",
        "    pd.set_option('display.max_rows', 800)  # 省略なしで表示\n",
        "    #columns1 = [\"EfficientNet_32\", \"EfficientNet_64\", \"EfficientNet_128\", \"EfficientNet_256\", \"EfficientNet_512\", \"EfficientNet_558\"]\n",
        "    columns1 = roc_label_list\n",
        "    index1 = [\"TP\",\"TN\",\"FP\",\"FN\",\"Accuracy\",\"Positive predictive value\",\"sensitity\",\"specificity\",\"F-value\",\"roc_auc\"]\n",
        "    df = pd.DataFrame(index=index1, columns=columns1)\n",
        "    return df\n",
        "\n",
        "def write_csv(df, col, TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc):\n",
        "    df.iloc[0:10, col] = TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc \n",
        "    #print(df)\n",
        "\n",
        "    # CSVとして出力\n",
        "    #df2.to_csv(\"/content/drive/My Drive/Grav_bootcamp/Posttrain_model_eval_result.csv\",encoding=\"shift_jis\")\n",
        "    return df\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == 'cont':\n",
        "                  y_true.append(0)\n",
        "            elif i == 'grav':\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == 'cont':\n",
        "              y_true.append(0)\n",
        "        elif i == 'grav':\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "def calcurate_ave_std(df, fold):\n",
        "    for i in range(5):\n",
        "        df.iloc[i,fold] = df[i,0:5].mean \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGfUwQXv6Jc",
        "colab_type": "text"
      },
      "source": [
        "#**まとめて解析**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObCZwRvYCPTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "d1ab7cdd-3a17-4e71-b286-ec6788b3594d"
      },
      "source": [
        "#create data_dir_list\n",
        "data_dir = '/content/drive/My Drive/gravcont_crossvalidation'\n",
        "fold = len(os.listdir(data_dir))\n",
        "print(fold)\n",
        "\n",
        "data_dir_list = [0]*fold\n",
        "\n",
        "for i in range(fold):\n",
        "    data_dir_list[i] = data_dir + '/' + str(i)\n",
        "    print(data_dir_list[i])\n",
        "\n",
        "#create roc_label_list\n",
        "roc_label_list = [0]*fold\n",
        "roc_label_list = list(range(fold))\n",
        "print(roc_label_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "/content/drive/My Drive/gravcont_crossvalidation/0\n",
            "/content/drive/My Drive/gravcont_crossvalidation/1\n",
            "/content/drive/My Drive/gravcont_crossvalidation/2\n",
            "/content/drive/My Drive/gravcont_crossvalidation/3\n",
            "/content/drive/My Drive/gravcont_crossvalidation/4\n",
            "[0, 1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM2VMXltwBs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8b935673153d4a0587ead305faff7385",
            "28b59b880b1148fe9308df138eeb160f",
            "9b7a8cb775f74040b3854e4a83e29ddc",
            "e1ac01a59b504cee8258f7157a530868",
            "0e4c1e4dfb6a4b6894ad231ea98cf6f0",
            "af14de6ec9b640849e85971560cb1a26",
            "421220f72c204a3d808c5b1409e52c1e",
            "7afa15a5dd5a4f3cb69eef9efb2b09b6"
          ]
        },
        "outputId": "72efcfdc-7b75-42cb-fb9e-3d4f8b8e569b"
      },
      "source": [
        "df = make_csv(roc_label_list)\n",
        "\n",
        "label_list_list, model_pred_prob_list, Y_TRUE, Y_SCORE = [],[],[],[]\n",
        "\n",
        "print(data_dir_list)\n",
        "print(roc_label_list)\n",
        "\n",
        "for i, t in enumerate(zip(data_dir_list, roc_label_list)):\n",
        "\n",
        "    image_datasets, dataloaders, dataset_sizes, class_names, device = pre_process(t[0]) #path\n",
        "    inputs, classes = getBatch(dataloaders)\n",
        "    model_ft, criterion, optimizer_ft = convnet()\n",
        "    training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50)  \n",
        "    TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob = evaluation(model_ft, '/content/drive/My Drive/Grav_bootcamp/Posttrain_250px')\n",
        "    roc_auc, y_true, y_score = calculate_auc(label_list, model_pred_prob)\n",
        "    Y_TRUE.append(y_true)\n",
        "    Y_SCORE.append(y_score)\n",
        "    df = write_csv(df, i,TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, roc_auc) #numberをcsvの行として指定\n",
        "\n",
        "    label_list_list.append(label_list)\n",
        "    model_pred_prob_list.append(model_pred_prob)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#Draw ROC curve\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list))\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "#それぞれの項目の平均を計算しcsvに追記する\n",
        "df.iloc[0:4,fold], df.iloc[9,fold]   = df.mean(axis=1)[0:4], df.mean(axis=1)[9] \n",
        "df.iloc[0:10,fold+1] = df.std(axis=1)[0:10]\n",
        "TP,TN,FP,FN = df.mean(axis=1)[0:4]\n",
        "df.iloc[4:9,fold] = calculateAccuracy (TP, TN, FP, FN)\n",
        "print(df)\n",
        "\n",
        "# CSVとして出力\n",
        "df.to_csv(\"/content/drive/My Drive/Grav_bootcamp/crossvaridation_efficientNet_ImageNet.csv\",encoding=\"shift_jis\")\n",
        "\n",
        "#ROC_curveを保存\n",
        "fig.savefig(\"/content/drive/My Drive/Grav_bootcamp/img.png\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/gravcont_crossvalidation/0', '/content/drive/My Drive/gravcont_crossvalidation/1', '/content/drive/My Drive/gravcont_crossvalidation/2', '/content/drive/My Drive/gravcont_crossvalidation/3', '/content/drive/My Drive/gravcont_crossvalidation/4']\n",
            "[0, 1, 2, 3, 4]\n",
            "['cont', 'grav']\n",
            "cont_train:223\n",
            "grav_train:223\n",
            "cont_val:56\n",
            "grav_val:56\n",
            "training data set_total：446\n",
            "validating data set_total：112\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b935673153d4a0587ead305faff7385",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=77999237.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch_optimizer/adabound.py:142: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.5995 Acc: 0.6704\n",
            "val Loss: 0.7425 Acc: 0.4732\n",
            "Validation loss decreased (inf --> 0.742500).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4027 Acc: 0.8049\n",
            "val Loss: 0.4765 Acc: 0.7500\n",
            "Validation loss decreased (0.742500 --> 0.476498).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3335 Acc: 0.8520\n",
            "val Loss: 2.3798 Acc: 0.6339\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.3345 Acc: 0.8587\n",
            "val Loss: 1.3162 Acc: 0.7768\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2224 Acc: 0.9081\n",
            "val Loss: 1.5519 Acc: 0.7946\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1393 Acc: 0.9484\n",
            "val Loss: 1.3306 Acc: 0.7500\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1326 Acc: 0.9507\n",
            "val Loss: 2.0171 Acc: 0.7321\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1925 Acc: 0.9260\n",
            "val Loss: 0.9460 Acc: 0.7411\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1380 Acc: 0.9439\n",
            "val Loss: 0.8101 Acc: 0.7857\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.0580 Acc: 0.9731\n",
            "val Loss: 1.3058 Acc: 0.7321\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0384 Acc: 0.9843\n",
            "val Loss: 1.1585 Acc: 0.7232\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0385 Acc: 0.9888\n",
            "val Loss: 1.4095 Acc: 0.7232\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.1255 Acc: 0.9664\n",
            "val Loss: 0.7132 Acc: 0.7321\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0847 Acc: 0.9753\n",
            "val Loss: 0.4279 Acc: 0.8125\n",
            "Validation loss decreased (0.476498 --> 0.427908).  Saving model ...\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0805 Acc: 0.9709\n",
            "val Loss: 0.6666 Acc: 0.7321\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0350 Acc: 0.9910\n",
            "val Loss: 1.1525 Acc: 0.7054\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0349 Acc: 0.9865\n",
            "val Loss: 1.8131 Acc: 0.6518\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0848 Acc: 0.9686\n",
            "val Loss: 0.8470 Acc: 0.8214\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.1183 Acc: 0.9641\n",
            "val Loss: 0.8131 Acc: 0.7500\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0457 Acc: 0.9865\n",
            "val Loss: 0.7248 Acc: 0.7589\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0391 Acc: 0.9888\n",
            "val Loss: 0.8020 Acc: 0.7946\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.0665 Acc: 0.9865\n",
            "val Loss: 0.7334 Acc: 0.7589\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.1460 Acc: 0.9484\n",
            "val Loss: 0.5016 Acc: 0.7946\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0855 Acc: 0.9865\n",
            "val Loss: 0.6535 Acc: 0.7232\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.0651 Acc: 0.9821\n",
            "val Loss: 0.6859 Acc: 0.7411\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.0333 Acc: 0.9955\n",
            "val Loss: 0.6630 Acc: 0.7679\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.0174 Acc: 0.9955\n",
            "val Loss: 0.8264 Acc: 0.7768\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.0082 Acc: 1.0000\n",
            "val Loss: 0.8659 Acc: 0.7857\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.0048 Acc: 1.0000\n",
            "val Loss: 0.9059 Acc: 0.7857\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 12m 28s\n",
            "Best val Acc: 0.821429\n",
            "number of images: 108\n",
            "39 15 50 4\n",
            "Accuracy: 0.8240740740740741\n",
            "Precision (positive predictive value): 0.9069767441860465\n",
            "Recall (sensitivity): 0.7222222222222222\n",
            "Specificity: 0.9259259259259259\n",
            "F_value: 0.8041237113402061\n",
            "roc_auc: 0.9228395061728395\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:223\n",
            "grav_train:223\n",
            "cont_val:56\n",
            "grav_val:56\n",
            "training data set_total：446\n",
            "validating data set_total：112\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.5944 Acc: 0.6794\n",
            "val Loss: 0.8186 Acc: 0.5446\n",
            "Validation loss decreased (inf --> 0.818597).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4154 Acc: 0.8072\n",
            "val Loss: 1.0641 Acc: 0.6964\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3501 Acc: 0.8610\n",
            "val Loss: 4.0675 Acc: 0.5357\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2377 Acc: 0.9058\n",
            "val Loss: 4.4082 Acc: 0.5982\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2303 Acc: 0.9148\n",
            "val Loss: 0.5253 Acc: 0.8214\n",
            "Validation loss decreased (0.818597 --> 0.525319).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.2110 Acc: 0.9103\n",
            "val Loss: 0.9277 Acc: 0.8304\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1630 Acc: 0.9417\n",
            "val Loss: 0.5424 Acc: 0.8393\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1234 Acc: 0.9552\n",
            "val Loss: 0.8339 Acc: 0.7857\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1425 Acc: 0.9484\n",
            "val Loss: 0.5997 Acc: 0.8125\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.0852 Acc: 0.9709\n",
            "val Loss: 0.5352 Acc: 0.8661\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.1188 Acc: 0.9484\n",
            "val Loss: 0.4834 Acc: 0.8661\n",
            "Validation loss decreased (0.525319 --> 0.483379).  Saving model ...\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0487 Acc: 0.9865\n",
            "val Loss: 0.7008 Acc: 0.8036\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0533 Acc: 0.9865\n",
            "val Loss: 0.7403 Acc: 0.8393\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.1372 Acc: 0.9484\n",
            "val Loss: 0.4541 Acc: 0.8571\n",
            "Validation loss decreased (0.483379 --> 0.454094).  Saving model ...\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.1201 Acc: 0.9507\n",
            "val Loss: 0.4696 Acc: 0.8304\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0401 Acc: 0.9933\n",
            "val Loss: 0.4994 Acc: 0.8482\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0414 Acc: 0.9933\n",
            "val Loss: 0.5411 Acc: 0.8214\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0723 Acc: 0.9776\n",
            "val Loss: 0.7062 Acc: 0.7857\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0749 Acc: 0.9731\n",
            "val Loss: 0.6587 Acc: 0.8036\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0340 Acc: 0.9888\n",
            "val Loss: 0.6761 Acc: 0.8304\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0259 Acc: 0.9910\n",
            "val Loss: 0.8130 Acc: 0.8036\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.0180 Acc: 0.9933\n",
            "val Loss: 0.7023 Acc: 0.8214\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0584 Acc: 0.9776\n",
            "val Loss: 0.7153 Acc: 0.8304\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0440 Acc: 0.9910\n",
            "val Loss: 0.6080 Acc: 0.8571\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.0695 Acc: 0.9753\n",
            "val Loss: 1.0099 Acc: 0.7232\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.1134 Acc: 0.9596\n",
            "val Loss: 0.7384 Acc: 0.7768\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.0617 Acc: 0.9798\n",
            "val Loss: 0.6481 Acc: 0.8125\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.0607 Acc: 0.9843\n",
            "val Loss: 0.7665 Acc: 0.7768\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.0722 Acc: 0.9686\n",
            "val Loss: 0.8472 Acc: 0.7054\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 12m 1s\n",
            "Best val Acc: 0.866071\n",
            "number of images: 108\n",
            "48 6 46 8\n",
            "Accuracy: 0.8703703703703703\n",
            "Precision (positive predictive value): 0.8571428571428571\n",
            "Recall (sensitivity): 0.8888888888888888\n",
            "Specificity: 0.8518518518518519\n",
            "F_value: 0.8727272727272727\n",
            "roc_auc: 0.9451303155006858\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:223\n",
            "grav_train:223\n",
            "cont_val:56\n",
            "grav_val:56\n",
            "training data set_total：446\n",
            "validating data set_total：112\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.6390 Acc: 0.6435\n",
            "val Loss: 0.5191 Acc: 0.7143\n",
            "Validation loss decreased (inf --> 0.519120).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4701 Acc: 0.7691\n",
            "val Loss: 1.4511 Acc: 0.6429\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3429 Acc: 0.8498\n",
            "val Loss: 0.6796 Acc: 0.7857\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2831 Acc: 0.8946\n",
            "val Loss: 0.8113 Acc: 0.8304\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2748 Acc: 0.8901\n",
            "val Loss: 0.8607 Acc: 0.8125\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1850 Acc: 0.9305\n",
            "val Loss: 0.7037 Acc: 0.7946\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1865 Acc: 0.9260\n",
            "val Loss: 0.7265 Acc: 0.7946\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1445 Acc: 0.9507\n",
            "val Loss: 0.4564 Acc: 0.8036\n",
            "Validation loss decreased (0.519120 --> 0.456442).  Saving model ...\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1115 Acc: 0.9641\n",
            "val Loss: 0.6236 Acc: 0.8125\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1147 Acc: 0.9664\n",
            "val Loss: 1.5036 Acc: 0.6161\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.1146 Acc: 0.9507\n",
            "val Loss: 0.7666 Acc: 0.7768\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0598 Acc: 0.9776\n",
            "val Loss: 0.6688 Acc: 0.8125\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0370 Acc: 0.9888\n",
            "val Loss: 0.6344 Acc: 0.7768\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.1045 Acc: 0.9641\n",
            "val Loss: 0.6636 Acc: 0.7946\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0328 Acc: 0.9933\n",
            "val Loss: 0.8260 Acc: 0.7321\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0283 Acc: 0.9910\n",
            "val Loss: 0.7828 Acc: 0.7857\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0246 Acc: 0.9933\n",
            "val Loss: 0.7870 Acc: 0.7500\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0126 Acc: 0.9978\n",
            "val Loss: 0.9128 Acc: 0.7768\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0121 Acc: 0.9978\n",
            "val Loss: 0.8956 Acc: 0.7857\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0096 Acc: 0.9955\n",
            "val Loss: 1.2100 Acc: 0.7321\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0329 Acc: 0.9843\n",
            "val Loss: 1.6035 Acc: 0.7143\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.1541 Acc: 0.9552\n",
            "val Loss: 0.6791 Acc: 0.7232\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.1219 Acc: 0.9529\n",
            "val Loss: 1.1954 Acc: 0.7589\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 9m 39s\n",
            "Best val Acc: 0.830357\n",
            "number of images: 108\n",
            "47 7 47 7\n",
            "Accuracy: 0.8703703703703703\n",
            "Precision (positive predictive value): 0.8703703703703703\n",
            "Recall (sensitivity): 0.8703703703703703\n",
            "Specificity: 0.8703703703703703\n",
            "F_value: 0.8703703703703703\n",
            "roc_auc: 0.9262688614540466\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:223\n",
            "grav_train:223\n",
            "cont_val:56\n",
            "grav_val:56\n",
            "training data set_total：446\n",
            "validating data set_total：112\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.6267 Acc: 0.6637\n",
            "val Loss: 0.7083 Acc: 0.5982\n",
            "Validation loss decreased (inf --> 0.708273).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4090 Acc: 0.8206\n",
            "val Loss: 8.1823 Acc: 0.6339\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3495 Acc: 0.8475\n",
            "val Loss: 2.1831 Acc: 0.7768\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2898 Acc: 0.8901\n",
            "val Loss: 1.3122 Acc: 0.8036\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2548 Acc: 0.9193\n",
            "val Loss: 0.8450 Acc: 0.8304\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1591 Acc: 0.9507\n",
            "val Loss: 0.7074 Acc: 0.8571\n",
            "Validation loss decreased (0.708273 --> 0.707399).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1854 Acc: 0.9260\n",
            "val Loss: 1.4384 Acc: 0.7946\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1545 Acc: 0.9439\n",
            "val Loss: 0.7250 Acc: 0.7768\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1111 Acc: 0.9574\n",
            "val Loss: 0.6949 Acc: 0.8214\n",
            "Validation loss decreased (0.707399 --> 0.694941).  Saving model ...\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1173 Acc: 0.9484\n",
            "val Loss: 0.4147 Acc: 0.8482\n",
            "Validation loss decreased (0.694941 --> 0.414731).  Saving model ...\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0693 Acc: 0.9821\n",
            "val Loss: 0.5467 Acc: 0.7946\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.1097 Acc: 0.9552\n",
            "val Loss: 0.4876 Acc: 0.8750\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0409 Acc: 0.9843\n",
            "val Loss: 0.4884 Acc: 0.8393\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0688 Acc: 0.9753\n",
            "val Loss: 0.5524 Acc: 0.8571\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0368 Acc: 0.9865\n",
            "val Loss: 0.5888 Acc: 0.8304\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0283 Acc: 0.9910\n",
            "val Loss: 0.8339 Acc: 0.8125\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0116 Acc: 0.9978\n",
            "val Loss: 0.6878 Acc: 0.8125\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0452 Acc: 0.9910\n",
            "val Loss: 0.6644 Acc: 0.7946\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0374 Acc: 0.9888\n",
            "val Loss: 0.6400 Acc: 0.8214\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0421 Acc: 0.9798\n",
            "val Loss: 0.6570 Acc: 0.8125\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0454 Acc: 0.9910\n",
            "val Loss: 0.7959 Acc: 0.7946\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.1148 Acc: 0.9552\n",
            "val Loss: 0.4837 Acc: 0.8214\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0520 Acc: 0.9865\n",
            "val Loss: 0.3917 Acc: 0.8750\n",
            "Validation loss decreased (0.414731 --> 0.391716).  Saving model ...\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0395 Acc: 0.9821\n",
            "val Loss: 0.4277 Acc: 0.8750\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.0478 Acc: 0.9798\n",
            "val Loss: 0.5424 Acc: 0.8304\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.0240 Acc: 0.9955\n",
            "val Loss: 0.4447 Acc: 0.8482\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.0200 Acc: 0.9933\n",
            "val Loss: 0.5097 Acc: 0.8929\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.0157 Acc: 0.9955\n",
            "val Loss: 0.4570 Acc: 0.8571\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.0082 Acc: 0.9978\n",
            "val Loss: 0.5386 Acc: 0.8661\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.0120 Acc: 0.9955\n",
            "val Loss: 0.5170 Acc: 0.8482\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.0053 Acc: 1.0000\n",
            "val Loss: 0.5214 Acc: 0.8482\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.0093 Acc: 0.9978\n",
            "val Loss: 0.5008 Acc: 0.8393\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.0083 Acc: 0.9978\n",
            "val Loss: 0.5070 Acc: 0.8304\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.0382 Acc: 0.9888\n",
            "val Loss: 0.8240 Acc: 0.7946\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.0139 Acc: 0.9978\n",
            "val Loss: 1.0408 Acc: 0.7679\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.0167 Acc: 0.9933\n",
            "val Loss: 0.8526 Acc: 0.8036\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.0193 Acc: 0.9933\n",
            "val Loss: 0.8720 Acc: 0.8036\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.0093 Acc: 0.9978\n",
            "val Loss: 0.7908 Acc: 0.8214\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 15m 39s\n",
            "Best val Acc: 0.892857\n",
            "number of images: 108\n",
            "44 10 50 4\n",
            "Accuracy: 0.8703703703703703\n",
            "Precision (positive predictive value): 0.9166666666666666\n",
            "Recall (sensitivity): 0.8148148148148148\n",
            "Specificity: 0.9259259259259259\n",
            "F_value: 0.8627450980392156\n",
            "roc_auc: 0.9219821673525378\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:224\n",
            "grav_train:224\n",
            "cont_val:55\n",
            "grav_val:55\n",
            "training data set_total：448\n",
            "validating data set_total：110\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.5886 Acc: 0.6875\n",
            "val Loss: 0.6691 Acc: 0.7909\n",
            "Validation loss decreased (inf --> 0.669145).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4335 Acc: 0.8036\n",
            "val Loss: 1.2045 Acc: 0.7182\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3445 Acc: 0.8571\n",
            "val Loss: 1.1006 Acc: 0.8182\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2789 Acc: 0.8750\n",
            "val Loss: 0.3103 Acc: 0.9091\n",
            "Validation loss decreased (0.669145 --> 0.310346).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2035 Acc: 0.9286\n",
            "val Loss: 0.9830 Acc: 0.7909\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1727 Acc: 0.9286\n",
            "val Loss: 0.7717 Acc: 0.8455\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1786 Acc: 0.9353\n",
            "val Loss: 0.3697 Acc: 0.8909\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1161 Acc: 0.9598\n",
            "val Loss: 0.3972 Acc: 0.8818\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.0911 Acc: 0.9643\n",
            "val Loss: 0.3825 Acc: 0.9091\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.0550 Acc: 0.9821\n",
            "val Loss: 0.4613 Acc: 0.8727\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0480 Acc: 0.9844\n",
            "val Loss: 0.4594 Acc: 0.8818\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.1019 Acc: 0.9621\n",
            "val Loss: 0.4506 Acc: 0.8545\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0519 Acc: 0.9866\n",
            "val Loss: 0.5611 Acc: 0.8636\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0979 Acc: 0.9665\n",
            "val Loss: 0.6107 Acc: 0.8818\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0701 Acc: 0.9688\n",
            "val Loss: 0.4673 Acc: 0.9000\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0296 Acc: 0.9888\n",
            "val Loss: 0.5177 Acc: 0.9000\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0349 Acc: 0.9933\n",
            "val Loss: 0.3851 Acc: 0.8818\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0123 Acc: 0.9978\n",
            "val Loss: 0.4596 Acc: 0.8818\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0277 Acc: 0.9911\n",
            "val Loss: 0.3831 Acc: 0.8909\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 8m 9s\n",
            "Best val Acc: 0.909091\n",
            "number of images: 108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "45 9 50 4\n",
            "Accuracy: 0.8796296296296297\n",
            "Precision (positive predictive value): 0.9183673469387755\n",
            "Recall (sensitivity): 0.8333333333333334\n",
            "Specificity: 0.9259259259259259\n",
            "F_value: 0.8737864077669903\n",
            "roc_auc: 0.911522633744856\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                  0         1         2         3         4\n",
            "TP                               39        48        47        44        45\n",
            "TN                               50        46        47        50        50\n",
            "FP                                4         8         7         4         4\n",
            "FN                               15         6         7        10         9\n",
            "Accuracy                   0.824074   0.87037   0.87037   0.87037   0.87963\n",
            "Positive predictive value  0.906977  0.857143   0.87037  0.916667  0.918367\n",
            "sensitity                  0.722222  0.888889   0.87037  0.814815  0.833333\n",
            "specificity                0.925926  0.851852   0.87037  0.925926  0.925926\n",
            "F-value                    0.804124  0.872727   0.87037  0.862745  0.873786\n",
            "roc_auc                     0.92284   0.94513  0.926269  0.921982  0.911523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-JpdiMAAfb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save ROC data\n",
        "with open(\"/content/drive/My Drive/Grav_bootcamp/ROCdata_efficientNet_ImageNet.csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for i, t in enumerate(zip(Y_TRUE, Y_SCORE)):\n",
        "        writer.writerow([t[0],t[1]])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPO-b2cAT0yF",
        "colab_type": "text"
      },
      "source": [
        "#**ネットワークの保存と読み込み**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMYyFQ6ATzyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワークの保存\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c744XUtfT6xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73a923de-e875-4e71-b4b5-1ae266557981"
      },
      "source": [
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}