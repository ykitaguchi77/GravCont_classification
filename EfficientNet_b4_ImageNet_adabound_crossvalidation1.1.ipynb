{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "722de21b2fde47faa7c640c2946cb33b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_53f55810bac145918d315693899e398c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_28c597def6f9458ab3ccb5e4e3b50d49",
              "IPY_MODEL_d629db344b874f2a9f6f84de330360c1"
            ]
          }
        },
        "53f55810bac145918d315693899e398c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28c597def6f9458ab3ccb5e4e3b50d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cbd4dbca210f40189d8bf801ace551d3",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 77999237,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 77999237,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c4e961f7d7524e9db52267b21b438907"
          }
        },
        "d629db344b874f2a9f6f84de330360c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4ac6da1db2cc4506a953b48141990522",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 74.4M/74.4M [00:02&lt;00:00, 35.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2b177bda0977444aab60e7a1e26037ae"
          }
        },
        "cbd4dbca210f40189d8bf801ace551d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c4e961f7d7524e9db52267b21b438907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ac6da1db2cc4506a953b48141990522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2b177bda0977444aab60e7a1e26037ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/EfficientNet_b4_ImageNet_adabound_crossvalidation1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a4ZBlqPNdU",
        "colab_type": "text"
      },
      "source": [
        "#**GravCont: EfficientNet_b4_ImageNet**\n",
        "ValidationとTestに分けて解析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgM-Y7SVPNkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "9617ca95-7d0a-4867-f3b1-c69b608a1b18"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.6/dist-packages (0.0.1a15)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.6.0+cu101)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Random Seed:  1234\n",
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzp_09fWPNoU",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sJV06qPNsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(data_dir):\n",
        "    # 入力画像の前処理をするクラス\n",
        "    # 訓練時と推論時で処理が異なる\n",
        "\n",
        "    \"\"\"\n",
        "        画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "        画像のサイズをリサイズし、色を標準化する。\n",
        "        訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        resize : int\n",
        "            リサイズ先の画像の大きさ。\n",
        "        mean : (R, G, B)\n",
        "            各色チャネルの平均値。\n",
        "        std : (R, G, B)\n",
        "            各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    data_dir = data_dir\n",
        "    n_samples = len(data_dir)\n",
        "\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in ['train', 'val']}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                                shuffle=True, num_workers=4)\n",
        "                  for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "    print(class_names)\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_train:\"+str(len(os.listdir(path=data_dir + '/train/'+class_names[k]))))\n",
        "        k+=1\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_val:\"+str(len(os.listdir(path=data_dir + '/val/'+class_names[k]))))\n",
        "        k+=1\n",
        "\n",
        "    print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "    print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "    \n",
        "    return image_datasets, dataloaders, dataset_sizes, class_names, device\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "def getBatch(dataloaders):    \n",
        "    # Get a batch of training data\n",
        "    inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "    # Make a grid from batch\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "    #imshow(out, title=[class_names[x] for x in classes])\n",
        "    return(inputs, classes)\n",
        "\n",
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "def training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50):\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=patience, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "    \"\"\"\n",
        "    #probalilityを計算する\n",
        "    pred_prob = torch.topk(nn.Softmax(dim=1)(output), 1)[0]\n",
        "    pred_class = torch.topk(nn.Softmax(dim=1)(output), 1)[1]\n",
        "    if pred_class == 1:\n",
        "        pred_prob = pred_prob\n",
        "    elif pred_class == 0:\n",
        "        pred_prob = 1- pred_prob\n",
        "    return(model_pred, pred_prob)  #class_nameの番号で出力される\n",
        "    \"\"\"\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\"\"\"\n",
        "・True positive (TN)\n",
        "・False positive (FP)\n",
        "・True negative (TN)\n",
        "・False negative (FN)\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "Precision = TP/(FP + TP) ※positive predictive value\n",
        "Recall = TP/(TP + FN)　※sensitivity\n",
        "Specificity = TN/(FP + TN)\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)\n",
        "\"\"\"\n",
        "\n",
        "def evaluation(model_ft, testset_dir):\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #testデータセット内のファイル名を取得\n",
        "    image_path = glob.glob(testset_dir + \"/*/*\")\n",
        "    #random.shuffle(image_path)  #表示順をランダムにする\n",
        "    print('number of images: ' +str(len(image_path)))\n",
        "\n",
        "\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1     \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    return TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob\n",
        "\n",
        "\n",
        "def make_csv(roc_label_list):\n",
        "    #csvのdata tableを作成\n",
        "    pd.set_option('display.max_rows', 800)  # 省略なしで表示\n",
        "    #columns1 = [\"EfficientNet_32\", \"EfficientNet_64\", \"EfficientNet_128\", \"EfficientNet_256\", \"EfficientNet_512\", \"EfficientNet_558\"]\n",
        "    roc_label_list.extend([\"avg\", \"std\"])\n",
        "    index1 = [\"TP\",\"TN\",\"FP\",\"FN\",\"Accuracy\",\"Positive predictive value\",\"sensitity\",\"specificity\",\"F-value\",\"roc_auc\"]\n",
        "    df = pd.DataFrame(index=index1, columns=roc_label_list)\n",
        "    return df\n",
        "\n",
        "def write_csv(df, col, TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc):\n",
        "    df.iloc[0:10, col] = TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc \n",
        "    #print(df)\n",
        "\n",
        "    # CSVとして出力\n",
        "    #df2.to_csv(\"/content/drive/My Drive/Grav_bootcamp/Posttrain_model_eval_result.csv\",encoding=\"shift_jis\")\n",
        "    return df\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == 'cont':\n",
        "                  y_true.append(0)\n",
        "            elif i == 'grav':\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == 'cont':\n",
        "              y_true.append(0)\n",
        "        elif i == 'grav':\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "def calcurate_ave_std(df, fold):\n",
        "    for i in range(5):\n",
        "        df.iloc[i,fold] = df[i,0:5].mean \n",
        "\n",
        "def convnet():\n",
        "    model_ft = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "    num_ftrs = model_ft._fc.in_features\n",
        "    model_ft._fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    optimizer_ft = optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=0,\n",
        "        amsbound=False,\n",
        "    )\n",
        "    return (model_ft, criterion, optimizer_ft)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGfUwQXv6Jc",
        "colab_type": "text"
      },
      "source": [
        "#**まとめて解析**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM2VMXltwBs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "722de21b2fde47faa7c640c2946cb33b",
            "53f55810bac145918d315693899e398c",
            "28c597def6f9458ab3ccb5e4e3b50d49",
            "d629db344b874f2a9f6f84de330360c1",
            "cbd4dbca210f40189d8bf801ace551d3",
            "c4e961f7d7524e9db52267b21b438907",
            "4ac6da1db2cc4506a953b48141990522",
            "2b177bda0977444aab60e7a1e26037ae"
          ]
        },
        "outputId": "31f9525d-e575-4f0b-f3da-3a448fa0b315"
      },
      "source": [
        "# 出力名を記入\n",
        "out_name = \"EfficientNet_ImageNet_128\"\n",
        "\n",
        "#create data_dir_list\n",
        "data_dir = '/content/drive/My Drive/crossvalidation/128'\n",
        "fold = len(os.listdir(data_dir))\n",
        "print(str(fold)+'-fold cross validation')\n",
        "\n",
        "\n",
        "data_dir_list = [0]*fold\n",
        "\n",
        "for i in range(fold):\n",
        "    data_dir_list[i] = data_dir + '/' + str(i)\n",
        "    print(data_dir_list[i])\n",
        "\n",
        "#create roc_label_list\n",
        "roc_label_list = [0]*fold\n",
        "roc_label_list = list(range(fold))\n",
        "#print(roc_label_list)\n",
        "\n",
        "\n",
        "\n",
        "df = make_csv(roc_label_list)\n",
        "\n",
        "label_list_list, model_pred_prob_list, Y_TRUE, Y_SCORE = [],[],[],[]\n",
        "\n",
        "#print(data_dir_list)\n",
        "#print(roc_label_list)\n",
        "\n",
        "for i, t in enumerate(zip(data_dir_list, roc_label_list)):\n",
        "\n",
        "    image_datasets, dataloaders, dataset_sizes, class_names, device = pre_process(t[0]) #path\n",
        "    inputs, classes = getBatch(dataloaders)\n",
        "    model_ft, criterion, optimizer_ft = convnet()\n",
        "    training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50)  \n",
        "    TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob = evaluation(model_ft, '/content/drive/My Drive/Grav_bootcamp/Posttrain_250px')\n",
        "    roc_auc, y_true, y_score = calculate_auc(label_list, model_pred_prob)\n",
        "    Y_TRUE.append(y_true)\n",
        "    Y_SCORE.append(y_score)\n",
        "    df = write_csv(df, i,TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, roc_auc) #numberをcsvの行として指定\n",
        "\n",
        "    label_list_list.append(label_list)\n",
        "    model_pred_prob_list.append(model_pred_prob)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#Draw ROC curve\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list))\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "#それぞれの項目の平均を計算しcsvに追記する\n",
        "df.iloc[0:4,fold], df.iloc[9,fold]   = df.mean(axis=1)[0:4], df.mean(axis=1)[9] \n",
        "df.iloc[0:10,fold+1] = df.std(axis=1)[0:10]\n",
        "TP,TN,FP,FN = df.mean(axis=1)[0:4]\n",
        "df.iloc[4:9,fold] = calculateAccuracy (TP, TN, FP, FN)\n",
        "print(df)\n",
        "\n",
        "# CSVとして出力\n",
        "df.to_csv(\"/content/drive/My Drive/Grav_bootcamp/crossvaridation_\" + out_name + \".csv\",encoding=\"shift_jis\")\n",
        "\n",
        "#ROC_curveを保存\n",
        "fig.savefig(\"/content/drive/My Drive/Grav_bootcamp/crossvaridation_\" + out_name +\".png\")\n",
        "\n",
        "#Save ROC data\n",
        "with open(\"/content/drive/My Drive/Grav_bootcamp/ROCdata_\"+out_name+\".csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for i, t in enumerate(zip(Y_TRUE, Y_SCORE)):\n",
        "        writer.writerow([t[0],t[1]])\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5-fold cross validation\n",
            "/content/drive/My Drive/crossvalidation/128/0\n",
            "/content/drive/My Drive/crossvalidation/128/1\n",
            "/content/drive/My Drive/crossvalidation/128/2\n",
            "/content/drive/My Drive/crossvalidation/128/3\n",
            "/content/drive/My Drive/crossvalidation/128/4\n",
            "['cont', 'grav']\n",
            "cont_train:51\n",
            "grav_train:51\n",
            "cont_val:13\n",
            "grav_val:13\n",
            "training data set_total：102\n",
            "validating data set_total：26\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "722de21b2fde47faa7c640c2946cb33b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=77999237.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch_optimizer/adabound.py:142: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.6874 Acc: 0.5980\n",
            "val Loss: 0.5732 Acc: 0.8462\n",
            "Validation loss decreased (inf --> 0.573188).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.5356 Acc: 0.7745\n",
            "val Loss: 0.5289 Acc: 0.7308\n",
            "Validation loss decreased (0.573188 --> 0.528923).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.2483 Acc: 0.9216\n",
            "val Loss: 0.3369 Acc: 0.8077\n",
            "Validation loss decreased (0.528923 --> 0.336857).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2706 Acc: 0.8824\n",
            "val Loss: 0.6529 Acc: 0.6154\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2297 Acc: 0.8922\n",
            "val Loss: 0.3439 Acc: 0.8846\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.2499 Acc: 0.9020\n",
            "val Loss: 0.4140 Acc: 0.8462\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1567 Acc: 0.9510\n",
            "val Loss: 2.4961 Acc: 0.5000\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1751 Acc: 0.9412\n",
            "val Loss: 1.6036 Acc: 0.6154\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1025 Acc: 0.9706\n",
            "val Loss: 0.9611 Acc: 0.7692\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1020 Acc: 0.9804\n",
            "val Loss: 0.4929 Acc: 0.8846\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0633 Acc: 0.9902\n",
            "val Loss: 0.5867 Acc: 0.8846\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.3266 Acc: 0.9118\n",
            "val Loss: 0.7014 Acc: 0.6538\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.5967 Acc: 0.8039\n",
            "val Loss: 0.6732 Acc: 0.5000\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.2620 Acc: 0.9314\n",
            "val Loss: 0.3887 Acc: 0.8077\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.2404 Acc: 0.9314\n",
            "val Loss: 0.5807 Acc: 0.8077\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.1946 Acc: 0.9412\n",
            "val Loss: 0.8199 Acc: 0.6923\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.2327 Acc: 0.9314\n",
            "val Loss: 0.4074 Acc: 0.7692\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.1742 Acc: 0.9510\n",
            "val Loss: 0.5323 Acc: 0.7308\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 0m 57s\n",
            "Best val Acc: 0.884615\n",
            "number of images: 108\n",
            "46 8 36 18\n",
            "Accuracy: 0.7592592592592593\n",
            "Precision (positive predictive value): 0.71875\n",
            "Recall (sensitivity): 0.8518518518518519\n",
            "Specificity: 0.6666666666666666\n",
            "F_value: 0.7796610169491525\n",
            "roc_auc: 0.885116598079561\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:51\n",
            "grav_train:51\n",
            "cont_val:13\n",
            "grav_val:13\n",
            "training data set_total：102\n",
            "validating data set_total：26\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.6761 Acc: 0.6569\n",
            "val Loss: 0.7061 Acc: 0.5385\n",
            "Validation loss decreased (inf --> 0.706111).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.2555 Acc: 0.9216\n",
            "val Loss: 0.5483 Acc: 0.7308\n",
            "Validation loss decreased (0.706111 --> 0.548254).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.2391 Acc: 0.9314\n",
            "val Loss: 1.0504 Acc: 0.6154\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2150 Acc: 0.8824\n",
            "val Loss: 2.1010 Acc: 0.5385\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.1867 Acc: 0.9118\n",
            "val Loss: 1.3272 Acc: 0.5769\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.4120 Acc: 0.8333\n",
            "val Loss: 0.5310 Acc: 0.7308\n",
            "Validation loss decreased (0.548254 --> 0.531047).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.3285 Acc: 0.8725\n",
            "val Loss: 0.9491 Acc: 0.5385\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.4760 Acc: 0.7843\n",
            "val Loss: 0.8382 Acc: 0.5769\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.2063 Acc: 0.9216\n",
            "val Loss: 0.9591 Acc: 0.6154\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1293 Acc: 0.9510\n",
            "val Loss: 0.8075 Acc: 0.7308\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.1418 Acc: 0.9510\n",
            "val Loss: 1.9138 Acc: 0.7308\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.2409 Acc: 0.9118\n",
            "val Loss: 4.2077 Acc: 0.5769\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.2787 Acc: 0.8922\n",
            "val Loss: 2.7603 Acc: 0.6923\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.3237 Acc: 0.8725\n",
            "val Loss: 1.3556 Acc: 0.6923\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.2671 Acc: 0.8824\n",
            "val Loss: 0.8814 Acc: 0.7308\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.2526 Acc: 0.9118\n",
            "val Loss: 0.8814 Acc: 0.6923\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.1238 Acc: 0.9902\n",
            "val Loss: 0.8304 Acc: 0.6923\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0757 Acc: 0.9804\n",
            "val Loss: 1.1720 Acc: 0.6154\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0527 Acc: 0.9804\n",
            "val Loss: 1.6523 Acc: 0.6154\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.1044 Acc: 0.9510\n",
            "val Loss: 2.1238 Acc: 0.5385\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.1294 Acc: 0.9510\n",
            "val Loss: 2.7594 Acc: 0.5385\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 3s\n",
            "Best val Acc: 0.730769\n",
            "number of images: 108\n",
            "51 3 27 27\n",
            "Accuracy: 0.7222222222222222\n",
            "Precision (positive predictive value): 0.6538461538461539\n",
            "Recall (sensitivity): 0.9444444444444444\n",
            "Specificity: 0.5\n",
            "F_value: 0.7727272727272727\n",
            "roc_auc: 0.8772290809327846\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:51\n",
            "grav_train:51\n",
            "cont_val:13\n",
            "grav_val:13\n",
            "training data set_total：102\n",
            "validating data set_total：26\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.6416 Acc: 0.6667\n",
            "val Loss: 0.7057 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 0.705694).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.3872 Acc: 0.8235\n",
            "val Loss: 0.7345 Acc: 0.6154\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.2700 Acc: 0.9020\n",
            "val Loss: 0.5340 Acc: 0.6923\n",
            "Validation loss decreased (0.705694 --> 0.534015).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.1304 Acc: 0.9608\n",
            "val Loss: 0.8258 Acc: 0.6154\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.0774 Acc: 0.9804\n",
            "val Loss: 1.2959 Acc: 0.6154\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.2171 Acc: 0.9608\n",
            "val Loss: 0.7249 Acc: 0.8077\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.3791 Acc: 0.8824\n",
            "val Loss: 1.5214 Acc: 0.6154\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.3062 Acc: 0.8922\n",
            "val Loss: 0.7310 Acc: 0.5000\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.3502 Acc: 0.8922\n",
            "val Loss: 0.7224 Acc: 0.7308\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1572 Acc: 0.9804\n",
            "val Loss: 1.1970 Acc: 0.5769\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.1374 Acc: 0.9510\n",
            "val Loss: 1.5278 Acc: 0.6154\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.1561 Acc: 0.9314\n",
            "val Loss: 1.8584 Acc: 0.6154\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0789 Acc: 0.9608\n",
            "val Loss: 1.3525 Acc: 0.7308\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.1916 Acc: 0.9216\n",
            "val Loss: 1.7347 Acc: 0.5769\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.3210 Acc: 0.9020\n",
            "val Loss: 3.4864 Acc: 0.5000\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.2691 Acc: 0.8725\n",
            "val Loss: 3.1392 Acc: 0.5385\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.1580 Acc: 0.9608\n",
            "val Loss: 2.2409 Acc: 0.6538\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.1114 Acc: 0.9804\n",
            "val Loss: 2.3930 Acc: 0.6538\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 0m 54s\n",
            "Best val Acc: 0.807692\n",
            "number of images: 108\n",
            "42 12 47 7\n",
            "Accuracy: 0.8240740740740741\n",
            "Precision (positive predictive value): 0.8571428571428571\n",
            "Recall (sensitivity): 0.7777777777777778\n",
            "Specificity: 0.8703703703703703\n",
            "F_value: 0.8155339805825242\n",
            "roc_auc: 0.8926611796982167\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:51\n",
            "grav_train:51\n",
            "cont_val:13\n",
            "grav_val:13\n",
            "training data set_total：102\n",
            "validating data set_total：26\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.6611 Acc: 0.6078\n",
            "val Loss: 0.6353 Acc: 0.7692\n",
            "Validation loss decreased (inf --> 0.635307).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.5374 Acc: 0.7549\n",
            "val Loss: 0.6465 Acc: 0.6538\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3918 Acc: 0.8431\n",
            "val Loss: 1.7269 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2398 Acc: 0.8725\n",
            "val Loss: 0.9785 Acc: 0.6923\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2849 Acc: 0.8824\n",
            "val Loss: 0.3957 Acc: 0.8846\n",
            "Validation loss decreased (0.635307 --> 0.395714).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.3052 Acc: 0.9020\n",
            "val Loss: 0.5880 Acc: 0.8462\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.2251 Acc: 0.9314\n",
            "val Loss: 0.3849 Acc: 0.7692\n",
            "Validation loss decreased (0.395714 --> 0.384924).  Saving model ...\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.3362 Acc: 0.8922\n",
            "val Loss: 0.3522 Acc: 0.7692\n",
            "Validation loss decreased (0.384924 --> 0.352205).  Saving model ...\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.3402 Acc: 0.8725\n",
            "val Loss: 0.9472 Acc: 0.5385\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.2589 Acc: 0.9216\n",
            "val Loss: 2.4097 Acc: 0.5769\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.1555 Acc: 0.9608\n",
            "val Loss: 3.1826 Acc: 0.5769\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.1614 Acc: 0.9216\n",
            "val Loss: 1.9485 Acc: 0.6538\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.1480 Acc: 0.9314\n",
            "val Loss: 0.5925 Acc: 0.7692\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.1756 Acc: 0.9216\n",
            "val Loss: 1.7086 Acc: 0.8077\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.2020 Acc: 0.9216\n",
            "val Loss: 3.0788 Acc: 0.5769\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.1382 Acc: 0.9314\n",
            "val Loss: 2.9020 Acc: 0.5385\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.1812 Acc: 0.9510\n",
            "val Loss: 1.0734 Acc: 0.7692\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.1701 Acc: 0.9412\n",
            "val Loss: 0.7541 Acc: 0.8462\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.1013 Acc: 0.9902\n",
            "val Loss: 0.6577 Acc: 0.8462\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0658 Acc: 0.9804\n",
            "val Loss: 1.0183 Acc: 0.8077\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.2835 Acc: 0.8824\n",
            "val Loss: 1.7780 Acc: 0.6154\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.1232 Acc: 0.9608\n",
            "val Loss: 0.7512 Acc: 0.7692\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.1475 Acc: 0.9314\n",
            "val Loss: 0.6258 Acc: 0.7692\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 9s\n",
            "Best val Acc: 0.884615\n",
            "number of images: 108\n",
            "40 14 46 8\n",
            "Accuracy: 0.7962962962962963\n",
            "Precision (positive predictive value): 0.8333333333333334\n",
            "Recall (sensitivity): 0.7407407407407407\n",
            "Specificity: 0.8518518518518519\n",
            "F_value: 0.7843137254901961\n",
            "roc_auc: 0.8463648834019205\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:52\n",
            "grav_train:52\n",
            "cont_val:12\n",
            "grav_val:12\n",
            "training data set_total：104\n",
            "validating data set_total：24\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.6996 Acc: 0.5000\n",
            "val Loss: 0.6506 Acc: 0.5833\n",
            "Validation loss decreased (inf --> 0.650585).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.3568 Acc: 0.8654\n",
            "val Loss: 0.4088 Acc: 0.8333\n",
            "Validation loss decreased (0.650585 --> 0.408835).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.2651 Acc: 0.8846\n",
            "val Loss: 0.3933 Acc: 0.8750\n",
            "Validation loss decreased (0.408835 --> 0.393280).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.3476 Acc: 0.8365\n",
            "val Loss: 1.6511 Acc: 0.6250\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2015 Acc: 0.9327\n",
            "val Loss: 2.0039 Acc: 0.5833\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1780 Acc: 0.9615\n",
            "val Loss: 3.0119 Acc: 0.5417\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.3207 Acc: 0.9519\n",
            "val Loss: 3.3424 Acc: 0.5417\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.2331 Acc: 0.9327\n",
            "val Loss: 5.3485 Acc: 0.5417\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.2100 Acc: 0.9135\n",
            "val Loss: 4.0750 Acc: 0.5833\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1554 Acc: 0.9327\n",
            "val Loss: 1.6717 Acc: 0.7917\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.2142 Acc: 0.8846\n",
            "val Loss: 2.1204 Acc: 0.7500\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.1972 Acc: 0.9519\n",
            "val Loss: 1.9350 Acc: 0.6250\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.1242 Acc: 0.9615\n",
            "val Loss: 0.9166 Acc: 0.6667\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.1294 Acc: 0.9615\n",
            "val Loss: 0.7151 Acc: 0.7500\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.1011 Acc: 0.9808\n",
            "val Loss: 0.6148 Acc: 0.7083\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0937 Acc: 0.9615\n",
            "val Loss: 1.2074 Acc: 0.6667\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.1401 Acc: 0.9712\n",
            "val Loss: 1.7411 Acc: 0.6667\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.1835 Acc: 0.9423\n",
            "val Loss: 1.3479 Acc: 0.6667\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 1m 0s\n",
            "Best val Acc: 0.875000\n",
            "number of images: 108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "37 17 51 3\n",
            "Accuracy: 0.8148148148148148\n",
            "Precision (positive predictive value): 0.925\n",
            "Recall (sensitivity): 0.6851851851851852\n",
            "Specificity: 0.9444444444444444\n",
            "F_value: 0.7872340425531915\n",
            "roc_auc: 0.8806584362139918\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                  0         1         2         3         4  \\\n",
            "TP                               46        51        42        40        37   \n",
            "TN                               36        27        47        46        51   \n",
            "FP                               18        27         7         8         3   \n",
            "FN                                8         3        12        14        17   \n",
            "Accuracy                   0.759259  0.722222  0.824074  0.796296  0.814815   \n",
            "Positive predictive value   0.71875  0.653846  0.857143  0.833333     0.925   \n",
            "sensitity                  0.851852  0.944444  0.777778  0.740741  0.685185   \n",
            "specificity                0.666667       0.5   0.87037  0.851852  0.944444   \n",
            "F-value                    0.779661  0.772727  0.815534  0.784314  0.787234   \n",
            "roc_auc                    0.885117  0.877229  0.892661  0.846365  0.880658   \n",
            "\n",
            "                                avg        std  \n",
            "TP                             43.2    4.87442  \n",
            "TN                             41.4    8.73155  \n",
            "FP                             12.6    8.73155  \n",
            "FN                             10.8    4.87442  \n",
            "Accuracy                   0.771915  0.0422287  \n",
            "Positive predictive value   0.75795   0.109471  \n",
            "sensitity                  0.791237   0.100922  \n",
            "specificity                0.753029   0.180781  \n",
            "F-value                    0.774236  0.0163939  \n",
            "roc_auc                    0.876406  0.0158829  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e9700190753b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#Save ROC data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Grav_bootcamp/ROCdata_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mout_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_TRUE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_SCORE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPO-b2cAT0yF",
        "colab_type": "text"
      },
      "source": [
        "#**ネットワークの保存と読み込み**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMYyFQ6ATzyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワークの保存\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c744XUtfT6xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73a923de-e875-4e71-b4b5-1ae266557981"
      },
      "source": [
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}