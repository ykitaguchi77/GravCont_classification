{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled35.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbw+V/JeV/11gV2gN7UAsv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Assessment1.4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpC0Fk9lUn2",
        "colab_type": "text"
      },
      "source": [
        "#**Assessment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9B59fSXlT5f",
        "colab_type": "code",
        "outputId": "8e046008-5be3-431b-c8f9-5d71b405973f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  1234\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHO1RjNom0Pr",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-yVbbvSm3Ay",
        "colab_type": "code",
        "outputId": "45935eb4-d1cd-4ebf-9941-2b0bd44aa6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "#ModelNameをリストにする\n",
        "ModelName_list = []\n",
        "ModelName = ''\n",
        "model_pred_prob = []\n",
        "\n",
        "def checkModelName(ModelName, ModelName_list):\n",
        "    if ModelName in ModelName_list:\n",
        "        pass\n",
        "        #raise Exception(\"This model has been already loaded\")\n",
        "    else:\n",
        "        ModelName_list.append(ModelName)\n",
        "\n",
        "\n",
        "# 入力画像の前処理をするクラス\n",
        "# 訓練時と推論時で処理が異なる\n",
        "\n",
        "\"\"\"\n",
        "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "\"\"\"\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/My Drive/Deep_learning/gravcont_seed_1234'\n",
        "n_samples = len(data_dir)\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "print(class_names)\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_train:\"+str(len(os.listdir(path='/content/drive/My Drive/Deep_learning/gravcont_seed_1234/train/'+class_names[k]))))\n",
        "    k+=1\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_val:\"+str(len(os.listdir(path='/content/drive/My Drive/Deep_learning/gravcont_seed_1234/val/'+class_names[k]))))\n",
        "    k+=1\n",
        "\n",
        "print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cont', 'grav']\n",
            "cont_train:256\n",
            "grav_train:252\n",
            "cont_val:65\n",
            "grav_val:63\n",
            "training data set_total：508\n",
            "validating data set_total：128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zLnknjNnqNU",
        "colab_type": "text"
      },
      "source": [
        "#**Calculate Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doQbOyGHpJYU",
        "colab_type": "code",
        "outputId": "d7cd6a07-c2d4-47c1-d28d-0df0ffbfa60e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "##########Calculate Accuracy#############\n",
        "#valフォルダ内のファイル名を取得\n",
        "image_path = glob.glob(\"/content/drive/My Drive/Deep_learning/gravcont_seed_1234/val/*/*\")\n",
        "random.shuffle(image_path)  #表示順をランダムにする\n",
        "print('number of images: ' +str(len(image_path)))\n",
        "#print(image_path) \n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    \n",
        "    if ModelName == 'Attention_branch_Network_ImageNet':\n",
        "        _, output, _ = model_ft(image_tensor)\n",
        "    else:\n",
        "        output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (image_path):\n",
        "\n",
        "    #ここからがメイン\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力 \n",
        "\n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    \n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    #return(accuracy, precision, recall, specificity, f_value)\n",
        "    return model_pred_prob, label_list\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of images: 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk-s71bSlLMJ",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_VGGFace2のネットワーク**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twcn29TKk7kM",
        "colab_type": "code",
        "outputId": "58f2b487-46a2-4150-eded-5bc756338eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "ModelName = 'ResNet50_VGGFace2'\n",
        "checkModelName(ModelName, ModelName_list)\n",
        "\n",
        "class Resnet50_ft_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Resnet50_ft_dag, self).__init__()\n",
        "        self.meta = {'mean': [131.0912, 103.8827, 91.4953],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv1_relu_7x7_s2 = nn.ReLU()\n",
        "        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
        "        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_3x3_relu = nn.ReLU()\n",
        "        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_3x3_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_3x3_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_3x3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_3x3_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_3x3_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_3x3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_3x3_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_3x3_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_3x3_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_3x3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_3x3_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_3x3_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_3x3_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_3x3_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_3x3_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_relu = nn.ReLU()\n",
        "        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
        "        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))\n",
        "\n",
        "    def forward(self, data):\n",
        "        conv1_7x7_s2 = self.conv1_7x7_s2(data)\n",
        "        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)\n",
        "        conv1_7x7_s2_bnxx = self.conv1_relu_7x7_s2(conv1_7x7_s2_bn)\n",
        "        pool1_3x3_s2 = self.pool1_3x3_s2(conv1_7x7_s2_bnxx)\n",
        "        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(pool1_3x3_s2)\n",
        "        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)\n",
        "        conv2_1_1x1_reduce_bnxx = self.conv2_1_1x1_reduce_relu(conv2_1_1x1_reduce_bn)\n",
        "        conv2_1_3x3 = self.conv2_1_3x3(conv2_1_1x1_reduce_bnxx)\n",
        "        conv2_1_3x3_bn = self.conv2_1_3x3_bn(conv2_1_3x3)\n",
        "        conv2_1_3x3_bnxx = self.conv2_1_3x3_relu(conv2_1_3x3_bn)\n",
        "        conv2_1_1x1_increase = self.conv2_1_1x1_increase(conv2_1_3x3_bnxx)\n",
        "        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)\n",
        "        conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)\n",
        "        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)\n",
        "        conv2_1 = torch.add(conv2_1_1x1_proj_bn, 1, conv2_1_1x1_increase_bn)\n",
        "        conv2_1x = self.conv2_1_relu(conv2_1)\n",
        "        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(conv2_1x)\n",
        "        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)\n",
        "        conv2_2_1x1_reduce_bnxx = self.conv2_2_1x1_reduce_relu(conv2_2_1x1_reduce_bn)\n",
        "        conv2_2_3x3 = self.conv2_2_3x3(conv2_2_1x1_reduce_bnxx)\n",
        "        conv2_2_3x3_bn = self.conv2_2_3x3_bn(conv2_2_3x3)\n",
        "        conv2_2_3x3_bnxx = self.conv2_2_3x3_relu(conv2_2_3x3_bn)\n",
        "        conv2_2_1x1_increase = self.conv2_2_1x1_increase(conv2_2_3x3_bnxx)\n",
        "        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)\n",
        "        conv2_2 = torch.add(conv2_1x, 1, conv2_2_1x1_increase_bn)\n",
        "        conv2_2x = self.conv2_2_relu(conv2_2)\n",
        "        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(conv2_2x)\n",
        "        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)\n",
        "        conv2_3_1x1_reduce_bnxx = self.conv2_3_1x1_reduce_relu(conv2_3_1x1_reduce_bn)\n",
        "        conv2_3_3x3 = self.conv2_3_3x3(conv2_3_1x1_reduce_bnxx)\n",
        "        conv2_3_3x3_bn = self.conv2_3_3x3_bn(conv2_3_3x3)\n",
        "        conv2_3_3x3_bnxx = self.conv2_3_3x3_relu(conv2_3_3x3_bn)\n",
        "        conv2_3_1x1_increase = self.conv2_3_1x1_increase(conv2_3_3x3_bnxx)\n",
        "        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)\n",
        "        conv2_3 = torch.add(conv2_2x, 1, conv2_3_1x1_increase_bn)\n",
        "        conv2_3x = self.conv2_3_relu(conv2_3)\n",
        "        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(conv2_3x)\n",
        "        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)\n",
        "        conv3_1_1x1_reduce_bnxx = self.conv3_1_1x1_reduce_relu(conv3_1_1x1_reduce_bn)\n",
        "        conv3_1_3x3 = self.conv3_1_3x3(conv3_1_1x1_reduce_bnxx)\n",
        "        conv3_1_3x3_bn = self.conv3_1_3x3_bn(conv3_1_3x3)\n",
        "        conv3_1_3x3_bnxx = self.conv3_1_3x3_relu(conv3_1_3x3_bn)\n",
        "        conv3_1_1x1_increase = self.conv3_1_1x1_increase(conv3_1_3x3_bnxx)\n",
        "        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)\n",
        "        conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)\n",
        "        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)\n",
        "        conv3_1 = torch.add(conv3_1_1x1_proj_bn, 1, conv3_1_1x1_increase_bn)\n",
        "        conv3_1x = self.conv3_1_relu(conv3_1)\n",
        "        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(conv3_1x)\n",
        "        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)\n",
        "        conv3_2_1x1_reduce_bnxx = self.conv3_2_1x1_reduce_relu(conv3_2_1x1_reduce_bn)\n",
        "        conv3_2_3x3 = self.conv3_2_3x3(conv3_2_1x1_reduce_bnxx)\n",
        "        conv3_2_3x3_bn = self.conv3_2_3x3_bn(conv3_2_3x3)\n",
        "        conv3_2_3x3_bnxx = self.conv3_2_3x3_relu(conv3_2_3x3_bn)\n",
        "        conv3_2_1x1_increase = self.conv3_2_1x1_increase(conv3_2_3x3_bnxx)\n",
        "        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)\n",
        "        conv3_2 = torch.add(conv3_1x, 1, conv3_2_1x1_increase_bn)\n",
        "        conv3_2x = self.conv3_2_relu(conv3_2)\n",
        "        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(conv3_2x)\n",
        "        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)\n",
        "        conv3_3_1x1_reduce_bnxx = self.conv3_3_1x1_reduce_relu(conv3_3_1x1_reduce_bn)\n",
        "        conv3_3_3x3 = self.conv3_3_3x3(conv3_3_1x1_reduce_bnxx)\n",
        "        conv3_3_3x3_bn = self.conv3_3_3x3_bn(conv3_3_3x3)\n",
        "        conv3_3_3x3_bnxx = self.conv3_3_3x3_relu(conv3_3_3x3_bn)\n",
        "        conv3_3_1x1_increase = self.conv3_3_1x1_increase(conv3_3_3x3_bnxx)\n",
        "        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)\n",
        "        conv3_3 = torch.add(conv3_2x, 1, conv3_3_1x1_increase_bn)\n",
        "        conv3_3x = self.conv3_3_relu(conv3_3)\n",
        "        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(conv3_3x)\n",
        "        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)\n",
        "        conv3_4_1x1_reduce_bnxx = self.conv3_4_1x1_reduce_relu(conv3_4_1x1_reduce_bn)\n",
        "        conv3_4_3x3 = self.conv3_4_3x3(conv3_4_1x1_reduce_bnxx)\n",
        "        conv3_4_3x3_bn = self.conv3_4_3x3_bn(conv3_4_3x3)\n",
        "        conv3_4_3x3_bnxx = self.conv3_4_3x3_relu(conv3_4_3x3_bn)\n",
        "        conv3_4_1x1_increase = self.conv3_4_1x1_increase(conv3_4_3x3_bnxx)\n",
        "        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)\n",
        "        conv3_4 = torch.add(conv3_3x, 1, conv3_4_1x1_increase_bn)\n",
        "        conv3_4x = self.conv3_4_relu(conv3_4)\n",
        "        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(conv3_4x)\n",
        "        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)\n",
        "        conv4_1_1x1_reduce_bnxx = self.conv4_1_1x1_reduce_relu(conv4_1_1x1_reduce_bn)\n",
        "        conv4_1_3x3 = self.conv4_1_3x3(conv4_1_1x1_reduce_bnxx)\n",
        "        conv4_1_3x3_bn = self.conv4_1_3x3_bn(conv4_1_3x3)\n",
        "        conv4_1_3x3_bnxx = self.conv4_1_3x3_relu(conv4_1_3x3_bn)\n",
        "        conv4_1_1x1_increase = self.conv4_1_1x1_increase(conv4_1_3x3_bnxx)\n",
        "        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)\n",
        "        conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)\n",
        "        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)\n",
        "        conv4_1 = torch.add(conv4_1_1x1_proj_bn, 1, conv4_1_1x1_increase_bn)\n",
        "        conv4_1x = self.conv4_1_relu(conv4_1)\n",
        "        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(conv4_1x)\n",
        "        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)\n",
        "        conv4_2_1x1_reduce_bnxx = self.conv4_2_1x1_reduce_relu(conv4_2_1x1_reduce_bn)\n",
        "        conv4_2_3x3 = self.conv4_2_3x3(conv4_2_1x1_reduce_bnxx)\n",
        "        conv4_2_3x3_bn = self.conv4_2_3x3_bn(conv4_2_3x3)\n",
        "        conv4_2_3x3_bnxx = self.conv4_2_3x3_relu(conv4_2_3x3_bn)\n",
        "        conv4_2_1x1_increase = self.conv4_2_1x1_increase(conv4_2_3x3_bnxx)\n",
        "        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)\n",
        "        conv4_2 = torch.add(conv4_1x, 1, conv4_2_1x1_increase_bn)\n",
        "        conv4_2x = self.conv4_2_relu(conv4_2)\n",
        "        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(conv4_2x)\n",
        "        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)\n",
        "        conv4_3_1x1_reduce_bnxx = self.conv4_3_1x1_reduce_relu(conv4_3_1x1_reduce_bn)\n",
        "        conv4_3_3x3 = self.conv4_3_3x3(conv4_3_1x1_reduce_bnxx)\n",
        "        conv4_3_3x3_bn = self.conv4_3_3x3_bn(conv4_3_3x3)\n",
        "        conv4_3_3x3_bnxx = self.conv4_3_3x3_relu(conv4_3_3x3_bn)\n",
        "        conv4_3_1x1_increase = self.conv4_3_1x1_increase(conv4_3_3x3_bnxx)\n",
        "        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)\n",
        "        conv4_3 = torch.add(conv4_2x, 1, conv4_3_1x1_increase_bn)\n",
        "        conv4_3x = self.conv4_3_relu(conv4_3)\n",
        "        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(conv4_3x)\n",
        "        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)\n",
        "        conv4_4_1x1_reduce_bnxx = self.conv4_4_1x1_reduce_relu(conv4_4_1x1_reduce_bn)\n",
        "        conv4_4_3x3 = self.conv4_4_3x3(conv4_4_1x1_reduce_bnxx)\n",
        "        conv4_4_3x3_bn = self.conv4_4_3x3_bn(conv4_4_3x3)\n",
        "        conv4_4_3x3_bnxx = self.conv4_4_3x3_relu(conv4_4_3x3_bn)\n",
        "        conv4_4_1x1_increase = self.conv4_4_1x1_increase(conv4_4_3x3_bnxx)\n",
        "        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)\n",
        "        conv4_4 = torch.add(conv4_3x, 1, conv4_4_1x1_increase_bn)\n",
        "        conv4_4x = self.conv4_4_relu(conv4_4)\n",
        "        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(conv4_4x)\n",
        "        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)\n",
        "        conv4_5_1x1_reduce_bnxx = self.conv4_5_1x1_reduce_relu(conv4_5_1x1_reduce_bn)\n",
        "        conv4_5_3x3 = self.conv4_5_3x3(conv4_5_1x1_reduce_bnxx)\n",
        "        conv4_5_3x3_bn = self.conv4_5_3x3_bn(conv4_5_3x3)\n",
        "        conv4_5_3x3_bnxx = self.conv4_5_3x3_relu(conv4_5_3x3_bn)\n",
        "        conv4_5_1x1_increase = self.conv4_5_1x1_increase(conv4_5_3x3_bnxx)\n",
        "        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)\n",
        "        conv4_5 = torch.add(conv4_4x, 1, conv4_5_1x1_increase_bn)\n",
        "        conv4_5x = self.conv4_5_relu(conv4_5)\n",
        "        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(conv4_5x)\n",
        "        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)\n",
        "        conv4_6_1x1_reduce_bnxx = self.conv4_6_1x1_reduce_relu(conv4_6_1x1_reduce_bn)\n",
        "        conv4_6_3x3 = self.conv4_6_3x3(conv4_6_1x1_reduce_bnxx)\n",
        "        conv4_6_3x3_bn = self.conv4_6_3x3_bn(conv4_6_3x3)\n",
        "        conv4_6_3x3_bnxx = self.conv4_6_3x3_relu(conv4_6_3x3_bn)\n",
        "        conv4_6_1x1_increase = self.conv4_6_1x1_increase(conv4_6_3x3_bnxx)\n",
        "        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)\n",
        "        conv4_6 = torch.add(conv4_5x, 1, conv4_6_1x1_increase_bn)\n",
        "        conv4_6x = self.conv4_6_relu(conv4_6)\n",
        "        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(conv4_6x)\n",
        "        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)\n",
        "        conv5_1_1x1_reduce_bnxx = self.conv5_1_1x1_reduce_relu(conv5_1_1x1_reduce_bn)\n",
        "        conv5_1_3x3 = self.conv5_1_3x3(conv5_1_1x1_reduce_bnxx)\n",
        "        conv5_1_3x3_bn = self.conv5_1_3x3_bn(conv5_1_3x3)\n",
        "        conv5_1_3x3_bnxx = self.conv5_1_3x3_relu(conv5_1_3x3_bn)\n",
        "        conv5_1_1x1_increase = self.conv5_1_1x1_increase(conv5_1_3x3_bnxx)\n",
        "        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)\n",
        "        conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)\n",
        "        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)\n",
        "        conv5_1 = torch.add(conv5_1_1x1_proj_bn, 1, conv5_1_1x1_increase_bn)\n",
        "        conv5_1x = self.conv5_1_relu(conv5_1)\n",
        "        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(conv5_1x)\n",
        "        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)\n",
        "        conv5_2_1x1_reduce_bnxx = self.conv5_2_1x1_reduce_relu(conv5_2_1x1_reduce_bn)\n",
        "        conv5_2_3x3 = self.conv5_2_3x3(conv5_2_1x1_reduce_bnxx)\n",
        "        conv5_2_3x3_bn = self.conv5_2_3x3_bn(conv5_2_3x3)\n",
        "        conv5_2_3x3_bnxx = self.conv5_2_3x3_relu(conv5_2_3x3_bn)\n",
        "        conv5_2_1x1_increase = self.conv5_2_1x1_increase(conv5_2_3x3_bnxx)\n",
        "        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)\n",
        "        conv5_2 = torch.add(conv5_1x, 1, conv5_2_1x1_increase_bn)\n",
        "        conv5_2x = self.conv5_2_relu(conv5_2)\n",
        "        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(conv5_2x)\n",
        "        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)\n",
        "        conv5_3_1x1_reduce_bnxx = self.conv5_3_1x1_reduce_relu(conv5_3_1x1_reduce_bn)\n",
        "        conv5_3_3x3 = self.conv5_3_3x3(conv5_3_1x1_reduce_bnxx)\n",
        "        conv5_3_3x3_bn = self.conv5_3_3x3_bn(conv5_3_3x3)\n",
        "        conv5_3_3x3_bnxx = self.conv5_3_3x3_relu(conv5_3_3x3_bn)\n",
        "        conv5_3_1x1_increase = self.conv5_3_1x1_increase(conv5_3_3x3_bnxx)\n",
        "        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)\n",
        "        conv5_3 = torch.add(conv5_2x, 1, conv5_3_1x1_increase_bn)\n",
        "        conv5_3x = self.conv5_3_relu(conv5_3)\n",
        "        pool5_7x7_s1 = self.pool5_7x7_s1(conv5_3x)\n",
        "        classifier_preflatten = self.classifier(pool5_7x7_s1)\n",
        "        classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)\n",
        "        #return classifier, pool5_7x7_s1 　出力を変更しておかないと次元が合わないと言われる\n",
        "        return classifier\n",
        "\n",
        "def resnet50_ft_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Resnet50_ft_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "#モデルのロード\n",
        "model_ft = Resnet50_ft_dag()\n",
        "\n",
        "#最終結合層のリセットと付け替え(全結合層を2つに)\n",
        "model_ft.classifier = nn.Linear(2048, 2)\n",
        "model_ft.classifier = nn.Sequential(*([Flatten()] + list(model_ft.children())[-1:])) #Flattenを挿入\n",
        "\n",
        "#重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_VGGFace2_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "result = calculateAccuracy(image_path)\n",
        "model_pred_prob.append(result[0])\n",
        "label_list = result[1]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50 15 54 9\n",
            "Accuracy: 0.8125\n",
            "Precision (positive predictive value): 0.847457627118644\n",
            "Recall (sensitivity): 0.7692307692307693\n",
            "Specificity: 0.8571428571428571\n",
            "F_value: 0.8064516129032259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72UkHpANnFjw",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNwSFAOfnEtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ModelName = 'ResNet50_ImageNet'\n",
        "checkModelName(ModelName, ModelName_list)\n",
        "\n",
        "model_ft = models.resnet50(pretrained=False)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# 重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_ImageNet_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "result = calculateAccuracy(image_path)\n",
        "model_pred_prob.append(result[0])\n",
        "label_list = result[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H64GGa2JmT9D",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_nonPretrained**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVKpRFdAmUDR",
        "colab_type": "code",
        "outputId": "3f5ee770-dde7-45df-f605-9c72e65f0665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "ModelName = 'ResNet50_nonPretrained'\n",
        "checkModelName(ModelName, ModelName_list)\n",
        "\n",
        "model_ft = models.resnet50(pretrained=False)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# 重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_ImageNet_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "result = calculateAccuracy(image_path)\n",
        "model_pred_prob.append(result[0])\n",
        "label_list = result[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47 18 36 27\n",
            "Accuracy: 0.6484375\n",
            "Precision (positive predictive value): 0.6351351351351351\n",
            "Recall (sensitivity): 0.7230769230769231\n",
            "Specificity: 0.5714285714285714\n",
            "F_value: 0.6762589928057553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWhxHmDPns3R",
        "colab_type": "text"
      },
      "source": [
        "#**EfficientNet_b4_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syACduJCntAB",
        "colab_type": "code",
        "outputId": "42ea06b2-1122-4cfe-ed39-f6dd4bddc7df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "ModelName = 'EfficientNet_b4_ImageNet'\n",
        "checkModelName(ModelName, ModelName_list)    \n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "model_ft = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "num_ftrs = model_ft._fc.in_features\n",
        "model_ft._fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "result = calculateAccuracy(image_path)\n",
        "model_pred_prob.append(result[0])\n",
        "label_list = result[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "55 10 52 11\n",
            "Accuracy: 0.8359375\n",
            "Precision (positive predictive value): 0.8333333333333334\n",
            "Recall (sensitivity): 0.8461538461538461\n",
            "Specificity: 0.8253968253968254\n",
            "F_value: 0.8396946564885497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymGrWxHfntKI",
        "colab_type": "text"
      },
      "source": [
        "#**Attention_branch_Network_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gESMORA2ntRo",
        "colab_type": "code",
        "outputId": "c16eaacb-d108-41cb-cc5f-15d296f3417f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "ModelName = 'Attention_branch_Network_ImageNet'\n",
        "checkModelName(ModelName, ModelName_list)   \n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "\n",
        "class ResNet_ARN(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet_ARN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], down_size=True)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, down_size=True)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, down_size=True)\n",
        "\n",
        "        self.att_layer4 = self._make_layer(block, 512, layers[3], stride=1, down_size=False)\n",
        "        self.bn_att = nn.BatchNorm2d(512 * block.expansion)\n",
        "        self.att_conv   = nn.Conv2d(512 * block.expansion, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.bn_att2 = nn.BatchNorm2d(num_classes)\n",
        "        self.att_conv2  = nn.Conv2d(num_classes, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.att_conv3  = nn.Conv2d(num_classes, 1, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn_att3 = nn.BatchNorm2d(1)\n",
        "        self.att_gap = nn.AvgPool2d(14)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, down_size=True)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, down_size=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "\n",
        "        if down_size:\n",
        "            self.inplanes = planes * block.expansion\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "        else:\n",
        "            inplanes = planes * block.expansion\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        fe = x\n",
        "\n",
        "        ax = self.bn_att(self.att_layer4(x))\n",
        "        ax = self.relu(self.bn_att2(self.att_conv(ax)))\n",
        "        bs, cs, ys, xs = ax.shape\n",
        "        self.att = self.sigmoid(self.bn_att3(self.att_conv3(ax)))        \n",
        "        # self.att = self.att.view(bs, 1, ys, xs)\n",
        "        ax = self.att_conv2(ax)\n",
        "        ax = self.att_gap(ax)\n",
        "        ax = ax.view(ax.size(0), -1)\n",
        "\n",
        "        rx = x * self.att\n",
        "        rx = rx + x\n",
        "        per = rx\n",
        "        rx = self.layer4(rx)\n",
        "        rx = self.avgpool(rx)\n",
        "        rx = rx.view(rx.size(0), -1)\n",
        "        rx = self.fc(rx)\n",
        "\n",
        "        return ax, rx, [self.att, fe, per]\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "def resnetARN50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_ARN(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "#モデルのロード\n",
        "model_ft = resnetARN50().to(device)\n",
        "\n",
        "#attention branch networkの最終出力を2つにする\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/gravcont_att_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/gravcont_att_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "model_ft.to(device)\n",
        "\n",
        "#Prediction\n",
        "result = calculateAccuracy(image_path)\n",
        "model_pred_prob.append(result[0])\n",
        "label_list = result[1]"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49 16 52 11\n",
            "Accuracy: 0.7890625\n",
            "Precision (positive predictive value): 0.8166666666666667\n",
            "Recall (sensitivity): 0.7538461538461538\n",
            "Specificity: 0.8253968253968254\n",
            "F_value: 0.784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVay7Id8oIPK",
        "colab_type": "text"
      },
      "source": [
        "#**Drawing ROC curve** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q1bF1hpg5Qd",
        "colab_type": "code",
        "outputId": "7c30029b-f116-4ea8-99f0-639312381657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "print(ModelName_list)\n",
        "\n",
        "print(model_pred_prob)\n",
        "len(model_pred_prob[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ResNet50_VGGFace2', 'Attention_branch_Network_ImageNet']\n",
            "[[0.9998389482498169, 0.12338149547576904, 0.11166650056838989, 0.9790109992027283, 0.07347381114959717, 0.07400882244110107, 0.8118093013763428, 1.0, 0.38154661655426025, 0.7727088928222656, 0.9996811151504517, 0.08021163940429688, 0.2465541958808899, 0.9999029636383057, 0.991655707359314, 0.9998177886009216, 0.7021567821502686, 0.026246607303619385, 0.9997712969779968, 0.9999992847442627, 0.4469411373138428, 0.9999922513961792, 0.14328545331954956, 0.027871131896972656, 0.15973007678985596, 0.9829356670379639, 0.08215898275375366, 0.020694434642791748, 0.7320812940597534, 0.04878145456314087, 0.999922513961792, 0.5010124444961548, 0.017274916172027588, 0.4335808753967285, 0.12976133823394775, 0.04064488410949707, 0.999994158744812, 0.42002350091934204, 0.7397900223731995, 0.980217695236206, 0.20673340559005737, 0.9790650606155396, 0.9912379384040833, 0.036663711071014404, 0.34198999404907227, 0.9947236776351929, 0.9999620914459229, 0.3338891863822937, 0.4994310736656189, 0.034967899322509766, 0.9999786615371704, 0.9997441172599792, 0.5207807421684265, 0.9965883493423462, 0.7264546155929565, 0.46475404500961304, 0.5534926056861877, 0.36267054080963135, 0.9550123810768127, 0.13279128074645996, 0.8180245757102966, 0.035286784172058105, 0.2422674298286438, 0.9411635994911194, 0.8475645184516907, 0.9999887943267822, 0.86346834897995, 0.8928737640380859, 0.054137587547302246, 0.9045776128768921, 0.08155953884124756, 0.2386661171913147, 0.7642732262611389, 0.9999929666519165, 0.2542803883552551, 0.2495620846748352, 0.9999979734420776, 0.056265175342559814, 0.9950608611106873, 0.8987222909927368, 0.9999529123306274, 0.7178725004196167, 0.21190613508224487, 0.9999947547912598, 0.9999843835830688, 0.115531325340271, 0.4631900191307068, 0.8348885774612427, 0.617763340473175, 0.0979154109954834, 0.7218027114868164, 0.15833890438079834, 0.03722041845321655, 0.10793709754943848, 0.034631550312042236, 0.2631845474243164, 0.08007705211639404, 0.5611247420310974, 0.9988924860954285, 0.9987905621528625, 0.044655799865722656, 0.999891996383667, 0.7494967579841614, 0.10501933097839355, 0.9771180152893066, 0.10690063238143921, 0.9999953508377075, 0.11153936386108398, 0.18998384475708008, 0.9981873631477356, 0.1644439697265625, 0.4843406677246094, 0.04016178846359253, 0.18250668048858643, 0.5824018120765686, 0.9950986504554749, 0.9993981122970581, 0.9597899317741394, 0.9909512996673584, 0.9705369472503662, 0.018199443817138672, 0.9997963309288025, 0.009984791278839111, 0.5122610926628113, 0.7029680013656616, 0.9999799728393555, 0.5520241260528564, 0.25128239393234253], [0.9998389482498169, 0.12338149547576904, 0.11166650056838989, 0.9790109992027283, 0.07347381114959717, 0.07400882244110107, 0.8118093013763428, 1.0, 0.38154661655426025, 0.7727088928222656, 0.9996811151504517, 0.08021163940429688, 0.2465541958808899, 0.9999029636383057, 0.991655707359314, 0.9998177886009216, 0.7021567821502686, 0.026246607303619385, 0.9997712969779968, 0.9999992847442627, 0.4469411373138428, 0.9999922513961792, 0.14328545331954956, 0.027871131896972656, 0.15973007678985596, 0.9829356670379639, 0.08215898275375366, 0.020694434642791748, 0.7320812940597534, 0.04878145456314087, 0.999922513961792, 0.5010124444961548, 0.017274916172027588, 0.4335808753967285, 0.12976133823394775, 0.04064488410949707, 0.999994158744812, 0.42002350091934204, 0.7397900223731995, 0.980217695236206, 0.20673340559005737, 0.9790650606155396, 0.9912379384040833, 0.036663711071014404, 0.34198999404907227, 0.9947236776351929, 0.9999620914459229, 0.3338891863822937, 0.4994310736656189, 0.034967899322509766, 0.9999786615371704, 0.9997441172599792, 0.5207807421684265, 0.9965883493423462, 0.7264546155929565, 0.46475404500961304, 0.5534926056861877, 0.36267054080963135, 0.9550123810768127, 0.13279128074645996, 0.8180245757102966, 0.035286784172058105, 0.2422674298286438, 0.9411635994911194, 0.8475645184516907, 0.9999887943267822, 0.86346834897995, 0.8928737640380859, 0.054137587547302246, 0.9045776128768921, 0.08155953884124756, 0.2386661171913147, 0.7642732262611389, 0.9999929666519165, 0.2542803883552551, 0.2495620846748352, 0.9999979734420776, 0.056265175342559814, 0.9950608611106873, 0.8987222909927368, 0.9999529123306274, 0.7178725004196167, 0.21190613508224487, 0.9999947547912598, 0.9999843835830688, 0.115531325340271, 0.4631900191307068, 0.8348885774612427, 0.617763340473175, 0.0979154109954834, 0.7218027114868164, 0.15833890438079834, 0.03722041845321655, 0.10793709754943848, 0.034631550312042236, 0.2631845474243164, 0.08007705211639404, 0.5611247420310974, 0.9988924860954285, 0.9987905621528625, 0.044655799865722656, 0.999891996383667, 0.7494967579841614, 0.10501933097839355, 0.9771180152893066, 0.10690063238143921, 0.9999953508377075, 0.11153936386108398, 0.18998384475708008, 0.9981873631477356, 0.1644439697265625, 0.4843406677246094, 0.04016178846359253, 0.18250668048858643, 0.5824018120765686, 0.9950986504554749, 0.9993981122970581, 0.9597899317741394, 0.9909512996673584, 0.9705369472503662, 0.018199443817138672, 0.9997963309288025, 0.009984791278839111, 0.5122610926628113, 0.7029680013656616, 0.9999799728393555, 0.5520241260528564, 0.25128239393234253], [0.9998389482498169, 0.12338149547576904, 0.11166650056838989, 0.9790109992027283, 0.07347381114959717, 0.07400882244110107, 0.8118093013763428, 1.0, 0.38154661655426025, 0.7727088928222656, 0.9996811151504517, 0.08021163940429688, 0.2465541958808899, 0.9999029636383057, 0.991655707359314, 0.9998177886009216, 0.7021567821502686, 0.026246607303619385, 0.9997712969779968, 0.9999992847442627, 0.4469411373138428, 0.9999922513961792, 0.14328545331954956, 0.027871131896972656, 0.15973007678985596, 0.9829356670379639, 0.08215898275375366, 0.020694434642791748, 0.7320812940597534, 0.04878145456314087, 0.999922513961792, 0.5010124444961548, 0.017274916172027588, 0.4335808753967285, 0.12976133823394775, 0.04064488410949707, 0.999994158744812, 0.42002350091934204, 0.7397900223731995, 0.980217695236206, 0.20673340559005737, 0.9790650606155396, 0.9912379384040833, 0.036663711071014404, 0.34198999404907227, 0.9947236776351929, 0.9999620914459229, 0.3338891863822937, 0.4994310736656189, 0.034967899322509766, 0.9999786615371704, 0.9997441172599792, 0.5207807421684265, 0.9965883493423462, 0.7264546155929565, 0.46475404500961304, 0.5534926056861877, 0.36267054080963135, 0.9550123810768127, 0.13279128074645996, 0.8180245757102966, 0.035286784172058105, 0.2422674298286438, 0.9411635994911194, 0.8475645184516907, 0.9999887943267822, 0.86346834897995, 0.8928737640380859, 0.054137587547302246, 0.9045776128768921, 0.08155953884124756, 0.2386661171913147, 0.7642732262611389, 0.9999929666519165, 0.2542803883552551, 0.2495620846748352, 0.9999979734420776, 0.056265175342559814, 0.9950608611106873, 0.8987222909927368, 0.9999529123306274, 0.7178725004196167, 0.21190613508224487, 0.9999947547912598, 0.9999843835830688, 0.115531325340271, 0.4631900191307068, 0.8348885774612427, 0.617763340473175, 0.0979154109954834, 0.7218027114868164, 0.15833890438079834, 0.03722041845321655, 0.10793709754943848, 0.034631550312042236, 0.2631845474243164, 0.08007705211639404, 0.5611247420310974, 0.9988924860954285, 0.9987905621528625, 0.044655799865722656, 0.999891996383667, 0.7494967579841614, 0.10501933097839355, 0.9771180152893066, 0.10690063238143921, 0.9999953508377075, 0.11153936386108398, 0.18998384475708008, 0.9981873631477356, 0.1644439697265625, 0.4843406677246094, 0.04016178846359253, 0.18250668048858643, 0.5824018120765686, 0.9950986504554749, 0.9993981122970581, 0.9597899317741394, 0.9909512996673584, 0.9705369472503662, 0.018199443817138672, 0.9997963309288025, 0.009984791278839111, 0.5122610926628113, 0.7029680013656616, 0.9999799728393555, 0.5520241260528564, 0.25128239393234253], [0.9013834595680237, 0.10933226346969604, 0.020971357822418213, 0.6817405819892883, 0.033984601497650146, 0.04287010431289673, 0.5581653118133545, 0.9999985694885254, 0.059578657150268555, 0.48967891931533813, 0.9955253005027771, 0.006657660007476807, 0.26131898164749146, 0.9999998807907104, 0.7241644263267517, 0.9585177302360535, 0.4972720146179199, 0.047308146953582764, 0.8065115213394165, 0.9997435212135315, 0.10411667823791504, 0.9993699193000793, 0.09376770257949829, 0.20170456171035767, 0.1450669765472412, 0.43840110301971436, 0.18716275691986084, 0.006848454475402832, 0.5028619766235352, 0.03191089630126953, 0.999737560749054, 0.3239080309867859, 0.0020933151245117188, 0.7645732164382935, 0.27207982540130615, 0.07066148519515991, 0.9981837868690491, 0.6284942030906677, 0.8190549612045288, 0.8477585315704346, 0.523574709892273, 0.9998331069946289, 0.8928790092468262, 0.10639292001724243, 0.04961514472961426, 0.9674075841903687, 0.913486123085022, 0.5283310413360596, 0.7368251085281372, 0.22335082292556763, 0.973350465297699, 0.9323363900184631, 0.0284348726272583, 0.9062340259552002, 0.19886451959609985, 0.3434271216392517, 0.13870441913604736, 0.03692948818206787, 0.8822904825210571, 0.1711687445640564, 0.5492003560066223, 0.04912972450256348, 0.7048995494842529, 0.8080409169197083, 0.5773994326591492, 0.9743345379829407, 0.568537175655365, 0.8157558441162109, 0.03488600254058838, 0.6878512501716614, 0.15869605541229248, 0.5264374017715454, 0.16312569379806519, 0.9238229393959045, 0.5848947167396545, 0.6723499298095703, 1.0, 0.0636715292930603, 0.9132652282714844, 0.8791587948799133, 0.9998475313186646, 0.20580220222473145, 0.17334693670272827, 0.9999746084213257, 0.868098795413971, 0.07900810241699219, 0.016269803047180176, 0.5408828258514404, 0.825607180595398, 0.03621870279312134, 0.033384859561920166, 0.11759328842163086, 0.02117025852203369, 0.02027946710586548, 0.009530961513519287, 0.8715779185295105, 0.0519370436668396, 0.3279690146446228, 0.9296479821205139, 0.8831198811531067, 0.061676204204559326, 0.9999804496765137, 0.5260176658630371, 0.6509225368499756, 0.844965398311615, 0.10157102346420288, 0.991766631603241, 0.22890013456344604, 0.448685884475708, 0.5420143008232117, 0.6835908889770508, 0.43813782930374146, 0.09485447406768799, 0.5565565228462219, 0.8133178353309631, 0.2416958212852478, 0.9449374079704285, 0.9299746155738831, 0.951073944568634, 0.8197617530822754, 0.016292154788970947, 0.9984000325202942, 0.004872679710388184, 0.3164275884628296, 0.8163159489631653, 0.9867516756057739, 0.08534902334213257, 0.12119263410568237]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_c2zVMKoIVK",
        "colab_type": "code",
        "outputId": "2fb0b267-e4c4-4dc4-e13f-c802451c0d9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_score = []\n",
        "y_true = []\n",
        "\n",
        "k=0\n",
        "for i in label_list:\n",
        "    if label_list[k] == 'cont':\n",
        "          y_true.append(0)\n",
        "    elif label_list[k] == 'grav':\n",
        "          y_true.append(1)\n",
        "    k+=1\n",
        "\n",
        "\n",
        "#健康な状態を「0」、病気を「1」としてラベルよりリストを作成\n",
        "y_true = y_true\n",
        "#それぞれの画像における陽性の確率についてリストを作成\n",
        "#for構文でグラフを上書きしていく予定\n",
        "y_score = model_pred_prob[0]\n",
        "\n",
        "#print(y_true)\n",
        "#print(y_score)\n",
        "\n",
        "fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#print(fpr)\n",
        "#print(tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gom3bMGgSXVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_score = []\n",
        "y_true = []\n",
        "\n",
        "k=0\n",
        "for i in label_list:\n",
        "    if label_list[k] == 'cont':\n",
        "          y_true.append(0)\n",
        "    elif label_list[k] == 'grav':\n",
        "          y_true.append(1)\n",
        "    k+=1\n",
        "\n",
        "\n",
        "#健康な状態を「0」、病気を「1」としてラベルよりリストを作成\n",
        "y_true = y_true\n",
        "\n",
        "#グラフの外形を作成\n",
        "plt.figure()\n",
        "lw = 2\n",
        "\n",
        "k=0\n",
        "for i in ModelName_list:\n",
        "    y_score = model_pred_prob[k]\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}