{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4931a8ef780f41f29790c0294d2e6a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7276de84e0764b83aa3ee93a0acfb896",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_174a9de763e34da3acf47d0f0f9b3835",
              "IPY_MODEL_6fad35ef63734d37bd5bffcc7d727163"
            ]
          }
        },
        "7276de84e0764b83aa3ee93a0acfb896": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "174a9de763e34da3acf47d0f0f9b3835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e3ea4c7d07b4ceb824a1c3dac70fc34",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 77999237,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 77999237,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b371f37bbca94fc4aa4e4e9926db331f"
          }
        },
        "6fad35ef63734d37bd5bffcc7d727163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_29a5dbdef53b4b0dbf538fe1b3df8ab6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 74.4M/74.4M [00:06&lt;00:00, 13.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_792688b6f2ea420ba2fce8f1b92b8a31"
          }
        },
        "9e3ea4c7d07b4ceb824a1c3dac70fc34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b371f37bbca94fc4aa4e4e9926db331f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "29a5dbdef53b4b0dbf538fe1b3df8ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "792688b6f2ea420ba2fce8f1b92b8a31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/EfficientNet_b4_ImageNet_adabound_crossvalidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a4ZBlqPNdU",
        "colab_type": "text"
      },
      "source": [
        "#**GravCont: EfficientNet_b4_ImageNet**\n",
        "ValidationとTestに分けて解析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgM-Y7SVPNkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a2a36efe-25b1-4299-a3db-e79e0d1ec69a"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.6/dist-packages (0.0.1a15)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Random Seed:  1234\n",
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzp_09fWPNoU",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sJV06qPNsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(data_dir):\n",
        "    # 入力画像の前処理をするクラス\n",
        "    # 訓練時と推論時で処理が異なる\n",
        "\n",
        "    \"\"\"\n",
        "        画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "        画像のサイズをリサイズし、色を標準化する。\n",
        "        訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        resize : int\n",
        "            リサイズ先の画像の大きさ。\n",
        "        mean : (R, G, B)\n",
        "            各色チャネルの平均値。\n",
        "        std : (R, G, B)\n",
        "            各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    data_dir = data_dir\n",
        "    n_samples = len(data_dir)\n",
        "\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in ['train', 'val']}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                                shuffle=True, num_workers=4)\n",
        "                  for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "    print(class_names)\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_train:\"+str(len(os.listdir(path=data_dir + '/train/'+class_names[k]))))\n",
        "        k+=1\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_val:\"+str(len(os.listdir(path=data_dir + '/val/'+class_names[k]))))\n",
        "        k+=1\n",
        "\n",
        "    print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "    print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "    \n",
        "    return image_datasets, dataloaders, dataset_sizes, class_names, device\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "def getBatch(dataloaders):    \n",
        "    # Get a batch of training data\n",
        "    inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "    # Make a grid from batch\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "    #imshow(out, title=[class_names[x] for x in classes])\n",
        "    return(inputs, classes)\n",
        "\n",
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "def convnet():\n",
        "    model_ft = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "    num_ftrs = model_ft._fc.in_features\n",
        "    model_ft._fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    optimizer_ft = optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=0,\n",
        "        amsbound=False,\n",
        "    )\n",
        "    return (model_ft, criterion, optimizer_ft)\n",
        "\n",
        "\n",
        "def training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50):\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=patience, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "    \"\"\"\n",
        "    #probalilityを計算する\n",
        "    pred_prob = torch.topk(nn.Softmax(dim=1)(output), 1)[0]\n",
        "    pred_class = torch.topk(nn.Softmax(dim=1)(output), 1)[1]\n",
        "    if pred_class == 1:\n",
        "        pred_prob = pred_prob\n",
        "    elif pred_class == 0:\n",
        "        pred_prob = 1- pred_prob\n",
        "    return(model_pred, pred_prob)  #class_nameの番号で出力される\n",
        "    \"\"\"\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\"\"\"\n",
        "・True positive (TN)\n",
        "・False positive (FP)\n",
        "・True negative (TN)\n",
        "・False negative (FN)\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "Precision = TP/(FP + TP) ※positive predictive value\n",
        "Recall = TP/(TP + FN)　※sensitivity\n",
        "Specificity = TN/(FP + TN)\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)\n",
        "\"\"\"\n",
        "\n",
        "def evaluation(model_ft, testset_dir):\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #testデータセット内のファイル名を取得\n",
        "    image_path = glob.glob(testset_dir + \"/*/*\")\n",
        "    #random.shuffle(image_path)  #表示順をランダムにする\n",
        "    print('number of images: ' +str(len(image_path)))\n",
        "\n",
        "\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1     \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    return TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob\n",
        "\n",
        "\n",
        "def make_csv(roc_label_list):\n",
        "    #csvのdata tableを作成\n",
        "    pd.set_option('display.max_rows', 800)  # 省略なしで表示\n",
        "    #columns1 = [\"EfficientNet_32\", \"EfficientNet_64\", \"EfficientNet_128\", \"EfficientNet_256\", \"EfficientNet_512\", \"EfficientNet_558\"]\n",
        "    columns1 = roc_label_list\n",
        "    index1 = [\"TP\",\"TN\",\"FP\",\"FN\",\"Accuracy\",\"Positive predictive value\",\"sensitity\",\"specificity\",\"F-value\",\"roc_auc\"]\n",
        "    df = pd.DataFrame(index=index1, columns=columns1)\n",
        "    return df\n",
        "\n",
        "def write_csv(df, col, TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc):\n",
        "    df.iloc[0:10, col] = TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc \n",
        "    #print(df)\n",
        "\n",
        "    # CSVとして出力\n",
        "    #df2.to_csv(\"/content/drive/My Drive/Grav_bootcamp/Posttrain_model_eval_result.csv\",encoding=\"shift_jis\")\n",
        "    return df\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == 'cont':\n",
        "                  y_true.append(0)\n",
        "            elif i == 'grav':\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == 'cont':\n",
        "              y_true.append(0)\n",
        "        elif i == 'grav':\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGfUwQXv6Jc",
        "colab_type": "text"
      },
      "source": [
        "#**まとめて解析**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObCZwRvYCPTI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7b610093-0617-414c-d1f8-8538fc5132cc"
      },
      "source": [
        "#create data_dir_list\n",
        "data_dir = '/content/drive/My Drive/gravcont_crossvalidation'\n",
        "fold = len(os.listdir(data_dir))\n",
        "print(fold)\n",
        "\n",
        "data_dir_list = [0]*fold\n",
        "\n",
        "for i in range(fold):\n",
        "    data_dir_list[i] = data_dir + '/' + str(i)\n",
        "    print(data_dir_list[i])\n",
        "\n",
        "#create roc_label_list\n",
        "roc_label_list = [0]*fold\n",
        "roc_label_list = list(range(fold))\n",
        "print(roc_label_list)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "/content/drive/My Drive/gravcont_crossvalidation/0\n",
            "/content/drive/My Drive/gravcont_crossvalidation/1\n",
            "/content/drive/My Drive/gravcont_crossvalidation/2\n",
            "/content/drive/My Drive/gravcont_crossvalidation/3\n",
            "/content/drive/My Drive/gravcont_crossvalidation/4\n",
            "[0, 1, 2, 3, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM2VMXltwBs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4931a8ef780f41f29790c0294d2e6a16",
            "7276de84e0764b83aa3ee93a0acfb896",
            "174a9de763e34da3acf47d0f0f9b3835",
            "6fad35ef63734d37bd5bffcc7d727163",
            "9e3ea4c7d07b4ceb824a1c3dac70fc34",
            "b371f37bbca94fc4aa4e4e9926db331f",
            "29a5dbdef53b4b0dbf538fe1b3df8ab6",
            "792688b6f2ea420ba2fce8f1b92b8a31"
          ]
        },
        "outputId": "b30e9595-bedc-4ce7-fcf7-c626a7afd5b8"
      },
      "source": [
        "df = make_csv(roc_label_list)\n",
        "\n",
        "label_list_list, model_pred_prob_list = [],[]\n",
        "\n",
        "print(data_dir_list)\n",
        "print(roc_label_list)\n",
        "\n",
        "for i, t in enumerate(zip(data_dir_list, roc_label_list)):\n",
        "\n",
        "    image_datasets, dataloaders, dataset_sizes, class_names, device = pre_process(t[0]) #path\n",
        "    inputs, classes = getBatch(dataloaders)\n",
        "    model_ft, criterion, optimizer_ft = convnet()\n",
        "    training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50)  \n",
        "    TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob = evaluation(model_ft, '/content/drive/My Drive/Grav_bootcamp/Posttrain_250px')\n",
        "    roc_auc = calculate_auc(label_list, model_pred_prob)\n",
        "    df = write_csv(df, i,TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, roc_auc) #numberをcsvの行として指定\n",
        "\n",
        "\n",
        "    label_list_list.append(label_list)\n",
        "    model_pred_prob_list.append(model_pred_prob)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#Draw ROC curve\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list))\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "print(df)\n",
        "\n",
        "\n",
        "# CSVとして出力\n",
        "df.to_csv(\"/content/drive/My Drive/Grav_bootcamp/crossvaridation_efficientNet_ImageNet.csv\",encoding=\"shift_jis\")\n",
        "\n",
        "#ROC_curveを保存\n",
        "fig.savefig(\"/content/drive/My Drive/Grav_bootcamp/img.png\")\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/drive/My Drive/gravcont_crossvalidation/0', '/content/drive/My Drive/gravcont_crossvalidation/1', '/content/drive/My Drive/gravcont_crossvalidation/2', '/content/drive/My Drive/gravcont_crossvalidation/3', '/content/drive/My Drive/gravcont_crossvalidation/4']\n",
            "[0, 1, 2, 3, 4]\n",
            "['cont', 'grav']\n",
            "cont_train:266\n",
            "grav_train:266\n",
            "cont_val:67\n",
            "grav_val:67\n",
            "training data set_total：532\n",
            "validating data set_total：134\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b4-6ed6700e.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b4-6ed6700e.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4931a8ef780f41f29790c0294d2e6a16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=77999237.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch_optimizer/adabound.py:142: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
            "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 0.5364 Acc: 0.7481\n",
            "val Loss: 0.6169 Acc: 0.6642\n",
            "Validation loss decreased (inf --> 0.616900).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.3541 Acc: 0.8402\n",
            "val Loss: 1.6087 Acc: 0.7388\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.2792 Acc: 0.8797\n",
            "val Loss: 0.7566 Acc: 0.7687\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2925 Acc: 0.8835\n",
            "val Loss: 1.4200 Acc: 0.6642\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2660 Acc: 0.9004\n",
            "val Loss: 1.0976 Acc: 0.7388\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1497 Acc: 0.9455\n",
            "val Loss: 0.7434 Acc: 0.7537\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1455 Acc: 0.9380\n",
            "val Loss: 1.5069 Acc: 0.6791\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1290 Acc: 0.9586\n",
            "val Loss: 1.1039 Acc: 0.7239\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.0822 Acc: 0.9680\n",
            "val Loss: 0.5565 Acc: 0.8060\n",
            "Validation loss decreased (0.616900 --> 0.556503).  Saving model ...\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.0647 Acc: 0.9756\n",
            "val Loss: 1.0262 Acc: 0.7612\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0459 Acc: 0.9812\n",
            "val Loss: 0.9325 Acc: 0.7388\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0673 Acc: 0.9756\n",
            "val Loss: 0.8322 Acc: 0.7463\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0364 Acc: 0.9868\n",
            "val Loss: 1.0402 Acc: 0.7687\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0229 Acc: 0.9944\n",
            "val Loss: 1.2543 Acc: 0.7239\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0330 Acc: 0.9831\n",
            "val Loss: 1.1793 Acc: 0.7910\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0447 Acc: 0.9868\n",
            "val Loss: 2.0835 Acc: 0.6791\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0566 Acc: 0.9793\n",
            "val Loss: 1.8025 Acc: 0.6791\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0407 Acc: 0.9906\n",
            "val Loss: 1.1151 Acc: 0.6940\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0275 Acc: 0.9906\n",
            "val Loss: 1.1967 Acc: 0.7537\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0428 Acc: 0.9868\n",
            "val Loss: 1.1035 Acc: 0.7537\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0249 Acc: 0.9906\n",
            "val Loss: 1.1777 Acc: 0.6940\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.0279 Acc: 0.9925\n",
            "val Loss: 1.3249 Acc: 0.7313\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0131 Acc: 0.9962\n",
            "val Loss: 1.2679 Acc: 0.7239\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0235 Acc: 0.9887\n",
            "val Loss: 1.3329 Acc: 0.7164\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 5m 39s\n",
            "Best val Acc: 0.805970\n",
            "number of images: 108\n",
            "51 3 52 2\n",
            "Accuracy: 0.9537037037037037\n",
            "Precision (positive predictive value): 0.9622641509433962\n",
            "Recall (sensitivity): 0.9444444444444444\n",
            "Specificity: 0.9629629629629629\n",
            "F_value: 0.9532710280373832\n",
            "roc_auc: 0.9921124828532236\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:266\n",
            "grav_train:266\n",
            "cont_val:67\n",
            "grav_val:67\n",
            "training data set_total：532\n",
            "validating data set_total：134\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.5471 Acc: 0.7049\n",
            "val Loss: 0.7655 Acc: 0.6493\n",
            "Validation loss decreased (inf --> 0.765522).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4540 Acc: 0.8045\n",
            "val Loss: 0.8580 Acc: 0.7313\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3994 Acc: 0.8214\n",
            "val Loss: 0.4374 Acc: 0.7836\n",
            "Validation loss decreased (0.765522 --> 0.437428).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2910 Acc: 0.8778\n",
            "val Loss: 0.5287 Acc: 0.7836\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.1833 Acc: 0.9286\n",
            "val Loss: 1.7781 Acc: 0.7239\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1718 Acc: 0.9474\n",
            "val Loss: 0.9397 Acc: 0.7313\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.0891 Acc: 0.9680\n",
            "val Loss: 0.6875 Acc: 0.7836\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.0831 Acc: 0.9662\n",
            "val Loss: 0.6234 Acc: 0.7985\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1125 Acc: 0.9568\n",
            "val Loss: 0.8220 Acc: 0.7687\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1176 Acc: 0.9605\n",
            "val Loss: 0.5993 Acc: 0.7836\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0512 Acc: 0.9850\n",
            "val Loss: 0.7606 Acc: 0.7985\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0388 Acc: 0.9887\n",
            "val Loss: 1.0536 Acc: 0.7463\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0796 Acc: 0.9756\n",
            "val Loss: 0.8080 Acc: 0.8060\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0812 Acc: 0.9718\n",
            "val Loss: 0.7043 Acc: 0.7985\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0533 Acc: 0.9793\n",
            "val Loss: 0.6149 Acc: 0.8060\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0337 Acc: 0.9850\n",
            "val Loss: 0.6376 Acc: 0.7985\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0151 Acc: 0.9981\n",
            "val Loss: 0.8102 Acc: 0.8134\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0155 Acc: 0.9962\n",
            "val Loss: 0.7939 Acc: 0.7836\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 4m 36s\n",
            "Best val Acc: 0.813433\n",
            "number of images: 108\n",
            "50 4 53 1\n",
            "Accuracy: 0.9537037037037037\n",
            "Precision (positive predictive value): 0.9803921568627451\n",
            "Recall (sensitivity): 0.9259259259259259\n",
            "Specificity: 0.9814814814814815\n",
            "F_value: 0.9523809523809523\n",
            "roc_auc: 0.9866255144032922\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:266\n",
            "grav_train:266\n",
            "cont_val:67\n",
            "grav_val:67\n",
            "training data set_total：532\n",
            "validating data set_total：134\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.5805 Acc: 0.7049\n",
            "val Loss: 0.5039 Acc: 0.7985\n",
            "Validation loss decreased (inf --> 0.503883).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4195 Acc: 0.8289\n",
            "val Loss: 4.8151 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3127 Acc: 0.8684\n",
            "val Loss: 0.9990 Acc: 0.7388\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.3008 Acc: 0.8722\n",
            "val Loss: 0.4566 Acc: 0.8358\n",
            "Validation loss decreased (0.503883 --> 0.456563).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.1950 Acc: 0.9436\n",
            "val Loss: 0.4896 Acc: 0.8358\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1955 Acc: 0.9267\n",
            "val Loss: 0.6357 Acc: 0.8433\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1402 Acc: 0.9436\n",
            "val Loss: 0.7097 Acc: 0.8433\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1306 Acc: 0.9530\n",
            "val Loss: 0.5314 Acc: 0.8507\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1429 Acc: 0.9398\n",
            "val Loss: 0.3953 Acc: 0.8209\n",
            "Validation loss decreased (0.456563 --> 0.395305).  Saving model ...\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.0621 Acc: 0.9812\n",
            "val Loss: 0.3060 Acc: 0.8731\n",
            "Validation loss decreased (0.395305 --> 0.306001).  Saving model ...\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0409 Acc: 0.9887\n",
            "val Loss: 0.4459 Acc: 0.8582\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0785 Acc: 0.9737\n",
            "val Loss: 0.6614 Acc: 0.8582\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0524 Acc: 0.9812\n",
            "val Loss: 0.5476 Acc: 0.8358\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0471 Acc: 0.9906\n",
            "val Loss: 0.8547 Acc: 0.7910\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0273 Acc: 0.9925\n",
            "val Loss: 0.8638 Acc: 0.7687\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0152 Acc: 0.9981\n",
            "val Loss: 0.5945 Acc: 0.8507\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0455 Acc: 0.9868\n",
            "val Loss: 0.5844 Acc: 0.8507\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0533 Acc: 0.9831\n",
            "val Loss: 0.5027 Acc: 0.8731\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0310 Acc: 0.9906\n",
            "val Loss: 0.4619 Acc: 0.8209\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0246 Acc: 0.9906\n",
            "val Loss: 0.5104 Acc: 0.8358\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0266 Acc: 0.9925\n",
            "val Loss: 0.8539 Acc: 0.7836\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.0268 Acc: 0.9925\n",
            "val Loss: 0.8453 Acc: 0.7985\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0224 Acc: 0.9906\n",
            "val Loss: 0.6392 Acc: 0.8209\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0151 Acc: 0.9962\n",
            "val Loss: 1.0002 Acc: 0.7985\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.0102 Acc: 0.9962\n",
            "val Loss: 0.8910 Acc: 0.7836\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 5m 47s\n",
            "Best val Acc: 0.873134\n",
            "number of images: 108\n",
            "51 3 54 0\n",
            "Accuracy: 0.9722222222222222\n",
            "Precision (positive predictive value): 1.0\n",
            "Recall (sensitivity): 0.9444444444444444\n",
            "Specificity: 1.0\n",
            "F_value: 0.9714285714285714\n",
            "roc_auc: 0.9979423868312757\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:267\n",
            "grav_train:267\n",
            "cont_val:66\n",
            "grav_val:66\n",
            "training data set_total：534\n",
            "validating data set_total：132\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.5938 Acc: 0.6873\n",
            "val Loss: 0.7333 Acc: 0.7424\n",
            "Validation loss decreased (inf --> 0.733350).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.3896 Acc: 0.8258\n",
            "val Loss: 1.9096 Acc: 0.5303\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3169 Acc: 0.8708\n",
            "val Loss: 1.2126 Acc: 0.7955\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2731 Acc: 0.9101\n",
            "val Loss: 0.5430 Acc: 0.8864\n",
            "Validation loss decreased (0.733350 --> 0.542995).  Saving model ...\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2269 Acc: 0.9026\n",
            "val Loss: 0.8876 Acc: 0.7727\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1571 Acc: 0.9307\n",
            "val Loss: 0.4839 Acc: 0.8409\n",
            "Validation loss decreased (0.542995 --> 0.483927).  Saving model ...\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1032 Acc: 0.9588\n",
            "val Loss: 0.6979 Acc: 0.8333\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.1597 Acc: 0.9401\n",
            "val Loss: 0.7057 Acc: 0.8106\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1229 Acc: 0.9532\n",
            "val Loss: 0.4862 Acc: 0.8788\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.0982 Acc: 0.9644\n",
            "val Loss: 0.7336 Acc: 0.7879\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0429 Acc: 0.9850\n",
            "val Loss: 0.5879 Acc: 0.8636\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0449 Acc: 0.9831\n",
            "val Loss: 0.7972 Acc: 0.8182\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0443 Acc: 0.9794\n",
            "val Loss: 0.6586 Acc: 0.8409\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0722 Acc: 0.9700\n",
            "val Loss: 0.5678 Acc: 0.8561\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0795 Acc: 0.9663\n",
            "val Loss: 0.7058 Acc: 0.8030\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.0557 Acc: 0.9794\n",
            "val Loss: 0.6831 Acc: 0.8182\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0198 Acc: 0.9963\n",
            "val Loss: 0.6762 Acc: 0.8485\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0262 Acc: 0.9888\n",
            "val Loss: 0.8317 Acc: 0.8182\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0400 Acc: 0.9850\n",
            "val Loss: 0.7154 Acc: 0.8258\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0277 Acc: 0.9925\n",
            "val Loss: 0.6725 Acc: 0.8485\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0228 Acc: 0.9944\n",
            "val Loss: 0.6240 Acc: 0.8182\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 5m 7s\n",
            "Best val Acc: 0.886364\n",
            "number of images: 108\n",
            "46 8 53 1\n",
            "Accuracy: 0.9166666666666666\n",
            "Precision (positive predictive value): 0.9787234042553191\n",
            "Recall (sensitivity): 0.8518518518518519\n",
            "Specificity: 0.9814814814814815\n",
            "F_value: 0.9108910891089108\n",
            "roc_auc: 0.9722222222222223\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:267\n",
            "grav_train:267\n",
            "cont_val:66\n",
            "grav_val:66\n",
            "training data set_total：534\n",
            "validating data set_total：132\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.6102 Acc: 0.6667\n",
            "val Loss: 0.9886 Acc: 0.6212\n",
            "Validation loss decreased (inf --> 0.988627).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.4018 Acc: 0.8352\n",
            "val Loss: 1.4943 Acc: 0.5758\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.3660 Acc: 0.8446\n",
            "val Loss: 0.4506 Acc: 0.8636\n",
            "Validation loss decreased (0.988627 --> 0.450624).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.2580 Acc: 0.8914\n",
            "val Loss: 0.5662 Acc: 0.8561\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.2153 Acc: 0.9082\n",
            "val Loss: 0.8320 Acc: 0.7727\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.1774 Acc: 0.9288\n",
            "val Loss: 0.9209 Acc: 0.7500\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.1517 Acc: 0.9326\n",
            "val Loss: 0.3908 Acc: 0.8712\n",
            "Validation loss decreased (0.450624 --> 0.390849).  Saving model ...\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.0814 Acc: 0.9700\n",
            "val Loss: 0.6103 Acc: 0.8864\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.1224 Acc: 0.9438\n",
            "val Loss: 0.6165 Acc: 0.7652\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.1388 Acc: 0.9607\n",
            "val Loss: 0.5589 Acc: 0.8409\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.0798 Acc: 0.9757\n",
            "val Loss: 0.8017 Acc: 0.8106\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.0392 Acc: 0.9888\n",
            "val Loss: 0.4049 Acc: 0.8788\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.0573 Acc: 0.9794\n",
            "val Loss: 0.3397 Acc: 0.8864\n",
            "Validation loss decreased (0.390849 --> 0.339690).  Saving model ...\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.0429 Acc: 0.9850\n",
            "val Loss: 0.5944 Acc: 0.8864\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.0583 Acc: 0.9831\n",
            "val Loss: 2.6532 Acc: 0.5758\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.1008 Acc: 0.9625\n",
            "val Loss: 0.7253 Acc: 0.7955\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.0532 Acc: 0.9738\n",
            "val Loss: 0.5856 Acc: 0.8182\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.0424 Acc: 0.9831\n",
            "val Loss: 0.3766 Acc: 0.8939\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.0237 Acc: 0.9925\n",
            "val Loss: 0.4450 Acc: 0.8485\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.0325 Acc: 0.9869\n",
            "val Loss: 0.6943 Acc: 0.8409\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.0078 Acc: 1.0000\n",
            "val Loss: 0.5292 Acc: 0.8485\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.0053 Acc: 1.0000\n",
            "val Loss: 0.5059 Acc: 0.8788\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.0063 Acc: 0.9981\n",
            "val Loss: 0.5105 Acc: 0.8712\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.0065 Acc: 0.9981\n",
            "val Loss: 0.4518 Acc: 0.8939\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.0096 Acc: 0.9963\n",
            "val Loss: 0.5334 Acc: 0.8409\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.0106 Acc: 0.9944\n",
            "val Loss: 0.6787 Acc: 0.8636\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.0163 Acc: 0.9981\n",
            "val Loss: 0.6200 Acc: 0.8561\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.0117 Acc: 0.9981\n",
            "val Loss: 0.5670 Acc: 0.8561\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 6m 34s\n",
            "Best val Acc: 0.893939\n",
            "number of images: 108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "52 2 52 2\n",
            "Accuracy: 0.9629629629629629\n",
            "Precision (positive predictive value): 0.9629629629629629\n",
            "Recall (sensitivity): 0.9629629629629629\n",
            "Specificity: 0.9629629629629629\n",
            "F_value: 0.9629629629629629\n",
            "roc_auc: 0.9897119341563787\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                  0         1         2         3         4\n",
            "TP                               51        50        51        46        52\n",
            "TN                               52        53        54        53        52\n",
            "FP                                2         1         0         1         2\n",
            "FN                                3         4         3         8         2\n",
            "Accuracy                   0.953704  0.953704  0.972222  0.916667  0.962963\n",
            "Positive predictive value  0.962264  0.980392         1  0.978723  0.962963\n",
            "sensitity                  0.944444  0.925926  0.944444  0.851852  0.962963\n",
            "specificity                0.962963  0.981481         1  0.981481  0.962963\n",
            "F-value                    0.953271  0.952381  0.971429  0.910891  0.962963\n",
            "roc_auc                    0.992112  0.986626  0.997942  0.972222  0.989712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8a8q2pcQPWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "3ff8e8ea-f889-4cfa-a1e6-c46fc066994f"
      },
      "source": [
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          EfficientNet_32  ... EfficientNet_558\n",
            "TP                                    NaN  ...              NaN\n",
            "TN                                    NaN  ...              NaN\n",
            "FP                                    NaN  ...              NaN\n",
            "FN                                    NaN  ...              NaN\n",
            "Accuracy                              NaN  ...              NaN\n",
            "Positive predictive value             NaN  ...              NaN\n",
            "sensitity                             NaN  ...              NaN\n",
            "specificity                           NaN  ...              NaN\n",
            "F-value                               NaN  ...              NaN\n",
            "roc_auc                               NaN  ...              NaN\n",
            "\n",
            "[10 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPO-b2cAT0yF",
        "colab_type": "text"
      },
      "source": [
        "#**ネットワークの保存と読み込み**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMYyFQ6ATzyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワークの保存\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c744XUtfT6xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73a923de-e875-4e71-b4b5-1ae266557981"
      },
      "source": [
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}