{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled34.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNryI+YCenaKpYa7aj1n4aU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Resnet50_ImageNET_randomSeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJv9nZ-chhr5",
        "colab_type": "text"
      },
      "source": [
        "#**GravCont_Resnet_random**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDgbjW1whffo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnK5MLZRh1-9",
        "colab_type": "text"
      },
      "source": [
        "#**データセットの振り分け**\n",
        "\"content/learning/\"のフォルダ内に、\"tranin\"、\"val\"フォルダを作り、データを振り分ける"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AT3Nugz1hhmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#処理時間の計測\n",
        "start = time.time()\n",
        "\n",
        "path = '/content/drive/My Drive/Deep_learning/gravcont_processed/'\n",
        "classes = os.listdir(path)\n",
        "print(classes)\n",
        "\n",
        "#os.makedirs('/learning', exist_ok=True)\n",
        "\n",
        "list = [0] * len(classes)\n",
        "train = [0] * len(classes)\n",
        "val = [0] * len(classes)\n",
        "k=0\n",
        "for i in classes:\n",
        "    list[k] = os.listdir(path + i)\n",
        "    train[k] = random.sample(list[k], int(len(list[k])*0.8))\n",
        "    val[k] = [x for x in list[k] if x not in train[k]]\n",
        "    os.makedirs('learning/train/'+i, exist_ok=True) #trainのフォルダを作成\n",
        "    os.makedirs('learning/val/'+i, exist_ok=True) #valのフォルダを作成\n",
        "    for j in train[k]:\n",
        "          shutil.copy(path+i+'/'+j, 'learning/train/'+i)\n",
        "    for j in val[k]:\n",
        "          shutil.copy(path+i+'/'+j, 'learning/val/'+i)\n",
        "\n",
        "print('Process done!!')\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbUtKKwIhh2C",
        "colab_type": "text"
      },
      "source": [
        "#**画像のリサイズ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBnJ4rHZhhya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 入力画像の前処理をするクラス\n",
        "# 訓練時と推論時で処理が異なる\n",
        "\n",
        "\"\"\"\n",
        "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "\"\"\"\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.6,1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/learning'\n",
        "n_samples = len(data_dir)\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "print(class_names)\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_train:\"+str(len(os.listdir(path='/content/learning/train/'+class_names[k]))))\n",
        "    k+=1\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_val:\"+str(len(os.listdir(path='/content/learning/val/'+class_names[k]))))\n",
        "    k+=1\n",
        "\n",
        "print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "print(\"validating data set_total：\"+str(len(image_datasets['val'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfwvCFqIiLSi",
        "colab_type": "text"
      },
      "source": [
        "#**少数の画像を可視化**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viFqKF44iK5U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekKVMeOniT3D",
        "colab_type": "text"
      },
      "source": [
        "#**Train models with early stopping class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLcA6Ti-hh7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, scheduler, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "                scheduler.step()\n",
        "                print('epoch:{}, lr:{}'.format(epoch, scheduler.get_lr()[0]))\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJD4MpFHhh-x",
        "colab_type": "text"
      },
      "source": [
        "#**ConvNetを再調整する**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxHp1EgghiC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ft = models.resnet50(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "#exp_lr_scheduler = lr_scheduler.MultiStepLR(optimizer_ft, milestones=[5, 12, 20, 30], gamma=0.1)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22HX9zMIhiGq",
        "colab_type": "text"
      },
      "source": [
        "#**訓練と評価**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HcKq6SGhiKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, patience=5, num_epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upboR93lhiRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the loss as the network trained\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
        "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
        "\n",
        "# find position of lowest validation loss\n",
        "minposs = valid_loss.index(min(valid_loss))+1 \n",
        "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
        "\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.ylim(0, 1.0) # consistent scale\n",
        "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "fig.savefig('loss_plot.png', bbox_inches='tight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YomEGRV2hiZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_model(model_ft)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YDKCGRji5DL",
        "colab_type": "text"
      },
      "source": [
        "#**データセットの振り分けを保存**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igUyt4Jti4Y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#処理時間の計測\n",
        "start = time.time()\n",
        "\n",
        "path = '/content/drive/My Drive/Deep_learning/gravcont_processed/'\n",
        "seed_path = '/content/drive/My Drive/Deep_learning/gravcont_seed_'+str(manualSeed)\n",
        "\n",
        "if os.path.exists(seed_path):\n",
        "    shutil.rmtree(seed_path)\n",
        "os.makedirs(seed_path, exist_ok=True)\n",
        "\n",
        "classes = os.listdir(path)\n",
        "print(classes)\n",
        "\n",
        "\n",
        "list = [0] * len(classes)\n",
        "train = [0] * len(classes)\n",
        "val = [0] * len(classes)\n",
        "k=0\n",
        "for i in classes:\n",
        "    list[k] = os.listdir(path + i)\n",
        "    train[k] = random.sample(list[k], int(len(list[k])*0.8))\n",
        "    val[k] = [x for x in list[k] if x not in train[k]]\n",
        "    os.makedirs(seed_path +'/train/'+i, exist_ok=True) #trainのフォルダを作成\n",
        "    os.makedirs(seed_path +'/val/'+i, exist_ok=True) #valのフォルダを作成\n",
        "    for j in train[k]:\n",
        "          shutil.copy(path+i+'/'+j, seed_path +'/train/'+i)\n",
        "    for j in val[k]:\n",
        "          shutil.copy(path+i+'/'+j, seed_path +'/val/'+i)\n",
        "\n",
        "print('Process done!!')\n",
        "elapsed_time = time.time() - start\n",
        "print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS7jyEWbjVRe",
        "colab_type": "text"
      },
      "source": [
        "#フォルダとその中のファイルを削除するスクリプト"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M6lAV7zhigJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#フォルダを削除するスクリプト\n",
        "directory = '/content/drive/My Drive/Deep_learning/gravcont_seed_1234'\n",
        "files = os.listdir(directory)\n",
        "print(directory)\n",
        "print(files)\n",
        "\n",
        "for i in files:\n",
        "    try:\n",
        "        os.remove(directory+'/'+i)\n",
        "    except IsADirectoryError:\n",
        "        pass\n",
        "\n",
        "shutil.rmtree(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}