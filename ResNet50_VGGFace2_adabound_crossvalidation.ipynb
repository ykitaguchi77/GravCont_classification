{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/ResNet50_VGGFace2_adabound_crossvalidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a4ZBlqPNdU",
        "colab_type": "text"
      },
      "source": [
        "#**GravCont: EfficientNet_b4_ImageNet**\n",
        "ValidationとTestに分けて解析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgM-Y7SVPNkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "2be139a1-6475-4698-cacf-377a2cbc26c2"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_optimizer in /usr/local/lib/python3.6/dist-packages (0.0.1a15)\n",
            "Requirement already satisfied: pytorch-ranger>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (0.1.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Random Seed:  1234\n",
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzp_09fWPNoU",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFTBpTsjbI7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Resnet50_ft_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Resnet50_ft_dag, self).__init__()\n",
        "        self.meta = {'mean': [131.0912, 103.8827, 91.4953],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv1_relu_7x7_s2 = nn.ReLU()\n",
        "        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
        "        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_3x3_relu = nn.ReLU()\n",
        "        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_3x3_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_3x3_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_3x3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_3x3_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_3x3_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_3x3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_3x3_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_3x3_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_3x3_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_3x3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_3x3_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_3x3_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_3x3_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_3x3_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_3x3_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_relu = nn.ReLU()\n",
        "        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
        "        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))\n",
        "\n",
        "    def forward(self, data):\n",
        "        conv1_7x7_s2 = self.conv1_7x7_s2(data)\n",
        "        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)\n",
        "        conv1_7x7_s2_bnxx = self.conv1_relu_7x7_s2(conv1_7x7_s2_bn)\n",
        "        pool1_3x3_s2 = self.pool1_3x3_s2(conv1_7x7_s2_bnxx)\n",
        "        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(pool1_3x3_s2)\n",
        "        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)\n",
        "        conv2_1_1x1_reduce_bnxx = self.conv2_1_1x1_reduce_relu(conv2_1_1x1_reduce_bn)\n",
        "        conv2_1_3x3 = self.conv2_1_3x3(conv2_1_1x1_reduce_bnxx)\n",
        "        conv2_1_3x3_bn = self.conv2_1_3x3_bn(conv2_1_3x3)\n",
        "        conv2_1_3x3_bnxx = self.conv2_1_3x3_relu(conv2_1_3x3_bn)\n",
        "        conv2_1_1x1_increase = self.conv2_1_1x1_increase(conv2_1_3x3_bnxx)\n",
        "        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)\n",
        "        conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)\n",
        "        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)\n",
        "        conv2_1 = torch.add(conv2_1_1x1_proj_bn, 1, conv2_1_1x1_increase_bn)\n",
        "        conv2_1x = self.conv2_1_relu(conv2_1)\n",
        "        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(conv2_1x)\n",
        "        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)\n",
        "        conv2_2_1x1_reduce_bnxx = self.conv2_2_1x1_reduce_relu(conv2_2_1x1_reduce_bn)\n",
        "        conv2_2_3x3 = self.conv2_2_3x3(conv2_2_1x1_reduce_bnxx)\n",
        "        conv2_2_3x3_bn = self.conv2_2_3x3_bn(conv2_2_3x3)\n",
        "        conv2_2_3x3_bnxx = self.conv2_2_3x3_relu(conv2_2_3x3_bn)\n",
        "        conv2_2_1x1_increase = self.conv2_2_1x1_increase(conv2_2_3x3_bnxx)\n",
        "        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)\n",
        "        conv2_2 = torch.add(conv2_1x, 1, conv2_2_1x1_increase_bn)\n",
        "        conv2_2x = self.conv2_2_relu(conv2_2)\n",
        "        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(conv2_2x)\n",
        "        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)\n",
        "        conv2_3_1x1_reduce_bnxx = self.conv2_3_1x1_reduce_relu(conv2_3_1x1_reduce_bn)\n",
        "        conv2_3_3x3 = self.conv2_3_3x3(conv2_3_1x1_reduce_bnxx)\n",
        "        conv2_3_3x3_bn = self.conv2_3_3x3_bn(conv2_3_3x3)\n",
        "        conv2_3_3x3_bnxx = self.conv2_3_3x3_relu(conv2_3_3x3_bn)\n",
        "        conv2_3_1x1_increase = self.conv2_3_1x1_increase(conv2_3_3x3_bnxx)\n",
        "        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)\n",
        "        conv2_3 = torch.add(conv2_2x, 1, conv2_3_1x1_increase_bn)\n",
        "        conv2_3x = self.conv2_3_relu(conv2_3)\n",
        "        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(conv2_3x)\n",
        "        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)\n",
        "        conv3_1_1x1_reduce_bnxx = self.conv3_1_1x1_reduce_relu(conv3_1_1x1_reduce_bn)\n",
        "        conv3_1_3x3 = self.conv3_1_3x3(conv3_1_1x1_reduce_bnxx)\n",
        "        conv3_1_3x3_bn = self.conv3_1_3x3_bn(conv3_1_3x3)\n",
        "        conv3_1_3x3_bnxx = self.conv3_1_3x3_relu(conv3_1_3x3_bn)\n",
        "        conv3_1_1x1_increase = self.conv3_1_1x1_increase(conv3_1_3x3_bnxx)\n",
        "        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)\n",
        "        conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)\n",
        "        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)\n",
        "        conv3_1 = torch.add(conv3_1_1x1_proj_bn, 1, conv3_1_1x1_increase_bn)\n",
        "        conv3_1x = self.conv3_1_relu(conv3_1)\n",
        "        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(conv3_1x)\n",
        "        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)\n",
        "        conv3_2_1x1_reduce_bnxx = self.conv3_2_1x1_reduce_relu(conv3_2_1x1_reduce_bn)\n",
        "        conv3_2_3x3 = self.conv3_2_3x3(conv3_2_1x1_reduce_bnxx)\n",
        "        conv3_2_3x3_bn = self.conv3_2_3x3_bn(conv3_2_3x3)\n",
        "        conv3_2_3x3_bnxx = self.conv3_2_3x3_relu(conv3_2_3x3_bn)\n",
        "        conv3_2_1x1_increase = self.conv3_2_1x1_increase(conv3_2_3x3_bnxx)\n",
        "        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)\n",
        "        conv3_2 = torch.add(conv3_1x, 1, conv3_2_1x1_increase_bn)\n",
        "        conv3_2x = self.conv3_2_relu(conv3_2)\n",
        "        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(conv3_2x)\n",
        "        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)\n",
        "        conv3_3_1x1_reduce_bnxx = self.conv3_3_1x1_reduce_relu(conv3_3_1x1_reduce_bn)\n",
        "        conv3_3_3x3 = self.conv3_3_3x3(conv3_3_1x1_reduce_bnxx)\n",
        "        conv3_3_3x3_bn = self.conv3_3_3x3_bn(conv3_3_3x3)\n",
        "        conv3_3_3x3_bnxx = self.conv3_3_3x3_relu(conv3_3_3x3_bn)\n",
        "        conv3_3_1x1_increase = self.conv3_3_1x1_increase(conv3_3_3x3_bnxx)\n",
        "        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)\n",
        "        conv3_3 = torch.add(conv3_2x, 1, conv3_3_1x1_increase_bn)\n",
        "        conv3_3x = self.conv3_3_relu(conv3_3)\n",
        "        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(conv3_3x)\n",
        "        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)\n",
        "        conv3_4_1x1_reduce_bnxx = self.conv3_4_1x1_reduce_relu(conv3_4_1x1_reduce_bn)\n",
        "        conv3_4_3x3 = self.conv3_4_3x3(conv3_4_1x1_reduce_bnxx)\n",
        "        conv3_4_3x3_bn = self.conv3_4_3x3_bn(conv3_4_3x3)\n",
        "        conv3_4_3x3_bnxx = self.conv3_4_3x3_relu(conv3_4_3x3_bn)\n",
        "        conv3_4_1x1_increase = self.conv3_4_1x1_increase(conv3_4_3x3_bnxx)\n",
        "        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)\n",
        "        conv3_4 = torch.add(conv3_3x, 1, conv3_4_1x1_increase_bn)\n",
        "        conv3_4x = self.conv3_4_relu(conv3_4)\n",
        "        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(conv3_4x)\n",
        "        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)\n",
        "        conv4_1_1x1_reduce_bnxx = self.conv4_1_1x1_reduce_relu(conv4_1_1x1_reduce_bn)\n",
        "        conv4_1_3x3 = self.conv4_1_3x3(conv4_1_1x1_reduce_bnxx)\n",
        "        conv4_1_3x3_bn = self.conv4_1_3x3_bn(conv4_1_3x3)\n",
        "        conv4_1_3x3_bnxx = self.conv4_1_3x3_relu(conv4_1_3x3_bn)\n",
        "        conv4_1_1x1_increase = self.conv4_1_1x1_increase(conv4_1_3x3_bnxx)\n",
        "        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)\n",
        "        conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)\n",
        "        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)\n",
        "        conv4_1 = torch.add(conv4_1_1x1_proj_bn, 1, conv4_1_1x1_increase_bn)\n",
        "        conv4_1x = self.conv4_1_relu(conv4_1)\n",
        "        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(conv4_1x)\n",
        "        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)\n",
        "        conv4_2_1x1_reduce_bnxx = self.conv4_2_1x1_reduce_relu(conv4_2_1x1_reduce_bn)\n",
        "        conv4_2_3x3 = self.conv4_2_3x3(conv4_2_1x1_reduce_bnxx)\n",
        "        conv4_2_3x3_bn = self.conv4_2_3x3_bn(conv4_2_3x3)\n",
        "        conv4_2_3x3_bnxx = self.conv4_2_3x3_relu(conv4_2_3x3_bn)\n",
        "        conv4_2_1x1_increase = self.conv4_2_1x1_increase(conv4_2_3x3_bnxx)\n",
        "        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)\n",
        "        conv4_2 = torch.add(conv4_1x, 1, conv4_2_1x1_increase_bn)\n",
        "        conv4_2x = self.conv4_2_relu(conv4_2)\n",
        "        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(conv4_2x)\n",
        "        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)\n",
        "        conv4_3_1x1_reduce_bnxx = self.conv4_3_1x1_reduce_relu(conv4_3_1x1_reduce_bn)\n",
        "        conv4_3_3x3 = self.conv4_3_3x3(conv4_3_1x1_reduce_bnxx)\n",
        "        conv4_3_3x3_bn = self.conv4_3_3x3_bn(conv4_3_3x3)\n",
        "        conv4_3_3x3_bnxx = self.conv4_3_3x3_relu(conv4_3_3x3_bn)\n",
        "        conv4_3_1x1_increase = self.conv4_3_1x1_increase(conv4_3_3x3_bnxx)\n",
        "        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)\n",
        "        conv4_3 = torch.add(conv4_2x, 1, conv4_3_1x1_increase_bn)\n",
        "        conv4_3x = self.conv4_3_relu(conv4_3)\n",
        "        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(conv4_3x)\n",
        "        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)\n",
        "        conv4_4_1x1_reduce_bnxx = self.conv4_4_1x1_reduce_relu(conv4_4_1x1_reduce_bn)\n",
        "        conv4_4_3x3 = self.conv4_4_3x3(conv4_4_1x1_reduce_bnxx)\n",
        "        conv4_4_3x3_bn = self.conv4_4_3x3_bn(conv4_4_3x3)\n",
        "        conv4_4_3x3_bnxx = self.conv4_4_3x3_relu(conv4_4_3x3_bn)\n",
        "        conv4_4_1x1_increase = self.conv4_4_1x1_increase(conv4_4_3x3_bnxx)\n",
        "        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)\n",
        "        conv4_4 = torch.add(conv4_3x, 1, conv4_4_1x1_increase_bn)\n",
        "        conv4_4x = self.conv4_4_relu(conv4_4)\n",
        "        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(conv4_4x)\n",
        "        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)\n",
        "        conv4_5_1x1_reduce_bnxx = self.conv4_5_1x1_reduce_relu(conv4_5_1x1_reduce_bn)\n",
        "        conv4_5_3x3 = self.conv4_5_3x3(conv4_5_1x1_reduce_bnxx)\n",
        "        conv4_5_3x3_bn = self.conv4_5_3x3_bn(conv4_5_3x3)\n",
        "        conv4_5_3x3_bnxx = self.conv4_5_3x3_relu(conv4_5_3x3_bn)\n",
        "        conv4_5_1x1_increase = self.conv4_5_1x1_increase(conv4_5_3x3_bnxx)\n",
        "        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)\n",
        "        conv4_5 = torch.add(conv4_4x, 1, conv4_5_1x1_increase_bn)\n",
        "        conv4_5x = self.conv4_5_relu(conv4_5)\n",
        "        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(conv4_5x)\n",
        "        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)\n",
        "        conv4_6_1x1_reduce_bnxx = self.conv4_6_1x1_reduce_relu(conv4_6_1x1_reduce_bn)\n",
        "        conv4_6_3x3 = self.conv4_6_3x3(conv4_6_1x1_reduce_bnxx)\n",
        "        conv4_6_3x3_bn = self.conv4_6_3x3_bn(conv4_6_3x3)\n",
        "        conv4_6_3x3_bnxx = self.conv4_6_3x3_relu(conv4_6_3x3_bn)\n",
        "        conv4_6_1x1_increase = self.conv4_6_1x1_increase(conv4_6_3x3_bnxx)\n",
        "        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)\n",
        "        conv4_6 = torch.add(conv4_5x, 1, conv4_6_1x1_increase_bn)\n",
        "        conv4_6x = self.conv4_6_relu(conv4_6)\n",
        "        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(conv4_6x)\n",
        "        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)\n",
        "        conv5_1_1x1_reduce_bnxx = self.conv5_1_1x1_reduce_relu(conv5_1_1x1_reduce_bn)\n",
        "        conv5_1_3x3 = self.conv5_1_3x3(conv5_1_1x1_reduce_bnxx)\n",
        "        conv5_1_3x3_bn = self.conv5_1_3x3_bn(conv5_1_3x3)\n",
        "        conv5_1_3x3_bnxx = self.conv5_1_3x3_relu(conv5_1_3x3_bn)\n",
        "        conv5_1_1x1_increase = self.conv5_1_1x1_increase(conv5_1_3x3_bnxx)\n",
        "        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)\n",
        "        conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)\n",
        "        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)\n",
        "        conv5_1 = torch.add(conv5_1_1x1_proj_bn, 1, conv5_1_1x1_increase_bn)\n",
        "        conv5_1x = self.conv5_1_relu(conv5_1)\n",
        "        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(conv5_1x)\n",
        "        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)\n",
        "        conv5_2_1x1_reduce_bnxx = self.conv5_2_1x1_reduce_relu(conv5_2_1x1_reduce_bn)\n",
        "        conv5_2_3x3 = self.conv5_2_3x3(conv5_2_1x1_reduce_bnxx)\n",
        "        conv5_2_3x3_bn = self.conv5_2_3x3_bn(conv5_2_3x3)\n",
        "        conv5_2_3x3_bnxx = self.conv5_2_3x3_relu(conv5_2_3x3_bn)\n",
        "        conv5_2_1x1_increase = self.conv5_2_1x1_increase(conv5_2_3x3_bnxx)\n",
        "        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)\n",
        "        conv5_2 = torch.add(conv5_1x, 1, conv5_2_1x1_increase_bn)\n",
        "        conv5_2x = self.conv5_2_relu(conv5_2)\n",
        "        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(conv5_2x)\n",
        "        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)\n",
        "        conv5_3_1x1_reduce_bnxx = self.conv5_3_1x1_reduce_relu(conv5_3_1x1_reduce_bn)\n",
        "        conv5_3_3x3 = self.conv5_3_3x3(conv5_3_1x1_reduce_bnxx)\n",
        "        conv5_3_3x3_bn = self.conv5_3_3x3_bn(conv5_3_3x3)\n",
        "        conv5_3_3x3_bnxx = self.conv5_3_3x3_relu(conv5_3_3x3_bn)\n",
        "        conv5_3_1x1_increase = self.conv5_3_1x1_increase(conv5_3_3x3_bnxx)\n",
        "        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)\n",
        "        conv5_3 = torch.add(conv5_2x, 1, conv5_3_1x1_increase_bn)\n",
        "        conv5_3x = self.conv5_3_relu(conv5_3)\n",
        "        pool5_7x7_s1 = self.pool5_7x7_s1(conv5_3x)\n",
        "        classifier_preflatten = self.classifier(pool5_7x7_s1)\n",
        "        classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)\n",
        "        #return classifier, pool5_7x7_s1 　出力を変更しておかないと次元が合わないと言われる\n",
        "        return classifier\n",
        "\n",
        "def resnet50_ft_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Resnet50_ft_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sJV06qPNsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(data_dir):\n",
        "    # 入力画像の前処理をするクラス\n",
        "    # 訓練時と推論時で処理が異なる\n",
        "\n",
        "    \"\"\"\n",
        "        画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "        画像のサイズをリサイズし、色を標準化する。\n",
        "        訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        resize : int\n",
        "            リサイズ先の画像の大きさ。\n",
        "        mean : (R, G, B)\n",
        "            各色チャネルの平均値。\n",
        "        std : (R, G, B)\n",
        "            各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    data_dir = data_dir\n",
        "    n_samples = len(data_dir)\n",
        "\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in ['train', 'val']}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                                shuffle=True, num_workers=4)\n",
        "                  for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "    print(class_names)\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_train:\"+str(len(os.listdir(path=data_dir + '/train/'+class_names[k]))))\n",
        "        k+=1\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_val:\"+str(len(os.listdir(path=data_dir + '/val/'+class_names[k]))))\n",
        "        k+=1\n",
        "\n",
        "    print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "    print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "    \n",
        "    return image_datasets, dataloaders, dataset_sizes, class_names, device\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "def getBatch(dataloaders):    \n",
        "    # Get a batch of training data\n",
        "    inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "    # Make a grid from batch\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "    #imshow(out, title=[class_names[x] for x in classes])\n",
        "    return(inputs, classes)\n",
        "\n",
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "def training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50):\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=patience, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "    \"\"\"\n",
        "    #probalilityを計算する\n",
        "    pred_prob = torch.topk(nn.Softmax(dim=1)(output), 1)[0]\n",
        "    pred_class = torch.topk(nn.Softmax(dim=1)(output), 1)[1]\n",
        "    if pred_class == 1:\n",
        "        pred_prob = pred_prob\n",
        "    elif pred_class == 0:\n",
        "        pred_prob = 1- pred_prob\n",
        "    return(model_pred, pred_prob)  #class_nameの番号で出力される\n",
        "    \"\"\"\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\"\"\"\n",
        "・True positive (TN)\n",
        "・False positive (FP)\n",
        "・True negative (TN)\n",
        "・False negative (FN)\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "Precision = TP/(FP + TP) ※positive predictive value\n",
        "Recall = TP/(TP + FN)　※sensitivity\n",
        "Specificity = TN/(FP + TN)\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)\n",
        "\"\"\"\n",
        "\n",
        "def evaluation(model_ft, testset_dir):\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #testデータセット内のファイル名を取得\n",
        "    image_path = glob.glob(testset_dir + \"/*/*\")\n",
        "    #random.shuffle(image_path)  #表示順をランダムにする\n",
        "    print('number of images: ' +str(len(image_path)))\n",
        "\n",
        "\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1     \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    return TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob\n",
        "\n",
        "\n",
        "def make_csv(roc_label_list):\n",
        "    #csvのdata tableを作成\n",
        "    pd.set_option('display.max_rows', 800)  # 省略なしで表示\n",
        "    #columns1 = [\"EfficientNet_32\", \"EfficientNet_64\", \"EfficientNet_128\", \"EfficientNet_256\", \"EfficientNet_512\", \"EfficientNet_558\"]\n",
        "    roc_label_list.extend([\"avg\", \"std\"])\n",
        "    index1 = [\"TP\",\"TN\",\"FP\",\"FN\",\"Accuracy\",\"Positive predictive value\",\"sensitity\",\"specificity\",\"F-value\",\"roc_auc\"]\n",
        "    df = pd.DataFrame(index=index1, columns=roc_label_list)\n",
        "    return df\n",
        "\n",
        "def write_csv(df, col, TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc):\n",
        "    df.iloc[0:10, col] = TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc \n",
        "    #print(df)\n",
        "\n",
        "    # CSVとして出力\n",
        "    #df2.to_csv(\"/content/drive/My Drive/Grav_bootcamp/Posttrain_model_eval_result.csv\",encoding=\"shift_jis\")\n",
        "    return df\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == 'cont':\n",
        "                  y_true.append(0)\n",
        "            elif i == 'grav':\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == 'cont':\n",
        "              y_true.append(0)\n",
        "        elif i == 'grav':\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "def calcurate_ave_std(df, fold):\n",
        "    for i in range(5):\n",
        "        df.iloc[i,fold] = df[i,0:5].mean \n",
        "\n",
        "def convnet():\n",
        "    #Pretrained dataと結びつける\n",
        "    model_ft = Resnet50_ft_dag()\n",
        "    model_ft.load_state_dict(torch.load('/content/drive/My Drive/Grav_bootcamp/resnet50_ft_dag.pth'))\n",
        "    #最終結合層のリセットと付け替え(全結合層を2つに)\n",
        "    #model_ft.classifier = nn.Conv2d(2048, len(class_names), kernel_size=[1,1], stride=(1,1), bias = False)\n",
        "    model_ft.classifier = nn.Linear(2048, 2)\n",
        "    model_ft.classifier = nn.Sequential(*([Flatten()] + list(model_ft.children())[-1:])) #Flattenを挿入\n",
        "\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    optimizer_ft = optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=0,\n",
        "        amsbound=False,\n",
        "    )\n",
        "    return (model_ft, criterion, optimizer_ft)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGfUwQXv6Jc",
        "colab_type": "text"
      },
      "source": [
        "#**まとめて解析**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM2VMXltwBs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2c22da8-af91-4cc1-9c9e-b7ec1352e7d9"
      },
      "source": [
        "# 出力名を記入\n",
        "out_name = \"ResNet50_VGGFace2_32\"\n",
        "\n",
        "#create data_dir_list\n",
        "data_dir = '/content/drive/My Drive/crossvalidation/32'\n",
        "fold = len(os.listdir(data_dir))\n",
        "print(str(fold)+'-fold cross validation')\n",
        "\n",
        "\n",
        "data_dir_list = [0]*fold\n",
        "\n",
        "for i in range(fold):\n",
        "    data_dir_list[i] = data_dir + '/' + str(i)\n",
        "    print(data_dir_list[i])\n",
        "\n",
        "#create roc_label_list\n",
        "roc_label_list = [0]*fold\n",
        "roc_label_list = list(range(fold))\n",
        "#print(roc_label_list)\n",
        "\n",
        "\n",
        "\n",
        "df = make_csv(roc_label_list)\n",
        "\n",
        "label_list_list, model_pred_prob_list, Y_TRUE, Y_SCORE = [],[],[],[]\n",
        "\n",
        "#print(data_dir_list)\n",
        "#print(roc_label_list)\n",
        "\n",
        "for i, t in enumerate(zip(data_dir_list, roc_label_list)):\n",
        "\n",
        "    image_datasets, dataloaders, dataset_sizes, class_names, device = pre_process(t[0]) #path\n",
        "    inputs, classes = getBatch(dataloaders)\n",
        "    model_ft, criterion, optimizer_ft = convnet()\n",
        "    training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50)  \n",
        "    TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob = evaluation(model_ft, '/content/drive/My Drive/Grav_bootcamp/Posttrain_250px')\n",
        "    roc_auc, y_true, y_score = calculate_auc(label_list, model_pred_prob)\n",
        "    Y_TRUE.append(y_true)\n",
        "    Y_SCORE.append(y_score)\n",
        "    df = write_csv(df, i,TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, roc_auc) #numberをcsvの行として指定\n",
        "\n",
        "    label_list_list.append(label_list)\n",
        "    model_pred_prob_list.append(model_pred_prob)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#Draw ROC curve\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list))\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "#それぞれの項目の平均を計算しcsvに追記する\n",
        "df.iloc[0:4,fold], df.iloc[9,fold]   = df.mean(axis=1)[0:4], df.mean(axis=1)[9] \n",
        "df.iloc[0:10,fold+1] = df.std(axis=1)[0:10]\n",
        "TP,TN,FP,FN = df.mean(axis=1)[0:4]\n",
        "df.iloc[4:9,fold] = calculateAccuracy (TP, TN, FP, FN)\n",
        "print(df)\n",
        "\n",
        "# CSVとして出力\n",
        "df.to_csv(\"/content/drive/My Drive/Grav_bootcamp/crossvaridation_\" + out_name + \".csv\",encoding=\"shift_jis\")\n",
        "\n",
        "#ROC_curveを保存\n",
        "fig.savefig(\"/content/drive/My Drive/Grav_bootcamp/crossvaridation_\" + out_name +\".png\")\n",
        "\n",
        "#Save ROC data\n",
        "with open(\"/content/drive/My Drive/Grav_bootcamp/ROCdata_\"+out_name+\".csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for i, t in enumerate(zip(Y_TRUE, Y_SCORE)):\n",
        "        writer.writerow([t[0],t[1]])\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5-fold cross validation\n",
            "/content/drive/My Drive/crossvalidation/32/0\n",
            "/content/drive/My Drive/crossvalidation/32/1\n",
            "/content/drive/My Drive/crossvalidation/32/2\n",
            "/content/drive/My Drive/crossvalidation/32/3\n",
            "/content/drive/My Drive/crossvalidation/32/4\n",
            "['cont', 'grav']\n",
            "cont_train:12\n",
            "grav_train:12\n",
            "cont_val:4\n",
            "grav_val:4\n",
            "training data set_total：24\n",
            "validating data set_total：8\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 0.5916 Acc: 0.5833\n",
            "val Loss: 134.5702 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 134.570221).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 3.4591 Acc: 0.6250\n",
            "val Loss: 2.1171 Acc: 0.5000\n",
            "Validation loss decreased (134.570221 --> 2.117140).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 1.4950 Acc: 0.5000\n",
            "val Loss: 1.8193 Acc: 0.5000\n",
            "Validation loss decreased (2.117140 --> 1.819279).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 1.1071 Acc: 0.6250\n",
            "val Loss: 15.9915 Acc: 0.3750\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.6437 Acc: 0.7083\n",
            "val Loss: 43.4630 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 1.0438 Acc: 0.5833\n",
            "val Loss: 47.8868 Acc: 0.5000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.6051 Acc: 0.5000\n",
            "val Loss: 25.2742 Acc: 0.5000\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.6815 Acc: 0.5833\n",
            "val Loss: 3.0582 Acc: 0.5000\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 1.1202 Acc: 0.4583\n",
            "val Loss: 3.6866 Acc: 0.6250\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.6757 Acc: 0.6667\n",
            "val Loss: 10.6880 Acc: 0.5000\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.6821 Acc: 0.7083\n",
            "val Loss: 15.6345 Acc: 0.5000\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 1.1813 Acc: 0.5417\n",
            "val Loss: 22.3792 Acc: 0.6250\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.6114 Acc: 0.6667\n",
            "val Loss: 20.7433 Acc: 0.5000\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.5574 Acc: 0.6667\n",
            "val Loss: 19.1691 Acc: 0.5000\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.5782 Acc: 0.6667\n",
            "val Loss: 14.8747 Acc: 0.5000\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.5913 Acc: 0.6667\n",
            "val Loss: 15.1341 Acc: 0.6250\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.6200 Acc: 0.6250\n",
            "val Loss: 11.3791 Acc: 0.6250\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.7900 Acc: 0.6667\n",
            "val Loss: 13.5827 Acc: 0.6250\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 0m 24s\n",
            "Best val Acc: 0.625000\n",
            "number of images: 108\n",
            "9 45 47 7\n",
            "Accuracy: 0.5185185185185185\n",
            "Precision (positive predictive value): 0.5625\n",
            "Recall (sensitivity): 0.16666666666666666\n",
            "Specificity: 0.8703703703703703\n",
            "F_value: 0.2571428571428572\n",
            "roc_auc: 0.5183470507544582\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:13\n",
            "grav_train:13\n",
            "cont_val:3\n",
            "grav_val:3\n",
            "training data set_total：26\n",
            "validating data set_total：6\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.4140 Acc: 0.4615\n",
            "val Loss: 1.8287 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 1.828653).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 1.8119 Acc: 0.4615\n",
            "val Loss: 0.9447 Acc: 0.5000\n",
            "Validation loss decreased (1.828653 --> 0.944668).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 1.7031 Acc: 0.5000\n",
            "val Loss: 1.4328 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.9001 Acc: 0.5385\n",
            "val Loss: 1.5087 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 1.1064 Acc: 0.7692\n",
            "val Loss: 1.9467 Acc: 0.5000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 1.5867 Acc: 0.6538\n",
            "val Loss: 2.5484 Acc: 0.5000\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 1.0462 Acc: 0.5769\n",
            "val Loss: 3.0064 Acc: 0.5000\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.6721 Acc: 0.6538\n",
            "val Loss: 3.1650 Acc: 0.5000\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.5215 Acc: 0.7308\n",
            "val Loss: 2.9876 Acc: 0.5000\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.5583 Acc: 0.6923\n",
            "val Loss: 1.1244 Acc: 0.5000\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.7671 Acc: 0.6538\n",
            "val Loss: 0.5883 Acc: 0.6667\n",
            "Validation loss decreased (0.944668 --> 0.588346).  Saving model ...\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.7892 Acc: 0.5769\n",
            "val Loss: 0.8574 Acc: 0.6667\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.6029 Acc: 0.8077\n",
            "val Loss: 1.4693 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.8684 Acc: 0.6154\n",
            "val Loss: 1.4738 Acc: 0.5000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.7577 Acc: 0.6538\n",
            "val Loss: 0.5806 Acc: 0.6667\n",
            "Validation loss decreased (0.588346 --> 0.580554).  Saving model ...\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.7360 Acc: 0.6154\n",
            "val Loss: 0.7353 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.7097 Acc: 0.5000\n",
            "val Loss: 0.5991 Acc: 0.6667\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.7162 Acc: 0.5769\n",
            "val Loss: 1.0471 Acc: 0.5000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.5807 Acc: 0.7308\n",
            "val Loss: 1.1672 Acc: 0.5000\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.5773 Acc: 0.6923\n",
            "val Loss: 1.0201 Acc: 0.5000\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.5222 Acc: 0.7692\n",
            "val Loss: 0.9362 Acc: 0.5000\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.5380 Acc: 0.6538\n",
            "val Loss: 0.9052 Acc: 0.6667\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.4140 Acc: 0.8077\n",
            "val Loss: 0.9523 Acc: 0.6667\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.4313 Acc: 0.8462\n",
            "val Loss: 0.9989 Acc: 0.6667\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.4097 Acc: 0.8462\n",
            "val Loss: 1.6578 Acc: 0.6667\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.5560 Acc: 0.7308\n",
            "val Loss: 1.9020 Acc: 0.6667\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.3307 Acc: 0.9231\n",
            "val Loss: 1.5994 Acc: 0.6667\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.4124 Acc: 0.8462\n",
            "val Loss: 2.2095 Acc: 0.6667\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.4254 Acc: 0.9231\n",
            "val Loss: 2.3348 Acc: 0.5000\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.4376 Acc: 0.7308\n",
            "val Loss: 2.8614 Acc: 0.6667\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 0m 34s\n",
            "Best val Acc: 0.666667\n",
            "number of images: 108\n",
            "42 12 15 39\n",
            "Accuracy: 0.5277777777777778\n",
            "Precision (positive predictive value): 0.5185185185185185\n",
            "Recall (sensitivity): 0.7777777777777778\n",
            "Specificity: 0.2777777777777778\n",
            "F_value: 0.6222222222222222\n",
            "roc_auc: 0.46570644718792864\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:13\n",
            "grav_train:13\n",
            "cont_val:3\n",
            "grav_val:3\n",
            "training data set_total：26\n",
            "validating data set_total：6\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.3749 Acc: 0.4231\n",
            "val Loss: 0.7537 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 0.753747).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 1.5022 Acc: 0.5000\n",
            "val Loss: 0.7758 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 1.6249 Acc: 0.5385\n",
            "val Loss: 1.5698 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 1.1441 Acc: 0.6154\n",
            "val Loss: 1.4202 Acc: 0.5000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 1.0700 Acc: 0.4231\n",
            "val Loss: 0.8885 Acc: 0.5000\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.7362 Acc: 0.6538\n",
            "val Loss: 1.3664 Acc: 0.3333\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.9775 Acc: 0.5769\n",
            "val Loss: 1.9272 Acc: 0.3333\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.6719 Acc: 0.6538\n",
            "val Loss: 2.2860 Acc: 0.0000\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.9404 Acc: 0.6538\n",
            "val Loss: 1.5323 Acc: 0.0000\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.9829 Acc: 0.5000\n",
            "val Loss: 1.0402 Acc: 0.0000\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.7021 Acc: 0.5769\n",
            "val Loss: 0.9540 Acc: 0.1667\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.5608 Acc: 0.6923\n",
            "val Loss: 1.0423 Acc: 0.1667\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.7651 Acc: 0.6154\n",
            "val Loss: 1.0617 Acc: 0.3333\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.6108 Acc: 0.7308\n",
            "val Loss: 1.0828 Acc: 0.5000\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.6500 Acc: 0.6538\n",
            "val Loss: 1.4228 Acc: 0.1667\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.6284 Acc: 0.7308\n",
            "val Loss: 1.5147 Acc: 0.1667\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 0m 17s\n",
            "Best val Acc: 0.500000\n",
            "number of images: 108\n",
            "54 0 0 54\n",
            "Accuracy: 0.5\n",
            "Precision (positive predictive value): 0.5\n",
            "Recall (sensitivity): 1.0\n",
            "Specificity: 0.0\n",
            "F_value: 0.6666666666666666\n",
            "roc_auc: 0.5531550068587106\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:13\n",
            "grav_train:13\n",
            "cont_val:3\n",
            "grav_val:3\n",
            "training data set_total：26\n",
            "validating data set_total：6\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.0485 Acc: 0.5385\n",
            "val Loss: 1.0500 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 1.050017).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 1.4290 Acc: 0.6154\n",
            "val Loss: 0.7010 Acc: 0.3333\n",
            "Validation loss decreased (1.050017 --> 0.701042).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 1.7533 Acc: 0.4615\n",
            "val Loss: 0.7197 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 1.0083 Acc: 0.5769\n",
            "val Loss: 3.1948 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.4217 Acc: 0.7692\n",
            "val Loss: 6.2235 Acc: 0.5000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 1.5366 Acc: 0.5769\n",
            "val Loss: 1.8842 Acc: 0.5000\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.9091 Acc: 0.6538\n",
            "val Loss: 1.3319 Acc: 0.5000\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.9035 Acc: 0.6538\n",
            "val Loss: 0.7917 Acc: 0.5000\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.6592 Acc: 0.6923\n",
            "val Loss: 0.7477 Acc: 0.1667\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.6631 Acc: 0.6923\n",
            "val Loss: 1.0404 Acc: 0.5000\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.5225 Acc: 0.8077\n",
            "val Loss: 2.1723 Acc: 0.5000\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.5318 Acc: 0.8077\n",
            "val Loss: 2.7849 Acc: 0.5000\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.6323 Acc: 0.6923\n",
            "val Loss: 1.2698 Acc: 0.5000\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.4238 Acc: 0.8077\n",
            "val Loss: 0.6548 Acc: 0.6667\n",
            "Validation loss decreased (0.701042 --> 0.654807).  Saving model ...\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.7574 Acc: 0.7308\n",
            "val Loss: 0.9204 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.5676 Acc: 0.8077\n",
            "val Loss: 0.9122 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.5476 Acc: 0.7692\n",
            "val Loss: 0.7881 Acc: 0.5000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.5427 Acc: 0.7308\n",
            "val Loss: 0.8779 Acc: 0.3333\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.5217 Acc: 0.8077\n",
            "val Loss: 1.2805 Acc: 0.3333\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.5282 Acc: 0.7692\n",
            "val Loss: 1.4786 Acc: 0.1667\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.5365 Acc: 0.6923\n",
            "val Loss: 1.9984 Acc: 0.3333\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.3896 Acc: 0.8077\n",
            "val Loss: 2.3207 Acc: 0.3333\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.3326 Acc: 0.8846\n",
            "val Loss: 1.9356 Acc: 0.3333\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.4855 Acc: 0.7692\n",
            "val Loss: 1.2320 Acc: 0.5000\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.5788 Acc: 0.6923\n",
            "val Loss: 1.3553 Acc: 0.5000\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.2680 Acc: 0.8846\n",
            "val Loss: 1.5329 Acc: 0.3333\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.2624 Acc: 0.8846\n",
            "val Loss: 1.9110 Acc: 0.3333\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.3973 Acc: 0.8077\n",
            "val Loss: 2.1013 Acc: 0.3333\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.2760 Acc: 0.8462\n",
            "val Loss: 2.2346 Acc: 0.5000\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 0m 32s\n",
            "Best val Acc: 0.666667\n",
            "number of images: 108\n",
            "43 11 13 41\n",
            "Accuracy: 0.5185185185185185\n",
            "Precision (positive predictive value): 0.5119047619047619\n",
            "Recall (sensitivity): 0.7962962962962963\n",
            "Specificity: 0.24074074074074073\n",
            "F_value: 0.6231884057971014\n",
            "roc_auc: 0.5229766803840877\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:13\n",
            "grav_train:13\n",
            "cont_val:3\n",
            "grav_val:3\n",
            "training data set_total：26\n",
            "validating data set_total：6\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.4698 Acc: 0.5000\n",
            "val Loss: 1.9747 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 1.974682).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 1.3895 Acc: 0.4231\n",
            "val Loss: 0.7096 Acc: 0.3333\n",
            "Validation loss decreased (1.974682 --> 0.709642).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 1.5923 Acc: 0.4231\n",
            "val Loss: 1.3374 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 1.5618 Acc: 0.4615\n",
            "val Loss: 1.1646 Acc: 0.5000\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.8838 Acc: 0.7308\n",
            "val Loss: 1.1776 Acc: 0.5000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 1.1218 Acc: 0.5769\n",
            "val Loss: 0.9964 Acc: 0.5000\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.6300 Acc: 0.5385\n",
            "val Loss: 1.1500 Acc: 0.5000\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 1.0272 Acc: 0.5000\n",
            "val Loss: 1.3828 Acc: 0.5000\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 1.2745 Acc: 0.5000\n",
            "val Loss: 1.3450 Acc: 0.5000\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.8588 Acc: 0.5000\n",
            "val Loss: 1.0263 Acc: 0.5000\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.5427 Acc: 0.6154\n",
            "val Loss: 0.8488 Acc: 0.3333\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.6273 Acc: 0.5769\n",
            "val Loss: 0.8769 Acc: 0.3333\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 1.5145 Acc: 0.5769\n",
            "val Loss: 1.0321 Acc: 0.3333\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 1.1429 Acc: 0.5769\n",
            "val Loss: 1.1087 Acc: 0.3333\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.6385 Acc: 0.6923\n",
            "val Loss: 0.9437 Acc: 0.3333\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.5899 Acc: 0.7308\n",
            "val Loss: 0.8104 Acc: 0.5000\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.5518 Acc: 0.6923\n",
            "val Loss: 0.8291 Acc: 0.5000\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 0m 20s\n",
            "Best val Acc: 0.500000\n",
            "number of images: 108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "54 0 0 54\n",
            "Accuracy: 0.5\n",
            "Precision (positive predictive value): 0.5\n",
            "Recall (sensitivity): 1.0\n",
            "Specificity: 0.0\n",
            "F_value: 0.6666666666666666\n",
            "roc_auc: 0.4957133058984911\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfEAAAGDCAYAAAA72Cm3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3xUVfrH8c+ThBBI6BB6L9JEQDoq2F1QFGUFAZWfir2tutZdRRd3dXXXXkBFFEGwsIKKXZBdEGkCShGQIiX0EgiBkOT8/rhDHMgkmZTJZJLv+/Wa12TuOfecZyaBZ+69555jzjlEREQk8kSFOwAREREpGCVxERGRCKUkLiIiEqGUxEVERCKUkriIiEiEUhIXERGJUEriIicws+Vm1jfccYSbmb1qZn8t5j7Hm9no4uwzVMxsmJl9WcB99TcoQTHdJy4lmZltAGoDGcBB4HPgVufcwXDGVdqY2QjgOufcaWGOYzyw2Tn3lzDHMQpo4ZwbXgx9jacEvGeJTDoSl0hwkXMuAegIdAIeCHM8+WZmMWWx73DSZy5lgZK4RAzn3DbgC7xkDoCZ9TCzuWa2z8yW+p+CNLPqZvammW01s71m9pFf2YVmtsS331wz6+BXtsHMzjGzemaWambV/co6mdkuMyvne32Nma30tf+FmTX2q+vM7BYzWwOsCfSezGyA79TpPjObZWZtTojjATNb4Wv/TTOLy8d7uM/MlgEpZhZjZveb2a9mdsDX5kBf3TbAq0BPMztoZvt827NObZtZXzPbbGZ3m9kOM0sys//z66+GmX1sZslmtsDMRpvZ/3L6XZrZaX6/t02+MwHHVDOzT31x/mBmzf32e85XP9nMFpnZ6X5lo8zsAzN7x8ySgRFm1s3Mvvf1k2RmL5pZrN8+7czsKzPbY2bbzexBM7sAeBAY7Ps8lvrqVjGzN3ztbPG9x2hf2Qgzm2Nmz5jZbmCUb9v/fOXmK9vhi/0nM2tvZtcDw4B7fX197Pf7O8f3c7QvrmO/u0Vm1jCnz1bKGOecHnqU2AewATjH93MD4CfgOd/r+sBuoB/eF9Jzfa9r+co/BaYA1YByQB/f9k7ADqA7EA1c7eunfIA+vwVG+sXzFPCq7+eLgbVAGyAG+Asw16+uA74CqgMVAry3VkCKL+5ywL2+9mL94vgZaOhrYw4wOh/vYYlv3wq+bX8E6vk+q8G+vuv6ykYA/zshvvF+/fUF0oHHfLH2Aw4B1Xzlk32PikBbYNOJ7fm12xg4AFzha6sG0NGvz91AN99nOhGY7LfvcF/9GOBuYBsQ5ysbBRwFLvG9xwrAqUAPX/0mwErgTl/9SkCSr5043+vufm29c0Lc/wHGAPFAIjAfuMHv80sHbvP1VcH/MwXOBxYBVQHD+5upe+LnnMPf/Z/x/u5P8u17ClAj3P829SgZj7AHoIceuT18/5kd9P2n74BvgKq+svuACSfU/wIvodUFMo8lmRPqvAL87YRtv/B7kvf/D/Q64Fvfz+ZLTmf4Xn8GXOvXRhReYmvse+2As3J5b38F3jth/y1AX784bvQr7wf8mo/3cE0en+0S4GLfz1kJx688K7ngJfFUIMavfAdegozGS54n+ZWNPrE9v7IHgP/kUDYeeP2E97wql/ewFzjF9/MoYHYe7/nOY33jfYn4MYd6o/BL4njjMo7g92XMt/9Mv8/vtxPayPpMgbOA1b7PKyqnz/mEv/tjf4O/HPs96aHHiQ+dTpdIcIlzrhJeImkN1PRtbwz80XeqdJ/vNPBpeAm8IbDHObc3QHuNgbtP2K8h3lHqiT7EO81cFzgD74vBf/3aec6vjT14ib6+3/6bcnlf9YCNx1445zJ99XPaf6NfjMG8h+P6NrOr/E6/7wPa8/tnGYzdzrl0v9eHgASgFt7Rp39/ub3vhsCvuZRvC9AHAGZ2j3mXL/b73kMVjn8PJ77nVmb2iZlt851i/7tf/bzi8NcY76xBkt/nNwbviDxg3/6cc98CLwIvATvMbKyZVQ6y7/zEKWWMkrhEDOfcd3hHLU/7Nm3COxKv6veId8494SurbmZVAzS1CXj8hP0qOufeDdDnXuBLvNPPQ/FO7Tq/dm44oZ0Kzrm5/k3k8pa24iUHwLtuivcf9ha/Ov7XPhv59gn2PWT1bd61+teAW/FOxVbFO1VvQcSZl514p5Ib5BD3iTYBzXMpD8h3/fte4HK8MyxVgf38/h4g+/t4BVgFtHTOVca71n2s/iagWQ7dndjOJrwj8Zp+n3dl51y7XPY5vkHnnnfOnYp3uaEV3mnyPPejgJ+XlA1K4hJpngXONbNTgHeAi8zsfN/gnzjfAKwGzrkkvNPdL5tZNTMrZ2Zn+Np4DbjRzLr7BhzFm1l/M6uUQ5+TgKuAQb6fj3kVeMDM2kHWwKc/5uO9vAf0N7OzzRsodzdeovD/EnCLmTUwb3DdQ3jX+AvyHuLxksVOX6z/h3ckfsx2oIH/oK9gOecygKl4g7kqmllrvM8rJxOBc8zscvMG3NUws4651D+mEt6XhZ1AjJk9DOR1NFsJSAYO+uK6ya/sE6Cumd1pZuXNrJKZdfeVbQeamFmU7z0m4X2Z+5eZVTazKDNrbmZ9gogbM+vq+12VwxuLcBjvrM6xvnL6MgHwOvA3M2vp+113MLMawfQrpZ+SuEQU59xO4G3gYefcJrzBZQ/i/ce+Ce/o5tjf9ZV412pX4V2/vdPXxkJgJN7pzb14g8lG5NLtdKAlsM05t9Qvlv8ATwKTfadqfwb+kI/38gveQK0XgF3ARXi306X5VZuElzzW4Z1SHV2Q9+CcWwH8C/geL2mcjDdQ7phvgeXANjPbFex78HMr3qntbcAE4F28LySBYvkN71r33XiXIJbgDdbKyxd48wSsxru0cJjcT9sD3IN3BuUA3hefY1+CcM4dwBtUeJEv7jXAmb7i933Pu81sse/nq4BYYAXeZ/4B3qWbYFT29b/XF/tuvEGSAG8AbX2n6T8KsO+/8b7wfYn3heQNvIFzIprsRaSkMm+im+ucc1+HO5b8MrMngTrOuavDHYtIaaYjcREpNDNr7TvNa2bWDbgW75YsEQkhzSokIkWhEt4p9Hp4p+v/BUwLa0QiZYBOp4uIiEQonU4XERGJUEriIiIiESrironXrFnTNWnSJNxhiIiIFItFixbtcs7VClQWcUm8SZMmLFy4MNxhiIiIFAsz25hTmU6ni4iIRCglcRERkQilJC4iIhKhlMRFREQilJK4iIhIhFISFxERiVBK4iIiIhFKSVxERCRCKYmLiIhEqJAlcTMbZ2Y7zOznHMrNzJ43s7VmtszMOocqFhERkdIolEfi44ELcin/A9DS97geeCWEsYiIiJQ6IZs73Tk328ya5FLlYuBt5y1oPs/MqppZXedcUqhiEhGRsmH/3P3smrarQPt+8AGsW1fAjodsAuD5PzejfOdGBWwkeOFcAKU+sMnv9WbftmxJ3Myuxztap1Gj0H8oIiIS2X4Z+QuHVhwq0L7dfY8Cmew9HR24s9Qn8aA558YCYwG6dOniwhyOiIiUcBkpGQA0vLch5aqXy9e+993vPT/5RIDCZctg0kSoXQc6dQIgJeUoS5ZuIzn5CLOuPwuAXu3qFjj2/AhnEt8CNPR73cC3TUREpEjUu6keFZpUyNc+k31J/N37AhS+/wNMmgynD4L3nwGgX7+JfJacTuPG1dl4hW/fdvUKEXXwwnmL2XTgKt8o9R7Afl0PFxGRSPPqqxdyzTUdWbLkxmLvO2RH4mb2LtAXqGlmm4FHgHIAzrlXgRlAP2AtcAj4v1DFIiIikm8pKdm3HT4MwOIfk+iY6YiKMho1qsIbb1xczMF5Qjk6/Yo8yh1wS6j6FxGRsmvHDqgCNG0K2wvYhj2dELhgFMAc+FuAk9l9Zhawt4LRjG0iIlLqpKYWsoGWnxZJHKEWEaPTRURECmL9eqjQJH/72KMGgBuyCk46iVmzNjBs2FS2bj1A1apxvPHGAC69tE3gfWfNKlzA+aQkLiIikoOvv17HeedNwDno3bshkyZdRqNGVcIdVhYlcRERkRz06dOYXr0actZZTXn44T7ExJSsq9BK4iIiEpH694cZMwKXvZvbjqmp8NxzsHNn4PJe/4AaPbCkJEhKgtHNmAP87X+zCxlx0VMSFxGRiJRTAg9qxwceyLn8osKNMO9XvXqh9s8PJXEREYloLsBk3N83gSMbc9jhkG9O9c6dYehQALZtO8iEd5axbdvBrGqZffpgZkUbbBErWSf3RUREikvbtri77uK1yn1p9lIM927rwCetLsoqLukJHJTERUSkjHIOhg6dyvXXf0JqajojRnRk0aLrwx1Wvuh0uoiIRKRyZBBLJkf3BSjM9D0nJ8O+I8eX+aZTNYOGDSuTkBDLq6/2Z9iwDiGNNxSUxEVEJOIcWHSA6fxIHJnMqZZLxVNOIdDEq4lv/YOdjXp4L/r1ZDh7GF7ME7UUBSVxERGJOAd+PEAcmRzFiKsS4Mpw8gES3BriEg5DtDc5S6aDQ4eOkpGR+XsCz0Gtw7/ireFVsimJi4hIxPqS2jy1r3X2giZNYONG+Gk9NGnCZ5+t4eqrP2Jn8iFq1aqYVc317ZtDyzltL1k0sE1EREqttLQM7r77C/r1m8TOnYc455xmLF1a/Ot+h4qOxEVEpNQaNOg9Pv4pjehoY/Tos7j33t5ERRn8Eu7IioaSuIiIhEz/b75hRnR0gfbtvAh6zwlc1mQDdAbotw2btS17hfHjj3uZATxAOg/M/q5AsZRUSuIiIhIyBU3gAPc8DXUD5Gd/ByoVuHnYPY9IufadEyVxEREJOTdvHlSsmG273XG7V/7c89nK5qS04SjlaNJvGzFxGdnKo8o5XjhlP68u88o2bdrPhAnLGDykPc2bVYP27eGsswLGc2zNcC67v6BvqURQEhcRkdC78UaoWjX79jt8z7ffnr3s73PgwFHqvjGQ8nXK59i0c47nnvuB+57/mrS09sxd25JPnh1aNHGXcEriIiISsXbuTOH//m8an366BoCbb+7C00+fF+aoio+SuIiIhE0cGcSSQdquAIWZAbb5mTlzPcOGTSUp6SBVq8YxbtwABg5sE5I4SyolcRERKZT+y5YxY8+eXOtYteyn0tuzj2ksJRbH3Fr56zMlJY3Bgz9g585D9O7dkEmTLqNRoyr5a6QUUBIXEZFCySuBM696wM0tOEgsjvQoI65a4FHslbtWJjYxNtv2+PhYxo27mPnzt/Dww32IiSmbc5cpiYuISJEINIXpsSW53d592Qa2bX4R1t4GjW6qS6sXW+XZ/tSpK9m8OZnbb+8OwIUXtuLCC/PerzRTEhcRkRItNfUod931Ba++uojoaOPss5vSrl1iuMMqEZTERUSkxFq+fAeDB3/A8uU7iY2N5umnz6Vt23xeQC/FyuZFBBGRUqZ/f+/UdTgex+RWll/OOcaMWUiXLq+xfPlOTjqpBj/8cB233dYdK0zDpYySuIhIKTBjRrgjyFk/Ps33PqNHz+bGGz/l8OF0RozoyMKF19OxY50QRBfZlMRFREoR54r/kWvfVaryKRfm+32MGNGRxo2rMHHipbz55sUkJGQfoS5K4iIiUgJkZjomTlxGZqb3raBhwyqsWXMbQ4eeHObISjYNbBMRkbBKSTnKeedN4Jtv1rNlywHuvbc3AOXKFXwFtLJCSVxEpCzIyIAdO0LbR1JS9m2ZecydCkyZ/DPfHF5PYmI8HTrUDkFgpZeSuIhICdG/fwgHqPXoAQsXhqbtmTO953r1gt7lyJF0pk5dSQfKkXo4nXPOacaECQOpUychNDGWUkriIiIlRGETeL9+uRQeS+C1axfu3q/c1Mlh9Phpp0GV3+c13779IP36TaLRYqMDLenZswHPfHEGUVG6dSy/lMRFREoY/xHfRS4pqeiT+KxZv7cdhBo1KhIXF0ON6rGwBzp3rqsEXkAanS4iIiF38GAau3cfAiAmJor33/8j999/WpijinxK4iIiElKLFyfRufMYrrzyP1m3kNWrV4m4CjoZXFj6BEVEitO4cfD99zkUvuY9jRxZbOGEknOOocNHUn9VVQZUArZv5N6uP2aVN9nRhPa056X5L/H8o8+HL9AIpiQuIlJcUlK8BJ3jbVe+JP7666Hpv3rgdb1DYefOFEaMmMa13/Wjekru/SZXTC6mqI7Xr2VuIwEjg5K4iEhxSU/3EnhcHDwf4Mjzet/z2LGh6b9799CNTPfz7bfrGT58KklJB7mp/KkAtHi2BVEVs1/BjaoQxWsXv8abld4MeVylkZK4iEhxi40NfMr8WBKP8NPpX331K0lJBznttEawwNtW++ralKtaLryBlUJK4iIiUqQee+xMGjeuynXXdebzhBK8vFopoCQuIiKFUikZKqTC1iV7qV69AgAj+rUnfWsa5nT/dygpiYuIFKHcp06tAjhIBiIotyV+/gY745oHLOs6H/7zAERnwmqWZiuPJz7U4ZVpSuIiIkUopFOnhklOCRyg6XovgR+Ky+BA7K6AdXa3202fKn1CFV6ZpiQuIhICAadO3b8fqlaFypW9nyOM69sX5xxjxy7izju/4PDhdBJrtQLq0uqWxrR4+uxwh1jmaMY2EREJ2o8/buPGGz/l8OF0rrmmI3fc2SPcIZVpOhIXEZGgde5cl1Gj+tCqVQ2uuOJkfnv6t3CHVKYpiYuI5FNI1/0uoP7LljFjz55i6euRR/oWSz+SN51OFxHJp7wSeDgGp4U6gUdt+ZnDh9ND2ofkX0iPxM3sAuA5IBp43Tn3xAnljYC3gKq+Ovc750rY91sRkcBCuu53Abm+fQu0nz3q3fPmHvHe1KefrmbEiGns2nWIxMR43n77EuLidPK2pAnZb8TMooGXgHOBzcACM5vunFvhV+0vwHvOuVfMrC0wA2gSqphERCR3R46kc//9X/Pssz8AcO65zXj77YHUqZMQ5sgkkFB+reoGrHXOrQMws8nAxYB/EndAZd/PVYCtIYxHRETycMklU/j887XExETx+ONncc89vYiKiqCZacqYUCbx+sAmv9ebge4n1BkFfGlmtwHxwDkhjEdERPJw223dWL16N5MmXUr37g3CHY7kIdwXOK4Axjvn/mVmPYEJZtbeOXfcYrtmdj2+9X0aNWoUhjBFpCwpiaPPAfpP6s+MNTkE1mcm8Pu17YLq168l55zTjNjY6EK1I8UjlKPTtwAN/V438G3zdy3wHoBz7nsgDqh5YkPOubHOuS7OuS61atUKUbgiIp5gEnhYRqDnlMCLSL+W3ptSAo8coTwSXwC0NLOmeMl7CDD0hDq/AWcD482sDV4S3xnCmEREglYSR5/D7yPI/dmsWTmWZdvfOZ59dh733fc1R49m0qFDbSZPvow2bXSQFGlClsSdc+lmdivwBd7tY+Occ8vN7DFgoXNuOnA38JqZ/QlvkNsI50rqPxsRkci3c2cKI0ZMY8aMNQDccktXnn76PN0+FqFC+lvz3fM944RtD/v9vALoHcoYRETEM3/+Fi65ZDJJSQepVi2OceMu5pJLWoc7LCkEffUSkVIpbIPT1qyBJ5+E1NTsZWlpue6a68C1Qkp6M4mo6bu4cXdjqtSKo3fvhlR8P5MV76/Ie+dcHFpxqIgilIJQEheRUils63q/+iq88UbudRITA24OJoEfG3wWrC1bkqlTJ4HVN67GpTnOoCbshIMf7eVgvlrKXWxibBG2JsFSEheRUq3YR9kcO9q+8ko4//zAdXr1yrWJYAanBeODD1Zw3XXTue++3vRM89psPaE1ZkU7eUtUfBTVL6hepG1KcJTERURCoWtXGDYsLF0fOnSUP/3pc8aOXQzAokVJ9MQ7+q89rHaRJ3EJH61iJiJSivz88w66dXuNsWMXU758NC+++Afef/+P4Q5LQkRH4iIi+ZWSAps3By7buzdk3WakZnDktyMByxr+5j1fcv47pKWlc0bTujzzzPm0bl2L1NUBBtlJqaAkLiKSH0eOQMuWkJSUe70cTlkXdAS6y3Qs7LCQ1LWBE/LbWT919p7WQ/Il65nP+nz3JZFDSVxEJD/27fMSeFQUtGgRuE61anDBBQGL8krgOY0+d0ddVgKv0KpCtvJffLe01UuPplKlwCPFq/+huq6HlzJK4iIiBVGzJvzyS4F3L+gIdIs1uv/SnYyMTJ544n/MnbuZjz++gh6zv/Pa7Xt6gWOSyKMkLiISYbZuPcDw4VOZOXMDALNnbwxvQBI2Gp0uIhJBMjMdp5zyKjNnbiAxMZ4vvhhO375Nwh2WhImOxEVEilBhpk5N/TWVjaM3knEoI1tZxtFMAI4a7Hq/KwA7gPPZBLM2FTheiWxK4iIiRagwU6cmjUti2/htue67t1ou7VbXrGlljZK4iEgIFGTgmvNNjZo4NJGaA2oeV5aensnwX1axsg24vn2LIkQpBXRNXESkhEnomECFflUYPWcRnBlP4uBE6g2rw8yzYFvdcEcnJYmOxEVESpgtm5M5u/M01q7dw9atB/jgg8vDHZKUUEriIiIncg5+/RUOH85etmtXCLv1Tqe/+OJ81mbuoUOH2owefVbI+pPIpyQuIiVW//6FXxe8QP79b7jnnhyL+w+FGa12wKNFN/vZjh0pvPzzBgYQQ8bIZjCkGcuANtt+htzHukkZpiQuIiVWYRN4v8CDwPO2apX3XLcuBBjxPaPV8tz7zWH0eU6Sk4/QqdMYLjq9c551NQJd/CmJi0iJ5wo2Q2nhPfoojBwZYLt3BF7QqVNPVLlyea68sgP7Nniv/9m8GZP7NiqStqV00+h0EZEw2LBhH/Pnb8l6/be/nRnGaCRSKYmLiBSz999fTseOrzJw4BR27ToEQLly0WGOSiKRTqeLSNn02GOwZEngssWLQ9LloUNHuXPkbBpsiuauDj0AePnS+Vnlp60LSbdSiimJi0jZk5QEjzySd706dYqsy59+2s7Vl0/l36va5Fk3tk7g9cBFTqQkLiJlz9Gj3nONGjB2bOA6NWrA6UWzNvfbby/lhhs+IfqwNxDucHk4dVK7gHVjqsRQtW/VIulXSj8lcREpuypWhEsvDXk3iYnxHD6czo1XdoIJkB4DtS6tFfJ+pfTTwDYRkRDYuvVA1s8XXNCCH3+8gRdeKOiN6yKB6UhcRCSf4lPjqbOvDgeWHMhWlpnhGDfuR8aOXcyYMf3p1NlbsaQ58RxcdrC4Q5VSTklcRCQfMo9k8s4L71D1UFUWjVkUsE4n4BU6kXntVhaxtXgDlDJFSVxEJB/S96VT9VBVMiyDyh0qZ21PTj7Cb7/tJyPDERMTReNGVUiolH2U+dKDKczqCxcWY8xSeimJi4gUwP6K+zl7ydkcOZLOffd9zXPP/QDAeec15+23L6F27YSA+3WbNQuAd4orUCnVNLBNRKQQ9uxJZeLEn4iJieKf/zyHzz4blmMCFylqOhIXESmEunUr8e67l1G5cnm6dasf7nCkjFESF5GQCtua4IsWwd//Dmlp2csOHSpwswcPHsm27ZxzmhW4PZHCUBIXkZAK25rgL70EU6fmXqdevXw1uXDhVq4f9BH/pi3gnUqvXr1CAQMUKTwlcREpFsW+Jnh6uvd8551w1lmB6/TsGVRTmZmOZ575ngce+IaEo7+vNqYELuGmJC4ipVunTnDRRQXefceOFK6++iM+/3wtANde2wXeKKrgRApHo9NFRHJx222f8fnna6levQIffTSYxx8/O9whiWTRkbiISC7+9a/zSEvL4IUX/kCDBpVJ2x5goJxImCiJi4j4Wb9+L2e/cT7ryy34fWNH+Mh3Cr3awWpMJY8BcyLFRKfTRUR83ntvOR07jjk+geegfHT5YohIJHdBH4mbWUXnXMFvrhQRKaEOHTrKnXd+zmuvLT5uu3sk+5D6tO1pzH16LlXjqhZXeCI5yvNI3Mx6mdkKYJXv9Slm9nLIIxMRKQY//bSdLl3G8tpriylfPpqXXtKa3xI5gjmd/gxwPrAbwDm3FDgjlEGJiBSH+fO30LXra6xcuYs2bWoyf/5Ibr65a7jDEglaUKfTnXObzMx/U0ZowhERKT6dO9ela9f6tG5dg2efvYD4eN/Soe3/ATV6YL4Vx/xV2wNTge1HjwYsFylOwSTxTWbWC3BmVg64A1gZ2rBEREJjzpzfaNGiOrVrJxATE8WXXw6nQoVyx1eq0SOkMfSrXj2k7UvZEUwSvxF4DqgPbAG+BG4OZVAiIkUtIyOTv//9v4wa9R3nntuMGTOGERVl2RO4H9e3b7ZtadvTmMtcapcrh+vbO4QRi+QtmCR+knNumP8GM+sNzAlNSCIiRWvLlmSGD/8Ps2ZtAKBjxzpkZjqioiz3HUVKuGCS+AtA5yC2iYiUOD/+mMS5d73K7t2p1K4dz4QJAzn33ObhDkukSOSYxM2sJ9ALqGVmd/kVVQaiA+8lIlKMkpJg/fqARW7HDgx45tl57KYj55/fnLfeuoTatROKN0aREMrtSDwWSPDVqeS3PRkYFMqgRETytG8fNG8OqakBi4+dKLeoaJ7+57n86U89jzt93n9Sf2asyWGx8z4zizhYkdDIMYk7574DvjOz8c65jQVp3MwuwBsUFw287px7IkCdy4FRgAOWOueGFqQvESljduzwEnj58tDZu7rngKNHM4gt550szKxZk3v/NIp2Z7bPtnuOCVwkggRzTfyQmT0FtAPijm10zp2V205mFg28BJwLbAYWmNl059wKvzotgQeA3s65vWaWWID3ICJlWaNGMHcuyclHuOmmT5k5cz1Ll95IrVrxROH9x5WbQFOr6v5viRTBzNg2EW/K1abAo8AGIO/VAaAbsNY5t845lwZMBi4+oc5I4CXn3F4A59yOIOMWEcmyYMEWOncew6RJP7F//xGWLNkW7pBEikUwSbyGc+4N4Khz7jvn3DVArkfhPvWBTX6vN/u2+WsFtDKzOWY2z3f6PRszu97MFprZwp07dwbRtYiUFXv2HqZXr3H8+uteOnasw+LF12v0uZQZwSTxo77nJDPrb2adgKKabigGaAn0Ba4AXjOzbEsDOefGOue6OOe61KpVq4i6FpFItmtXStZzenomt9/eje+/v5aTTqoZ5shEik8w18RHm1kV4G68+xH4oh8AACAASURBVMMrA3cGsd8WoKHf6wa+bf42Az84544C681sNV5SD+Z0vYiUYatX76EmEB0VxfSPhnDRRSeFOySRYpfnkbhz7hPn3H7n3M/OuTOdc6cCe4JoewHQ0syamlksMASYfkKdj/COwjGzmnin19fl5w2ISNnh3O+D0Hr18o4RGjWuogQuZVaOSdzMos3sCjO7x8za+7ZdaGZzgRfzatg5lw7cCnyBt2DKe8655Wb2mJkN8FX7AtjtW698JvBn59zuQr4nESmF1q/fy2mnvZk1deox5WKCuSooUjrldjr9DbzT4fOB581sK9AFuN8591EwjTvnZgAzTtj2sN/PDrjL9xARCWjKlJ+5/vpPSE4+woMPfsOcOdegWc9Fck/iXYAOzrlMM4sDtgHNdaQsIsUlZf8hnr/yRT77eCWnAKef1pj77muE/fe/sGlTnvvnpkZyDervqc++2fuylXVY6j3vi8pednTP0WzbRMIltySe5pzLBHDOHTazdUrgIlJcli3bzoIzh/HAnm944NjG//ke/qLzv5RD+sF0JrwwgQpHK7Bk/JJs5c/5npeQvewYi9a5AAm/3JJ4azNb5vvZgOa+14Z3JrxDyKMTkTIpLS2DCy+cxNN7kgA43LQlcQ3qZK9oBtdck+/20/ekU+FoBdKi06jVK/ttq7P37wfgjCpVcmwj8QpNMCnhl1sSb1NsUYiI+ImNjWbMmAupdcuHsB7inhgNl19e5P3sjd/LebPPy7a9s2/aVde3U5H3KVKUclsApUCLnoiIFMR//7uRpUu3c+ut3QD4wx9aQpd6EHilUREhuMleRERCJiMjk8cf/y+PPvodAN2716dr1xNnaBaRQJTERSRsNv+2jyXdL6bPto18CzRqWIXGd3/tXesGWLEi1/1FyrqgkriZVQAaOed+CXE8IlJGTJ/+C09cOYa5ybN/37iJ45dNOqZRo+IKSySi5JnEzewi4GkgFmhqZh2Bx5xzA3LfU0QksFdeWcDNN8/gZFIByGjUhOi3xweuXLs2tG5dfMGJRJBgjsRH4a0NPgvAObfEzJqGMCYRKeUGDDiJv/1tNndf0RP+/SrRVSpBnz7hDksk4gS1FKlzbv8J21zAmiIiOfjkk9VkZGQCUL9+ZdauvZ2rr+4Y5qhEIlswR+LLzWwoEG1mLYHbgbmhDUtESpuLLnqX0aPP5KGHzgCgYsVyIe0vMz2T5O+TcWnZjznStqeFtG+R4hJMEr8NeAg4AkzCW3lsdCiDEpHI0r8/zJiRe534+HI0bJjzDGgF6nfZMmbsCbwy8rWvw/CJue+fWbE25pvYRSQSBZPEWzvnHsJL5CIi2eSVwCtX3sj8+ddz0kk1i7bfHBI4QOIO73lDY9hTPXCdL7NP1palX/UcdhIpQYJJ4v8yszrAB8AU59zPIY5JRCKUc7B//2EGD/6AL774FYA77ujOk0+eQ/nyoZuWwvXtm23bg3c8CJzHBY+1ps5VAeZdR2sgS+TL81+Vc+5MXxK/HBhjZpXxkrlOqYtINgkJsaSmplOjRgXGj7+ECy9sFe6QREqtoL4aO+e2Ac+b2UzgXuBhdF1cRAKIjo5i0qRLAW8UuoiETjCTvbQBBgOXAbuBKcDdIY5LRCLE+vV7gWoAZGY6oqLs+OT95Zfw+OOQnp5tX3fwEC+eOgU7nAjtZ+W77xd8z4urLM5W1m1tt3y3JxJpgjkSH4eXuM93zm0NcTwiEkGmTPmZ66//BLgfgM2bk2nU6IQR6C+/DLNnZ98ZSKEJJ1P4dbmTSc62rSpVAYhrElfo9kVKqmCuifcsjkBEJHKkpKRxxx2f88YbPx63PVsCB8j0JnjhH/+A008/vmytgxHpbKkHF75XtGt3nzbuNPZX3M9vZ/xWpO2KlCQ5JnEze885d7mZ/cTxM7QZ4JxzHUIenYiUOEuXbmPIkA9ZtWoX5ctH8+yzF3DTTUHs2LYt9O59/LZKB4GFHI6DKr2L9h7yn7/WjTRS+uV2JH6H7/nC4ghERCLD1KkrWbVqF23b1mLy5Ms4+eTawSVxESlyOSZx51yS78ebnXP3+ZeZ2ZPAfdn3EpHSyDmH+db4/utf+xAfH8utt3bLmjq1ISnU4TC7Ry/KvvPqqkBXWBwNsbuPKzq87nCoQxcp1YIZ2HYu2RP2HwJsE5EgJL6xjJ3Nc55pLCJ0g/vmzwEgcTu8axDl4Ke/xgaofI339CjATwGby4gOSZQipV5u18RvAm4GmpnZMr+iSsCcUAcmUlpFfAI/QfU9XgI/GA8r2uZ/f2eQNDih6AMTKQNyOxKfBHwG/INj9494DjjnStf/QiJhEGiq0JJi8+Zkhg2byuzZGzGD++8/jUcf7Uu5ctkPmZPXf89ijlCX37hw/lVhiFak7MotiTvn3AYzu+XEAjOrrkQuUjpNm7aKa66Zzp49qdSpk8A77wzk7LObhTssEQkgryPxC4FFeLeYmV+ZA/SvWqSUWb16NwMHTsE5+MMfWjB+/CUkJsaHOywRyUFuo9Mv9D03Lb5wRCJDbutYR7JWrWrw17+eQZUqcdx5Zw+ionzf3adNgyeegIyM7DvtrIHGuYqERzBzp/cGljjnUsxsONAZeNY5p2mQpMwqbAKv9Wt16Fs0sRSGc47x45fQpElVzjzT+77+6KNnZq/4yiswb14OrbT2nsprelOR4hbMLWavAKeY2Sl4C5+8DkwA+oQyMJFIkNPgNN8t1TgXsLhEJPDk5CPceOMnvPvuz9Stm8CqVbdSuXL5wJWPTZ363HPQo8fxZcsz4Joj0ExX2ESKWzBJPN0558zsYuBF59wbZnZtqAMTkdCZP38LV1zxIevW7SU+vhxPPHFOzgncX+vW0O3E1cGSgcW/f3MRkWITTBI/YGYPAFcCp5tZFFAutGGJSChkZjqefnouDz30LenpmXTqVIfJkwfRqlWNcIcmIgUQFUSdwcAR4Brn3DagAfBUSKMSkZAYMeIj7rvva9LTM7njju58//21SuAiESyYpUi3mdlEoKuZXQjMd869HfrQRMKrNI5AHz68A59/vpZx4y7mwgtbhTscESmkPI/EzexyYD7wR+By4AczGxTqwETCLa8E3q969WKKpOCOHs3gq69+zXp93nnNWbfuDiVwkVIimGviDwFdnXM7AMysFvA18EEoAxMpKUry9Ki5WbduL1dc8SELF27l22+vok+fJgAkJARapEREIlEwSTzqWAL32U1w19JFJEwmT/6ZG274hOTkIzRqVIXYWC0TJlIaBZPEPzezL4B3fa8HAzNCF5KIFFRKShq33/4Z48YtAeDSS9vw+usXUa1ahTBHJiKhEMzAtj+b2aXAab5NY51z/wltWCJFozQOTsvJqlW7GDhwCqtW7SIuLoZnnz2f668/FQvm/u3Dh2HIENi4MXD52rVFG6yIFInc1hNvCTwNNAd+Au5xzm0prsBEikJhE3gkDF47pkqV8uzefYi2bWsxZcog2rdPDH7nhQu9+dFzExUFTbWUgkhJktuR+DjgbWA2cBHwAnBpcQQlUtQidXBaXvbuTaVy5fJER0dRt24lvvrqSlq2rEHFivmcj+nY/LAdO8K4cYHr1KkDdesWLmARKVK5JfFKzrnXfD//YmaLiyMgEQnO7NkbGTZsKiNHdubhh72lDE45pU7hGk1IgE6diiA6ESkOuY0yjzOzTmbW2cw6AxVOeC0iYZCensmoUbM488y32Lw5ma++Wkd6ema4wxKRMMjtSDwJ+Lff621+rx1wVqiCEpHANm3az7BhU/nvf3/DDB588DRGjepLTExo7/pMP5DOvpn7cBnZl2VLXZMa0r5FJGc5JnHnXIBFhUVKlrI0+nzatFVcc8109uxJpW7dBCZMGMjZZxfP8p9rblnD9gnbc61j5Yp+FbP+k/ozY43uaBXJSTD3iYuUWMEk8EgaYZ4T5xzPPfcDe/ak0q9fS8aPv5hateKLrf+0bWkAVO5ZmdjaAWZ8i4J619cr8n4Lm8D7texXRJGIlExK4lIqlNbR5845zAwzY8KEgUydupJbbulGVFR41u5uMqoJ1c8r/i9F7pHsp/FFRNOnipRIzjnGjfuRiy+eTEaGN2itfv3K3HZb97AlcBEpeYJZxczMbLiZPex73cjMuoU+NJGyaf/+wwwdOpVrr53Oxx+v5uOPV4c7JBEpoYI5nf4ykIk3Gv0x4ADwIdA1rx3N7ALgOSAaeN0590QO9S7DWxWtq3NuYXChi4RW//4wo5jHVM2fv4UhQz5g/fp9xMeX4+WX+3PJJa2zypPGJ7H5mc3ev8iilJIBvAGL4+HkBdmKU9drBLpISRRMEu/unOtsZj8COOf2mlmeaxmaWTTwEnAusBlYYGbTnXMrTqhXCbgD+CHf0YuEUGETeL98jKnKzHQ8/fRcHnroW9LTM+nUqQ6TJw+iVasax9Xb+upWUpalFC6wHDWDQ8DPObQfDXHN4kLUt4gURDBJ/KgvITvIWk88mOOAbsBa59w6336TgYuBFSfU+xvwJPDnYIMWKU6uGMZUTZiwlPvu+xqAO+/szhNPnEP58gH+efpiafNOG+I7FOHo9EWL4f9GQKfO8Nb4gFVia8cSm6i1yEVKkmCS+PPAf4BEM3scGAT8JYj96gOb/F5vBrr7V/DN/NbQOfepmeWYxM3seuB6gEaNGgXRtUhkGTasA9Onr+aaazrSv3+rPOtXaFGBhJMTii6AfQash/j6UJTtikhIBbMU6UQzWwScDRhwiXNuZWE7NrMovBngRgQRw1hgLECXLl10r4lEvLS0DP7+9/9y441dqFMngZiYKD788PJwhyUiESbPJG5mjfCulH3sv80591seu24BGvq9buDbdkwloD0wy7fecR1gupkN0OA2Kc3WLNjJE1d8zbpf97J/2h4eeOC0oPc9uvtowTveuxe+/RYyMrKXrSz093IRCYNgTqd/inclzoA4oCnwC9Auj/0WAC3NrCle8h4CDD1W6JzbD9Q89trMZuGtWa4ELsUiHKPP3333J7Zd+StXZtQF6sISWDH4xGEiebPYAtwrfsMN8P77udeJLdg1b02PKhIewZxOP9n/te869s1B7JduZrcCX+DdYjbOObfczB4DFjrnphcwZpEiEUwCz88I89ykpKRx222f8eabS3idUwFI6FuZCrXK57utuGZxJJxSgOvW231zn/fpA4mJ2cujo71EXwChTOCaOlUkZ/medtU5t9jMuuddE5xzM4AZJ2x7OIe6ffMbi0hRCPXo88OH0+nW7XVWrNhJXFwM9WtUhi2ZtH62VcGScWE9+qiXyENA06OKFK9gronf5fcyCugMbA1ZRCKlTFxcDJde2hozmDx5EKnDNpKyJVT3eotIWRLM3OmV/B7l8a6RXxzKoEQi3e7dh1i06Pfvuo880pf580fSvn2A09giIgWU65G4b5KXSs65e4opHpFsglkz3ErQmiCzPvqVlYNXUDmjHEea/kZ09PHBHV5/uMBt93+yAzMO/1Swnc/yPWb1hVkFDkFESpAck7iZxfgGp/UuzoBETpTnmuHzCr40ZlENXANIT89k9OjZfPfYCh5xbQFIWxs4YUcnRFO+Yf4HtRU4gRcDDUATKX65HYnPx7v+vcTMpgPvA1kX8pxzU0Mcm8hxAq0ZfuwIvDimRs3Npk37GTZsKv/972/09d05WfXcarR8vkXA+uXrliemSr7HlWZxrSfDKafkf8fERKhe/OuBi0hoBPO/SBywG+9E3LH7xR2gJC4CzJixhuHDp7J372Hq1k3gryP7wGO7KFclhvjWRTi/ub8mTaB16zyriUjpllsST/SNTP+Z35P3MbqPRMQnNjaaffsO069fS8aPvxg3K4UV7Ap3WCJSBuSWxKOBBI5P3scoiUuJkMhhTmY/2ycVb78HD6aRkODNbnYy8cz66x9p1ao6mV8dJHlecsEbdg5mzYKkpKIJVERKtdySeJJz7rFii0SkAP7OTzQnhZXDwh0JrGLHca8LNDXq/Plw1lk5l4/yPRdwelQRKV1yS+Il6KYdkcCqkQZAzUtqElUhmGkPCu7o0Qzmz9/Cb7/tB6B165p06lQ3YF0rZzS4s0H+O9nh+yJQr14Os6q96z0VZFCbiJQ6uSXxs4stCpFCavlKS8rXyf8tW8H64YfNXHHFh6z/bR8JCbG88kp/hg3vELL+6NwZJgW4RvCoL4lHhfYLi4hEhhyTuHMuj5tzRUq/zEzHU0/N4S9/mUl6eiadO9dl8uTLaNmyRrhDExEJatpVkTJrz55U/v3veaSnZ/KnP/Vg7txrlMBFpMQo+GwTIpHuH/+A11/PtUpNYH10Oq6OI37a2zCt8N32PyuJGQ1Sc64wCuATeFTDUkQkd0riUnaNGQMbN+ZZrWIRdzvjqsLtr+lNReQYJXGRWbOgQQM2btzHnXd+zrKfdhBbLpqZM6+iTp1KRd/fO95UrG742sDlMTHQqFHJWtVFREokJXGRxo2ZNPcAN974JQcOpNO4cVPemHQZdXo1DG2/zZuHtn0RKfWUxKXMu+eeL/nXh94MaYMGteW11y6iatW4MEclIpI3JXEp8z74cAVxcbV47rkLGDmyM6bT2CISIZTEpUj0X7Ys73W/CylQbv2wEO31P2cHMxoCPAfADUl/4QZNNCwiEUT3iUuRCHUCZ17RrIG9e/chHnlkJhkZmcxomMttXiGmEeYiUhR0JC5FyvXtW+RtHjsCdwHWzptTB45uD66d777bwLBhU9my5QAVKpTL2u6uXu+tzy0iEmF0JC6lXnp6Jo88MpOzznqbLVsO0KtXQ664on24wxIRKTQdiUuptmnTfoYOncr//vcbZvDQQ6czalRfYmL0/VVEIp+SuATlwOIDrLxqJRkHMgKWv3vEe/6+/Pf5b3zfPkhJybH4Xep7bcdsyVZ2NKM6EAVdukL0/uPLjmbC9oO8k+mIjjZq1qhI3IRxMMFX4Zr8hyoiUpIoiUtQds/YzaHlh3Isr+N7PsKRArRewffIqW2vzSMZNQOWl2c75basBNKP214OyJquJQPYsS9wB7Vr5ydYEZESQ0lc8qXeLfVo9OdG2bY3+X4eABt69sh/oyefDAcOwGefQaXs05w2Oa2B1/b/NgfcvVzNxkTFeVOYrlmzmypVypOYmADAgQNHSEiIDXzv9/gm3nOFnL9AiIiUZEriki8xVWOIa5x9NrPt673nQGV5itoJ7IceTaFq1ext+57jerfIsQnnHG+88SO33/4ZvXo15MsvryQqygjBzOciIiWGkrhEvP37D3PDDZ8wZcpyAOrXr8yRI+nH3UYmIlIaKYlL0cpjfe6AjhTkOrpn3rzNXHHFh2zYsI+EhFheeaU/w4d3KHB7IiKRRElcitbIkQXbLyoKyuXvyPmpp+bw4IPfkp6eSefOdZk8+TJatqxxXJ3+k/ozY82MgsUkIlLCKYlL0br6am897Pzq2RPi4/O1S0rKUdLTM7nrrh78/e9nU7589n7zSuCa/lREIpmSuBStV1+FuNAt47lv3+GsZUL/8pczOPvsppx+euM893OPBJizVUQkwmnaKokY99zzJW3avMT27QcBiImJCiqBi4iUVkriEjH+9a/v2bkzhe++2xjuUERESgSdTpcsq65bxZ4ZgZcUzWm61aKQ2GkBO5d0zbviqFFkAINXPsLgR0MWjohIxFASlyzb3tiWe4UoqNSp6KdPCSqBt/y0wO1r8JqIlFZK4pJNz809IcAspVEVoihXLXQTqPivFz579kb69h1PXFwMzz13Addd1w8zDU4TEfGnJC7ZxNbLYa7xYnTGGY156aV+9OnThLZta4U1FhGRkkoD26TEuPjiyXz99bqs1zfd1FUJXEQkFzoSL0MyDmWwc+pOMg6GbpBaYUyf/gtr1+7hp59uIioqvGcCREQigZJ4GbJ1zFZ+vevXXOtY+dAkz9ynP/Wudffu3ZCJEy9VAhcRCZKSeBmSvicdgEpdKpFwakLAOtXOrBaS6+HBzF8+a9YIYmJ0hUdEJFhK4mVQjQE1aPLXJmHp2z3iyMx0tG//MitX7qJevUps9ZUpgYuI5I/+15RiFxVlPPfcBQwYcBJLl94Y7nBERCKWjsQlLM49tznnnts83GFIGXT06FE2b97M4cOHwx2KyHHi4uJo0KAB5fKxLLOSuBQpq5DTCmbe4DUbVWyhiAS0efNmKlWqRJMmTcI+H4LIMc45du/ezebNm2natGnQ++l0upQI/TQzqhSTw4cPU6NGDSVwKVHMjBo1auT7DJGOxKVIudTDEBfH999vYujQqWzYsI+EhFgO3vOgV651vaUEUAKXkqggf5c6Epci9957yzn99DfZsGEfXbrU48cfbwh3SCIipVJIk7iZXWBmv5jZWjO7P0D5XWa2wsyWmdk3ZtY4lPFI8Tj99EbUrFmRu+/uyZw519CiRfVwhyRSonz++eecdNJJtGjRgieeeCJgnVGjRlG/fn06duxI27Zteffdd7PKnHOMHj2ali1b0qpVK84880yWL1+eVX7w4EFuuOEGmjdvzqmnnkrfvn354YcfQv6+8mvQoEGsW7cu74phEszvafz48dSqVYuOHTvSsWNHXn/9dQCWLFlCz549adeuHR06dGDKlClZ+wwZMoQ1a9YUSYwhO51uZtHAS8C5wGZggZlNd86t8Kv2I9DFOXfIzG4C/gkMDlVMZcIPP8D8+YHLFtQGEmHeD/DCx0Xb78knZ/1Yt24lVq68hWrVKhRtHyKlQEZGBrfccgtfffUVDRo0oGvXrgwYMIC2bdtmq/unP/2Je+65hzVr1nDqqacyaNAgypUrx0svvcTcuXNZunQpFStW5Msvv2TAgAEsX76cuLg4rrvuOpo2bcqaNWuIiopi/fr1rFixIkA0BeOcwzlHVFTBjwOXL19ORkYGzZo1C3qfjIwMoqOjC9xnfuTn9zR48GBefPHF47ZVrFiRt99+m5YtW7J161ZOPfVUzj//fKpWrcpNN93EP//5T1577bVCxxnKa+LdgLXOuXUAZjYZuBjI+ktyzs30qz8PGB7CeEq/9HQ45xw4eDBg8btnTqAX8HDt2kw4uXZoYvD9A1MCl4gQqmvjLuexH/Pnz6dFixZZyWvIkCFMmzYtYHI4pmXLllSsWJG9e/eSmJjIk08+yXfffUfFihUBOO+88+jVqxcTJ07MOuqeOHFiVpJt2rRpwBHPn3/+OQ8++CAZGRnUrFmTb775hlGjRpGQkMA999wDQPv27fnkk08AOP/88+nevTuLFi3i8ssv5+DBgzz11FOAd0S6cOFCXnzxRd555x2ef/550tLS6N69Oy+//HK25Dtx4kQuvvjirNc33XQTCxYsIDU1lUGDBvHoo48C0KRJEwYPHsxXX33FvffeS/Xq1XnkkUc4cuQIzZs358033yQhIYHHHnuMjz/+mNTUVHr16sWYMWMKNfahIL8nf61atcr6uV69eiQmJrJz506qVq3K6aefzogRI0hPTycmpnBpOJSn0+sDm/xeb/Zty8m1wGchjKf0y8jwErgZ3HprtsfaBg1C2/+86pCP+xtFyqItW7bQsGHDrNcNGjRgy5YtADz88MNMnz492z6LFy+mZcuWJCYmkpycTEpKSrYj2C5durB8+XKWL19Ox44d8zxi3blzJyNHjuTDDz9k6dKlvP/++3nGvmbNGm6++WaWL1/OzTffzH/+85+ssilTpjBkyBBWrlzJlClTmDNnDkuWLCE6OpqJEydma2vOnDmceuqpWa8ff/xxFi5cyLJly/juu+9YtmxZVlmNGjVYvHgx55xzDqNHj+brr79m8eLFdOnShX//+98A3HrrrSxYsICff/6Z1NTUrC8e/iZOnJh12tv/MWjQoGx1c/s9nejDDz+kQ4cODBo0iE2bNmUrnz9/PmlpaTRv7s2NERUVRYsWLVi6dGnA9vKjRIxON7PhQBegTw7l1wPXAzRq1KgYI4tQ5crBCy9k337VLAAea9qEt/s2KVQX77yzjJtu+pSDB9No3LgKGzfe6RVkG/kgUoLlcsQcDo899thxr5955hnefPNNVq9ezccfF+0lsHnz5nHGGWdkHaFXr5732JXGjRvTo0cPAGrVqkWzZs2YN28eLVu2ZNWqVfTu3ZuXXnqJRYsW0bVrVwBSU1NJTEzM1lZSUhK1av2+1PB7773H2LFjSU9PJykpiRUrVtChQwfAO119LOYVK1bQu3dvANLS0ujZsycAM2fO5J///CeHDh1iz549tGvXjosuuui4PocNG8awYcPy9Tnl5aKLLuKKK66gfPnyjBkzhquvvppvv/32uPd55ZVX8tZbbx13+SExMTHrNHthhDKJbwEa+r1u4Nt2HDM7B3gI6OOcOxKoIefcWGAsQJcuXUrWv7oyJjX1KDfd9ClvveV9g7z88naMGXMh1aqFOTCRCFG/fv3jjtY2b95M/fqBT1IeuyY+ffp0rr32Wn799VcqV65MfHw869atO+5ofNGiRfTp04d27dqxdOnSAl8/jomJITMzM+u1/33L8fHxx9UdMmQI7733Hq1bt2bgwIGYGc45rr76av7xj3/k2k+FChWy2l6/fj1PP/00CxYsoFq1aowYMSJgv845zj333OMG+R2L8eabb2bhwoU0bNiQUaNGBbzfeuLEiVmn//21aNGCDz744Lhtwf6eatSokfXzddddx7333pv1Ojk5mf79+/P4449nffnxj7lChcJfdgzl6fQFQEsza2pmscAQ4LjzRGbWCRgDDHDO7QhhLFJEYmOj+e23/VSoEMNrr13E5MmXUbVqTrO0iciJunbtypo1a1i/fj1paWlMnjyZAQMG5LrPgAED6NKlC2+99RYAf/7zn7n99ttJTU0F4Ouvv+Z///sfQ4cOpXnz5nTp0oVHHnkE5zvTsGHDBj799NPj2uzRowezZ89m/fr1AOzZswfwrkEvXrwY8E7jHysPZODAgUybNo13332Xzyu/8gAAIABJREFUIUOGAHD22WfzwQcfsGPHjqx2N27cmG3fNm3asHbtWsBLdvHx8VSpUoXt27fz2WeBr6z26NGDOXPmZO2XkpLC6tWrsxJ2zZo1OXjwYLaEfMywYcNYsmRJtkeg+sH+npKSkrJ+nj59Om3atAG8swQDBw7kqquuCni6fvXq1bRv3z5gnPkRsiNx51y6md0KfAFEA+Occ8vN7DFgoXNuOvAUkAC87xuA8JtzLve/ZgmZ/v1hRp4rhkYBVwMwcqT3EJHgxcTE8OKLL3L++eeTkZHBNddcQ7t27QDvmniXLl0CJouHH36YoUOHMnLkSG677Tb27t3LySefTHR0NHXq1GHatGlZR3avv/46d999Ny1atKBChQrUrFkz2xForVq1GDt2LJdeeimZmZkkJiby1Vdfcdlll/H222/Trl07unfvftwArRNVq1aNNm3asGLFCrp16wZA27ZtGT16NOf9f3t3Hh/ztf9x/HUkCAliSa59TYgESQlBLVFbLTctorZrqeX2R1GXbr9rV9qLlupVP0uXqOa2Wi5FVVFSlRaRVFSURkUtRSNoFiHb+f0xk6/ETDYJycjn+XjMw8x8lzlzkvjM93y/c969epGRkWFcTd+gQfZvEPfr14+QkBB69OiBt7c3jz32GB4eHtSrV88YLr+Xi4sLQUFBDBs2jDt3TAO3CxcupGnTpkyYMIEWLVpQs2ZNYyi/MPL7c3rnnXfYtm0b9vb2VKtWjaCgIMB0euDAgQPExcUZzwUFBeHj48PVq1epUKECNWvWLHQ7lS5h54Ty4uvrq48ePVrczSiZ7twBBwcoV850/x5jR4UwagM0XNDQahRpYS/U7dsX7vmwf3ff8007lxnbRHH7+eefjaMlUXySk5Pp1q0boaGhD+1rYyXF8uXLqVy5MuPGjbNYZu33UykVrrX2tbYvmbFNWNDadNu3L4batZcB83F2XszJk7HGMmu3nAq4EELcq0KFCsyfPz/HK74fZc7OzowePbpI9lUirk4XJUtaWgbz54ewaNF3aA2dOtUnOHgg9etXKe6mCSEeIb179y7uJhSLZ599tsj2JUVcWPD3DyI09AJKwZw5XZg9uyv29jJoI4QQJY0UcWEhNPQCtWtXIjh4IP6F/D65EEKIB0eKuI3pd/w4O81fBbFqv3km25AQi0X5HcCZNaszL7zQnho1Kha4fUIIIR4eGSO1MbkW8Ptw8mQs3bt/xOXLCcZzr732hBRwIYSwAVLEbZT297e8deiA7tYN3bu31eWzs3xPU2vN2rXh+PquZd++GObM2Z/LqwkhitLYsWNxdXXNdbKPrBGXHh4eLF++PNvytWvX4uHhgYeHB+3atePgwYPGstTUVF599VXc3d1p3bo1HTp0yHECleI0bdo0Dhw4UNzNyFF4eDgtW7bEzc2NqVOnkttXssPCwrC3tzcmjtm/f3+2+dkdHBzYunUrULRRpFLES6Hbt9MYMmQTzz23g+TkNMaM8WH58ieLu1lClBpjxoxh165dea43ZMgQjh07RmhoKIsWLTKmAd2xYwdr1qzh4MGDnDp1itWrVzN8+HCuXLkCwOzZs7l8+TInTpwgIiKCrVu3kpCQkNtLFVh6enqhto+LizPmb8+vtLS0Qr1mQU2cOJF169YRHR1NdHR0jj+z9PR0XnnlFXr16mU8161bN2NGuH379lGxYkVjeWYUaVGQc+Kl0Kp3w/j8z1+oVKkcq1f3Z/jwlnlvJMQjKHMSoqKW16RGXbp04dy5c/neX/Xq1XFzc+Py5cvUq1ePxYsXs3TpUmrUqAFA69atGT16NO+++y7/+7//y7p164iJiaF8+fIA/OUvf+GZZ56x2G9YWBgvvPACSUlJlC9fnm+++YbNmzcbkaIA/fv358UXX8Tf3x8nJyeee+459u7dy+DBg7Oln4WEhPDmm2+yY8cOdu/ebTUuNKvNmzfz5JN3Dx5yihL19/fHx8eHgwcPMmzYMPz9/Zk+fTqJiYnUqFGDoKAgatWqxbp161i7di0pKSm4ubmxYcMGI6r1fly+fJn4+HhjzvNRo0axdetW+vTpY7Huv//9bwYNGkRYWJjVfW3atIk+ffoY7SnKKFIp4jbK+uxq5QENKYCV5c8Co4Cbf3YAhpGQACNGmG7Z9v2A/mMTQuRu9erVAPzP//xPtufPnz/P7du3jVSvqKgoi/SrzLnVz5w5Q/369alcuXKur5WSksKQIUPYuHEjbdu2JT4+Ps9AjqSkJPz8/HjrrbdIS0ujcePGJCUl4ejoaESRXrt2zYgLdXR0ZPHixSxbtow5c+Zk21doaGi2OcUnT55srDNy5Eh27NhhpJClpKRw9OhRUlNT6dq1K1988QUuLi5s3LiRmTNn8sEHHzBw4EAmmOeBnjVrFu+//z5TpkzJ9pr79+/nH//4h8X7qlixIt9//3225y5dukTdLPHNOUWRXrp0iS1btrB///4ci/inn37K9OnTjcdZo0hLcoqZsEXuD27atb7ufR/YvoW4HyVtGuB7i/fGjRs5cOAAp06dYuXKlTg4FF3Y0OnTp6lVq5Yxz3heRR/Azs6OQYMGAaa5xZ988km2b99OYGAgX375JUuWLOHbb7/NMS40q3ujSHOLEs2MIj19+jQnTpygZ8+egGkYu1atWgCcOHGCWbNmcfPmTRITE61OJJM5xF2Upk2bxuLFi7PFjN77Pn/66SeL9thCFKl4gPRrC2HaNOsLHR2NQ/WvvoqmXDk7undvTMxs+G0hLFgAH8223EzmNxeiZBkyZAgrV67k6NGj9OrVi4CAAGrWrImnpyfh4eE88cQTxrrh4eF4eXnh5ubG+fPniY+Pz1dhvlduUaQODg7Z5jkfOnQoK1eupFq1avj6+lKpUqUc40LvlTWKNK8o0axRpF5eXvzwww8W+xszZgxbt27F29uboKAgQqx8zbYgR+J16tTh4sWLxuOcokiPHj1qJLhdu3aNnTt3Ym9vz9NPPw2YglAGDBhA2bJls21nC1Gk4kEqXx6cnKzflCIlJZ0ZM76mb9//MHz4f4mNTSruFgsh7pOvry8jR45kxYoVALz88su88sorxMXFAXDs2DGCgoKYNGkSFStWZNy4cbzwwgukpKQAEBsba5y7ztSsWTMuX75sDAEnJCSQlpZGw4YNOXbsGBkZGVy4cIEjR47k2K6uXbsSERHBunXrjEKWU1zovbJGkeY3SrRZs2bExsYaRTw1NZWoqCij/bVq1SI1NZXg4GCr22e92Czr7d4CDlCrVi0qV67MoUOH0Frz0Ucf8dRTT1msFxMTw7lz5zh37hyBgYGsWrXKKOAAn3zyCcOGDbPYrqiiSKWIP4Kio+Po2PF9li07hL19GaZPb0/16vK9byFKimHDhtGhQwdOnz5N3bp1ef/99wHTOfHM8+L3euWVV/jwww9JSEggICCAsWPH0rFjRzw8PJgwYQIff/yxMbS8cOFCXFxc8PT0pEWLFvTv39/iqLxcuXJs3LiRKVOm4O3tTc+ePbl9+zaPP/44jRo1wtPTk6lTp9K6desc34ednR39+/fnq6++on///kD2uNBWrVrRoUMHTp06ZbFtZhQpmAJBMqNEe/funWOUaLly5di0aROvvPIK3t7e+Pj4GAX4tddew8/Pj8cffxwPD49cej//Vq1axfjx43Fzc6NJkybGRW25/ZyyOnfuHBcuXKBr167Znpco0kc8ivTGvhskHLX+dZBXfj0LwOKLF+CeXwyAiIjLbNlyipSUdKpWdWD48JZGcMn1r69zc9/NnKNIZThdlAISRVpydOrUiR07duDs7FzcTXmoijKKVM6JlzDpt9M53vc4+o71Qvqc+d+z1IOdZy2WOwPPYp7U5QakvRvHWeKyrfPywZf5fP7nFtsKIcTD9NZbb3H+/PlSV8SdnZ0ZOXJkkexLingJo+9o9B2NKquoO62uxfKl502TPbx05TK0a5dt2fnf/mTjxbW57j+pfBK7fHKeZEKuIBdCPCx+fn7F3YRiIVGkpUCZCmVosqSJxfNrQkxFfHXYRfSLQ/nhh4t07FgPgCbAE/PXADIkLoQQpYFc2GajEhNT+OtfP6FTpw/45hvLYXUhhBCPPjkSt1HLl//AlwlpVK3qwO3bD3c+YSGEECWDFHEbkpZ2dwKG+IQUOnWqT3DwQOPqcyGEEKWLDKfbiIsX4+naNch43LNHI/bvHy0FXAgbc+HCBbp164anpydeXl7GBC73kijS4pefKNKQkBCqVKliRI4uWLDAWNawYUNatmyJj48Pvr53vyH24osvsm/fviJpoxyJ24iyZcvw66/XgcYA9OrlBvbyGUwIW2Nvb89bb71F69atSUhIoE2bNvTs2RNPT0+LdTOnXY2Li6NZs2YEBgZSr169bFGkNWrUICIigqeffpojR45Qs2bNbFGk5cuX5+rVq3z77bdF+j7S09OzTcFaUJlRpG+//Xa+tymK1K+CyIwi9fPzo2/fvuzatctqilnnzp3ZsWOH1X3s37/fSJvLNGXKFCZMmJBt2tz7JVWgBEtOTjWG0P/yFye2b7ecuk8Icf+UejC33NSqVcuYBa1SpUo0b97cajpWVlmjSIFco0hv3brFunXr+Pe//52vKNKOHTvi7e1Nu3btSEhIICgoiMmTJxvr9O/f35hZzcnJiRkzZuDt7c0bb7zB4MGDjfVCQkKMWdt2795Nhw4daN26NYMHDyYxMdHita1FkbZt25YWLVrw97//3Tjq9ff3Z9q0afj6+rJixQrCw8Pp2rUrbdq0oXfv3kafrFu3jrZt2+Lt7c2gQYO4detWrn2al6xRpEopI4q0KDRo0IC4uDgj/70wpIiXUBkZmnbt3mPBgrufntu2tZx8Xwhhu86dO8ePP/5ofF86p+k88xtFGhUVVeAo0hUrVhAZGcnevXvzHUUaGRnJq6++yuHDh0lKMuUyWIsijYiIwNfXl2XLllnsKzQ0NNt7mDx5MmFhYZw4cYLk5ORsR7aZUaRTp05lypQpbNq0ifDwcMaOHcvMmTMBGDhwIGFhYURGRtK8eXNjKtus9u/fbwx7Z7117NjRYt38RpEC/PDDD3h7e9OnTx9jLncApRS9evWiTZs2rF2bfQ6P1q1bExoaanV/BSHD6SVM5qfP1MRbBJzYRPWrFUm1O0jZzKFzc7yfEKLwinPW6cTERAYNGsTbb79tFFyJIrW9KNLWrVvz22+/4eTkxM6dO3n66aeJjo4G4ODBg9SpU4c//viDnj174uHhQZcuXYC7UaSFJUW8BLl58zbTRv6XMdTmtqMDr+8w5YVaCxxVt16G+S8/3AYKIYpEamoqgwYNYsSIEQwcODDH9SSK1KQkR5Fm7eO+ffsyadIkrl27Ro0aNYz1XV1dGTBgAEeOHDGKuESRPmK+//4CPj6r+XrHz3mvfKharotl6lQhSi6tNePGjaN58+ZMnz49X9tIFOndNpe0KNIrV64YI6hHjhwhIyOD6tWrk5SUREJCgtEHu3fvzhY9WlRRpHIkXkIsXHiA3377k14tXeEn03Pa399ivcyLZmwtfU4IYRIaGsqGDRuMrx4BvP766/Tt29c4H37vsDqYokhbt27NP//5TwICArh06RIdO3ZEKUWlSpUsokhnzZqFp6cnDg4OODo6ZvvqE2SPIk1OTqZChQrs3bs3WxRp8+bN8xVFGhQUxPr164HsUaR37twx2tO0adNs2/br1481a9Ywfvz4bFGkNWvWzDOKdOrUqfz555+kpaUxbdo0vLy8jChSFxcX/Pz8jAJaGKtWrWLMmDEkJyfTp0+fbFGkYPo5bdq0if/7v//D3t6eChUq8Omnn6KU4urVqwwYMAAwXVU/fPhw40K+1NRUzpw5k+1rZ/dLokhLiCtXElm1KoxXR7fgiFsUiY7QP9HfYr27Rfzhtk+IR4VEkZYcpTWKdMuWLURERPDaa69ZLCtoFKkMpxeTnTujGTz4c9LTTeeeatZ0YsGCbpQre//fuxRCCFuSGUVa2qSlpTFjxowi2ZcMp+ei33/6sTN65/1t3OINqN4+5+UVgeddsP8u+2xFjolgfcoAIYR4tJTWKNKs368vLDkSz8V9F3DIvYDnQ5mM9EJtL4QQ4tEnR+L5cD/Z3Mr89YbMi9M2bIhk0qSdJCamEFTpG0YnfAdvvw1ZZkYCSPszjYP8QKWy5QrbbCGEEI84ORJ/CL744hSjRm0lMTGFIUO8GDrUy7SgTBmws7O8CSGEEPkgRfwh6N+/Kf36ufPee3/lk08GUb68DIAIIYQoPKkm+TFvXoFW11pDt27GtnbA9jYadeEXmL8NDh8u8iYKIWzD7du36dKlC3fu3CEtLY3AwEDmz59vsd68efNYt24dLi4upKSkMHv2bIYNM4Ugaa1ZtGgR69evRylFnTp1WLlyJV5eplG+xMREZsyYwd69e3F2dqZSpUosXry4xF1IFhgYyJIlS2jcuHFxN8WqXbt28cILL5Cens748eN59dVXLdYJCgripZdeMmZnmzx5MuPHj+fYsWNMnDiR+Ph47OzsmDlzpjF97NChQ3nttddwd3cvdBuliOfk2rW79638gfV74w12ts/HxWvmba0GG1WqdH9tE0LYrPLly7Nv3z6cnJxITU2lU6dO9OnTh/ZW/j/5xz/+wYsvvkh0dDRt2rQhMDCQsmXL8u677/L9998TGRlJxYoV2b17NwEBAURFReHg4MD48eNp1KgR0dHRlClThpiYGE6ePFlk70FrjdaaMmXufzA3KiqK9PT0AhXwwsafFkR6ejrPP/88e/bsoW7durRt25aAgIBcI2OzqlixIh999BHu7u78/vvvRuqas7MzEydOZMmSJaxbt67Q7ZQinpOsMXZz51oszk8B7/LbZavbAlC1KhTh1wyEEAWnrMyvXRSszbZovKZSODk5AaaZu1JTU1F55Je6u7tTsWJFbty4gaurK4sXL+bbb7+lYsWKAPTq1YuOHTsSHByMv78/hw8fJjg42CiyjRo1olGjRhb73bVrF//85z9JT0+nRo0afPPNN8ybNw8nJydefPFFAFq0aGEkivXu3Rs/Pz/Cw8N55plnSExMZOnSpYDpiPTo0aOsXLmSjz/+mHfeeYeUlBT8/PxYtWqVRfENDg7ONo3pxIkTCQsLIzk5OdvoRMOGDRkyZAh79uzh5Zdfplq1asydO5c7d+7QpEkTPvzwQ5ycnFiwYAHbt28nOTmZjh07smbNmjz7NTdHjhzBzc3N+JAxdOhQvvjiC6tF3JqsM9TVrl0bV1dXYmNjcXZ2pnPnzowZM6ZI8tGliOeHteH0LFefp6VlMHfuft544yBaQ+fO9QkOHki9XP6QhRClV3p6Om3atOHMmTM8//zzxjD3nDlz8PX1JSAgINv6ERERuLu74+rqSnx8PElJSRZHsJlRpC4uLvj4+OR5xBobG8uECRM4cOAAjRo14vr163m2Ozo6mvXr19O+fXtiY2Pp0KGDUcQ3btzIzJkz+fnnn9m4cSOhoaGULVuWSZMmERwczKhRo7LtKzQ01Dg9ALBo0SKqVatGeno63bt35/jx40b0avXq1YmIiODatWsMHDiQvXv34ujoyOLFi1m2bBlz5sxh8uTJzJkzB4CRI0eyY8cOIwUtU3BwsNHerNzc3Czma7906RL16tUzHtetW5fDOZwK3bx5MwcOHKBp06YsX74823Zg+kCQkpJCkyZNAChTpgxubm5ERkZaRMoWlBTxQkpLy+CJJ9bz3XfnKVNGMWdOF2bN6oK9vVwzKERJl9sR84NkZ2fHsWPHuHnzJgMGDODEiRO0aNHCYn7z5cuX8+GHH/LLL7+wffv2Im3DoUOH6NKli3GEXq1a7sFKAA0aNDCG/V1cXGjcuDGHDh3C3d2dU6dO8fjjj/Puu+8SHh5uzH+enJyMq6urxb7ujSL97LPPWLt2LWlpaVy+fJmTJ08aRTzzXPKhQ4dyjDnNLco004gRIxgxYkSB+ikvf/3rXxk2bBjly5dnzZo1jB49mn379mV7nyNHjmT9+vXZTj9kRpFKES9m9vZl6N69EWfP3iA4eCBduzYs7iYJIWyEs7Mz3bp1Y9euXVYTrTLPiW/bto1x48bx66+/UrlyZRwdHTl79my2o/Hw8HC6du2Kl5cXkZGR933+OLco0sxI0ExDhw7ls88+w8PDgwEDBqCUQmvN6NGjeeONN3J9naxRpDExMbz55puEhYVRtWpVxowZk2MUqbWY07yiTDMV5Ei8Tp06XLhwwXicUxRp9erVjfvjx4/n5ZfvRkTHx8fTr18/Fi1aZHHNg0SRFoF+/+mHmq+s3z5sYJo6tet+VEiIxS2rWbO6cPz4xCIp4ImRiYXehxCi5IqNjeXmzZuA6Sh1z549eHh45LpNQEAAvr6+RlLYSy+9xNSpU0lOTgZg7969HDx4kOHDh9OkSRN8fX2ZO3eukXZ47tw5vvzyy2z7bN++PQcOHCAmJgbAGE5v2LAhERERgGkYP3O5NQMGDOCLL77gk08+MaJIu3fvzqZNm/jjjz+M/f72228W22aNIo2Pj8fR0ZEqVapw9epVvvrqK6uvl1PMaX6jTEeMGGE1itTa+m3btiU6OpqYmBhSUlL49NNPLU5zgOlIO9O2bduM8JKUlBQGDBjAqFGjCAwMtNhOokiLQJ7TquYxdWp3xyoA2NmVoVq1wn2i0hmaC0svEDPL9AdTrVfeQ1tCCNtz+fJlRo8eTXp6OhkZGTzzzDP0798fyPmceOay4cOHM2HCBKZMmcKNGzdo2bIldnZ21KxZky+++MI4snvvvfeYMWMGbm5uVKhQgRo1algcgbq4uLB27VoGDhxIRkYGrq6u7Nmzh0GDBvHRRx/h5eWFn5+fRYRoVlWrVqV58+acPHmSdu3aAeDp6cnChQvp1asXGRkZxtX0DRo0yLZtv379CAkJoUePHnh7e/PYY4/h4eFBvXr1jOHye+UWc5qfKNOCsLe3Z+XKlfTu3Zv09HTGjh1rfIUv68/pnXfeYdu2bdjb21OtWjWCgoIA0+mBAwcOEBcXZzwXFBSEj48PV69epUKFCtSsWbPQ7SzVUaRqvunKRavTqp4/jzp71rTc3x+tNWvXhjNt2tfcvp1Gs2bV2bJlCM2bu1huW0B3rtzh1KhT3NhzA4C60+vS+PXGlClvOVAiUaRCFI5EkZYMycnJdOvWjdDQ0If2tbGSYvny5VSuXJlx48ZZLCtoFGmpPhLPrxs3kpkwYTubN/8MwNixPrzzTh8cHQs/v/n1r6/z86ifSf0jlbI1yuKx3oPqfavnvaEQQtiwChUqMH/+fC5dukT9+vWLuzkPlbOzMyNHjiySfUkRzwcfnzWcP/8nlSqVY82a/gwb1rLQ+8xIySBmVgwXlpounHB+wpnmG5pTvnb5Qu9bCCFsQe/evYu7CcXi2WefLbJ9le4ibs78zmvCh/PnpwGQkADDh5tu98OeDBpwi8YkMoBLNCeBdOBDGvHJvvpk1Ln/iQmEEEKUPqW7iOcn8/vQ/V1g5kwKTUg035JoQiL1uUVZ7p7MvkJ5FuJJFFUKtO++fe+rSUIIIR4xpbuIm2Wd8CFs3loaLZhODZ2EMhfc3C4iy0jN4NapWyQdTyIxMpHEyESSjieRciXFcmUFFdwq4NjKkUq+lXj8udoMrVq2iN+NEEKI0kKKuNmd+CQO+f+Nrj9uBeCnWt5wOfs6KddSSIpMIvG4uVhHJpF0MgmdYlnl7SrZ4djKESdvJ5y8nXBs5YhjC0fsnaTLhRBCFI0HOtmLUupJpdRppdQZpZRFhptSqrxSaqN5+WGlVMMH2Z6cxOw+wtmannT9cSuplCGk9yQa7jrAE1xlAr9yvO9xvq/zPd+7fE9kj0h+nf4rV9dfJfFYIjpF49DEgRoDatBwXkO8tnjhd9aPTjc70fpga5q+25Taf69NlfZVpIALIQzp6ek89thjxnfE7zVv3jzq1KmDj48Pnp6e2WYp01qzcOFC3N3dadq0Kd26dSMqKspYnpiYyHPPPUeTJk1o06aNEYpS0gQGBnLW/FXekmjXrl00a9YMNzc3/vWvf1ldJygoyJiv3sfHh/fee89Ytn79etzd3XF3dzcm6gHo0aMHN27cKJI2PrCqopSyA94FegIXgTCl1DatddY8vHHADa21m1JqKLAYGPKg2mRNpXgo/9TzlKUtR9Vokut1RYXYE+4dwWzzOtfNkweVcSyDU6u7R9ZO3k44tnTEvpIUZyFEwaxYsYLmzZsTHx+f4zoSRZqdLUWRXr9+nfnz53P06FGUUrRp04aAgACqVq3KyJEjWbVqFTNnzix0Ox9k9WkHnNFanwVQSn0KPAVk/U16Cphnvr8JWKmUUvphzECjNXPmQ7cQ+AXzTEYaOG+68zsOnMWRX3FiyWZT4XZo5IAqI1eQC/GoCFEhD2S//to/1+UXL17kyy+/ZObMmSxbtizP/UkUqe1FkX799df07NnTCJbp2bMnu3btYtiwYQQEBNC5c+ciKeIPcji9DnAhy+OL5uesrqO1TgP+BCxmOlFK/V0pdVQpdTQ2NrZoWqcU12pAsgOcpBLbqcUK3JmCD/3pxAjaM5uW/NG3ES4DXajQpIIUcCFEkZg2bRpLliyxOJKdM2cO27Zts1i/IFGkUVFRBYoi3bx5M5GRkXz++ed5tjs6OppJkyYRFRXFpEmT2LJli7Fs48aNDB06NFsU6bFjx7CzsyM4ONhiX6GhodkSvBYtWsTRo0c5fvw43377LcePHzeWZUaR9ujRg4ULF7J3714iIiLw9fU1PgRNnjyZsLAwTpw4QXJysvHBI6vg4GBj2Dvrzdrc5taiSC9dumS1XzZv3kyrVq0IDAw0QlNy275q1arcuXOHuLg46x1dADYxDqy1XgusBdO0q0W13xXvdcTu6nlUU7ei2qUQwobkdcT8IOzYsQNXV1fatGlDyD1zVEgU6aMXRZqTzChdcDRyAAALSklEQVTSrClo9+NBFvFLQNZk9Lrm56ytc1EpZQ9UAQr/0SSf7KuUgypSwIUQD09oaCjbtm1j586d3L59m/j4eP72t7/x8ccfW6wrUaTZX9eWokjr1KmT7UPaxYsX8c/ydeaiiiI1LlAo6humDwhngUZAOSAS8LpnneeB1eb7Q4HP8tpvmzZttBBC3K+TJ08WdxMM+/fv1/369bO6bO7cuXrp0qXG44CAAL169WqttdYrVqzQ/fr107du3dJaa71nzx7dqFEj4/HgwYP1zJkzdUZGhtZa65iYGL1jx45s+//jjz903bp19dmzZ7XWWsfFxWmttd6wYYMeMmSI1lrr8PBwXaZMGR0TE6NjYmK0l5dXtn1cv35dN27cWPv7++vDhw9rrbWOiorSbm5u+urVq8Z+z507Z/H+hgwZovfs2aO11vrYsWO6VatWOj09XV+5ckW7urrqDz/8UGutdYMGDXRsbKzR5nr16uno6GittdaJiYn69OnT+saNG9rV1VXfunVLJyQkaC8vLz137tycOz4fUlNTdaNGjfTZs2f1nTt3dKtWrfSJEycs1vv999+N+//973+1n5+f8b4bNmyor1+/rq9fv64bNmxo9HFGRoauXbu2Tk1Ntdiftd9P4KjOoSY+sCNxrXWaUmoy8DVgB3ygtY5SSi0wN2gb8D6wQSl1BrhuLuRCCFEqSRTpoxNFWq1aNWbPnm20Zc6cOcYpi/DwcNq3b4+9feFLcKmOIhVClD4SRVoylOYo0hdeeIGAgAC6d+9usaygUaQPdLIXIYQQwpqsUaSlTYsWLawW8PthE1enCyGEePSU1ijSCRMmFNm+5EhcCFHq2NppRFE63M/vpRRxIUSp4uDgQFxcnBRyUaJorYmLi8PBwaFA28lwuhCiVKlbty4XL16kyGZ/FKKIODg4ULdu3QJtI0VcCFGqlC1b1uo84kLYIhlOF0IIIWyUFHEhhBDCRkkRF0IIIWyUzc3YppSKBX4rwl3WAK4V4f5KK+nHwpM+LDzpw8KTPiy8ou7DBlprF2sLbK6IFzWl1NGcprMT+Sf9WHjSh4UnfVh40oeF9zD7UIbThRBCCBslRVwIIYSwUVLEYW1xN+ARIf1YeNKHhSd9WHjSh4X30Pqw1J8TF0IIIWyVHIkLIYQQNqrUFHGl1JNKqdNKqTNKqVetLC+vlNpoXn5YKdXw4beyZMtHH05XSp1USh1XSn2jlGpQHO0syfLqwyzrDVJKaaWUXCVsRX76USn1jPn3MUop9Z+H3caSLh9/z/WVUvuVUj+a/6b7Fkc7Syql1AdKqT+UUidyWK6UUu+Y+/e4Uqr1A2mI1vqRvwF2wK9AY6AcEAl43rPOJGC1+f5QYGNxt7sk3fLZh92Aiub7E6UPC96H5vUqAQeAQ4Bvcbe7pN3y+bvoDvwIVDU/di3udpekWz77cC0w0XzfEzhX3O0uSTegC9AaOJHD8r7AV4AC2gOHH0Q7SsuReDvgjNb6rNY6BfgUeOqedZ4C1pvvbwK6K6XUQ2xjSZdnH2qt92utb5kfHgIKFsfz6MvP7yHAa8Bi4PbDbJwNyU8/TgDe1VrfANBa//GQ21jS5acPNVDZfL8K8PtDbF+Jp7U+AFzPZZWngI+0ySHAWSlVq6jbUVqKeB3gQpbHF83PWV1Ha50G/AlUfyitsw356cOsxmH6FCruyrMPzUNu9bTWXz7MhtmY/PwuNgWaKqVClVKHlFJPPrTW2Yb89OE84G9KqYvATmDKw2naI6Og/2feF4kiFUVOKfU3wBfoWtxtsSVKqTLAMmBMMTflUWCPaUjdH9OI0AGlVEut9c1ibZVtGQYEaa3fUkp1ADYopVporTOKu2HirtJyJH4JqJflcV3zc1bXUUrZYxo+insorbMN+elDlFI9gJlAgNb6zkNqm63Iqw8rAS2AEKXUOUzn0bbJxW0W8vO7eBHYprVO1VrHAL9gKurCJD99OA74DEBr/QPggGlOcJE/+fo/s7BKSxEPA9yVUo2UUuUwXbi27Z51tgGjzfcDgX3afHWCAPLRh0qpx4A1mAq4nIO0lGsfaq3/1FrX0Fo31Fo3xHRdQYDW+mjxNLfEys/f81ZMR+EopWpgGl4/+zAbWcLlpw/PA90BlFLNMRXx2IfaStu2DRhlvkq9PfCn1vpyUb9IqRhO11qnKaUmA19juirzA611lFJqAXBUa70NeB/TcNEZTBcrDC2+Fpc8+ezDpYAT8Ln5msDzWuuAYmt0CZPPPhR5yGc/fg30UkqdBNKBl7TWMrJmls8+nAGsU0r9A9NFbmPkwOYupdQnmD4o1jBfNzAXKAugtV6N6TqCvsAZ4Bbw7ANph/xMhBBCCNtUWobThRBCiEeOFHEhhBDCRkkRF0IIIWyUFHEhhBDCRkkRF0IIIWyUFHEhioFSKl0pdSzLrWEu6yYWwesFKaVizK8VYZ6Bq6D7eE8p5Wm+/897ln1f2Daa95PZLyeUUtuVUs55rO8j6VqiNJOvmAlRDJRSiVprp6JeN5d9BAE7tNablFK9gDe11q0Ksb9Ctymv/Sql1gO/aK0X5bL+GExJb5OLui1C2AI5EheiBFBKOZkz2COUUj8ppSzSzZRStZRSB7IcqXY2P99LKfWDedvPlVJ5FdcDgJt52+nmfZ1QSk0zP+eolPpSKRVpfn6I+fkQpZSvUupfQAVzO4LNyxLN/36qlOqXpc1BSqlApZSdUmqpUirMnK38XD665QfMgRFKqXbm9/ijUup7pVQz80xjC4Ah5rYMMbf9A6XUEfO61lLihHhklIoZ24QogSoopY6Z78cAg4EBWut48zShh5RS2+6ZIWs48LXWepFSyg6oaF53FtBDa52klHoFmI6puOXkr8BPSqk2mGaR8sOUeXxYKfUtpozp37XW/QCUUlWybqy1flUpNVlr7WNl3xuBZ4AvzUW2O6Zs+XGYpp1sq5QqD4QqpXab5zW3YH5/3THNpAhwCuhsnmmsB/C61nqQUmoOWY7ElVKvY5oyeax5KP6IUmqv1jopl/4QwmZJEReieCRnLYJKqbLA60qpLkAGpiPQvwBXsmwTBnxgXner1vqYUqor4ImpKAKUw3QEa81SpdQsTPNfj8NUJLdkFjil1H+BzsAu4C2l1GJMQ/DfFeB9fQWsMBfqJ4EDWutk8xB+K6VUoHm9KpgCSe4t4pkfbuoAPwN7sqy/XinljmkK0LI5vH4vIEAp9aL5sQNQ37wvIR45UsSFKBlGAC5AG611qjKlmDlkXUFrfcBc5PsBQUqpZcANYI/Welg+XuMlrfWmzAdKqe7WVtJa/6JMueZ9gYVKqW+01rkd2Wfd9rZSKgToDQwBPs18OWCK1vrrPHaRrLX2UUpVxDSv9/PAO8BrwH6t9QDzRYAhOWyvgEFa69P5aa8Qtk7OiQtRMlQB/jAX8G5Ag3tXUEo1AK5qrdcB7wGtMSWdPa6UyjzH7aiUaprP1/wOeFopVVEp5QgMAL5TStUGbmmtP8YUatPayrap5hEBazZiGqbPPKoHU0GemLmNUqqp+TWt0lrfAqYCM9TdaODMGMcxWVZNwBThmulrYIoyD0soU7KeEI8sKeJClAzBgK9S6idgFKZzwPfyByKVUj9iOspdobWOxVTUPlFKHcc0lO6RnxfUWkcAQcAR4DDwntb6R6AlpnPJxzAlMy20svla4HjmhW332A10BfZqrVPMz70HnAQilFInMEXW5joSaG7LcWAYsAR4w/zes263H/DMvLAN0xF7WXPbosyPhXhkyVfMhBBCCBslR+JCCCGEjZIiLoQQQtgoKeJCCCGEjZIiLoQQQtgoKeJCCCGEjZIiLoQQQtgoKeJCCCGEjZIiLoQQQtio/wc9/LTdfcri7wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                  0         1         2         3         4  \\\n",
            "TP                                9        42        54        43        54   \n",
            "TN                               47        15         0        13         0   \n",
            "FP                                7        39        54        41        54   \n",
            "FN                               45        12         0        11         0   \n",
            "Accuracy                   0.518519  0.527778       0.5  0.518519       0.5   \n",
            "Positive predictive value    0.5625  0.518519       0.5  0.511905       0.5   \n",
            "sensitity                  0.166667  0.777778         1  0.796296         1   \n",
            "specificity                 0.87037  0.277778         0  0.240741         0   \n",
            "F-value                    0.257143  0.622222  0.666667  0.623188  0.666667   \n",
            "roc_auc                    0.518347  0.465706  0.553155  0.522977  0.495713   \n",
            "\n",
            "                                avg        std  \n",
            "TP                             40.4    16.5239  \n",
            "TN                               15     17.193  \n",
            "FP                               39     17.193  \n",
            "FN                             13.6    16.5239  \n",
            "Accuracy                   0.511741  0.0124226  \n",
            "Positive predictive value  0.507578  0.0258068  \n",
            "sensitity                   0.72518   0.342117  \n",
            "specificity                0.299099    0.35597  \n",
            "F-value                    0.597174   0.174703  \n",
            "roc_auc                     0.51118  0.0291873  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPO-b2cAT0yF",
        "colab_type": "text"
      },
      "source": [
        "#**ネットワークの保存と読み込み**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMYyFQ6ATzyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワークの保存\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c744XUtfT6xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73a923de-e875-4e71-b4b5-1ae266557981"
      },
      "source": [
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}