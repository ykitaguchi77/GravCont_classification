{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled31.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/ResNet50_VGGFace2_adabound_crossvalidation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-a4ZBlqPNdU",
        "colab_type": "text"
      },
      "source": [
        "#**GravCont: EfficientNet_b4_ImageNet**\n",
        "ValidationとTestに分けて解析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgM-Y7SVPNkM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "5499e6fe-08a1-4c7a-c8b5-3e2c3bfd9e95"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "!pip install torch_optimizer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch_optimizer as optim\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch_optimizer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/48/f670cf4b47c315861d0547f0c2be579cd801304c86e55008492f1acebd01/torch_optimizer-0.0.1a15-py3-none-any.whl (41kB)\n",
            "\r\u001b[K     |███████▉                        | 10kB 24.5MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 30kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer) (1.6.0+cu101)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/70/12256257d861bbc3e176130d25be1de085ce7a9e60594064888a950f2154/pytorch_ranger-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->torch_optimizer) (1.18.5)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.0.1a15\n",
            "Random Seed:  1234\n",
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp36-none-any.whl size=16031 sha256=acb7d82a40cca8ff2058045d34b94b7a69bd276f78a2bbb0efd08c4e9625602a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.0\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzp_09fWPNoU",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFTBpTsjbI7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Resnet50_ft_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Resnet50_ft_dag, self).__init__()\n",
        "        self.meta = {'mean': [131.0912, 103.8827, 91.4953],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv1_relu_7x7_s2 = nn.ReLU()\n",
        "        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
        "        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_3x3_relu = nn.ReLU()\n",
        "        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_3x3_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_3x3_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_3x3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_3x3_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_3x3_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_3x3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_3x3_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_3x3_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_3x3_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_3x3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_3x3_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_3x3_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_3x3_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_3x3_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_3x3_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_relu = nn.ReLU()\n",
        "        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
        "        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))\n",
        "\n",
        "    def forward(self, data):\n",
        "        conv1_7x7_s2 = self.conv1_7x7_s2(data)\n",
        "        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)\n",
        "        conv1_7x7_s2_bnxx = self.conv1_relu_7x7_s2(conv1_7x7_s2_bn)\n",
        "        pool1_3x3_s2 = self.pool1_3x3_s2(conv1_7x7_s2_bnxx)\n",
        "        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(pool1_3x3_s2)\n",
        "        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)\n",
        "        conv2_1_1x1_reduce_bnxx = self.conv2_1_1x1_reduce_relu(conv2_1_1x1_reduce_bn)\n",
        "        conv2_1_3x3 = self.conv2_1_3x3(conv2_1_1x1_reduce_bnxx)\n",
        "        conv2_1_3x3_bn = self.conv2_1_3x3_bn(conv2_1_3x3)\n",
        "        conv2_1_3x3_bnxx = self.conv2_1_3x3_relu(conv2_1_3x3_bn)\n",
        "        conv2_1_1x1_increase = self.conv2_1_1x1_increase(conv2_1_3x3_bnxx)\n",
        "        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)\n",
        "        conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)\n",
        "        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)\n",
        "        conv2_1 = torch.add(conv2_1_1x1_proj_bn, 1, conv2_1_1x1_increase_bn)\n",
        "        conv2_1x = self.conv2_1_relu(conv2_1)\n",
        "        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(conv2_1x)\n",
        "        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)\n",
        "        conv2_2_1x1_reduce_bnxx = self.conv2_2_1x1_reduce_relu(conv2_2_1x1_reduce_bn)\n",
        "        conv2_2_3x3 = self.conv2_2_3x3(conv2_2_1x1_reduce_bnxx)\n",
        "        conv2_2_3x3_bn = self.conv2_2_3x3_bn(conv2_2_3x3)\n",
        "        conv2_2_3x3_bnxx = self.conv2_2_3x3_relu(conv2_2_3x3_bn)\n",
        "        conv2_2_1x1_increase = self.conv2_2_1x1_increase(conv2_2_3x3_bnxx)\n",
        "        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)\n",
        "        conv2_2 = torch.add(conv2_1x, 1, conv2_2_1x1_increase_bn)\n",
        "        conv2_2x = self.conv2_2_relu(conv2_2)\n",
        "        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(conv2_2x)\n",
        "        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)\n",
        "        conv2_3_1x1_reduce_bnxx = self.conv2_3_1x1_reduce_relu(conv2_3_1x1_reduce_bn)\n",
        "        conv2_3_3x3 = self.conv2_3_3x3(conv2_3_1x1_reduce_bnxx)\n",
        "        conv2_3_3x3_bn = self.conv2_3_3x3_bn(conv2_3_3x3)\n",
        "        conv2_3_3x3_bnxx = self.conv2_3_3x3_relu(conv2_3_3x3_bn)\n",
        "        conv2_3_1x1_increase = self.conv2_3_1x1_increase(conv2_3_3x3_bnxx)\n",
        "        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)\n",
        "        conv2_3 = torch.add(conv2_2x, 1, conv2_3_1x1_increase_bn)\n",
        "        conv2_3x = self.conv2_3_relu(conv2_3)\n",
        "        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(conv2_3x)\n",
        "        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)\n",
        "        conv3_1_1x1_reduce_bnxx = self.conv3_1_1x1_reduce_relu(conv3_1_1x1_reduce_bn)\n",
        "        conv3_1_3x3 = self.conv3_1_3x3(conv3_1_1x1_reduce_bnxx)\n",
        "        conv3_1_3x3_bn = self.conv3_1_3x3_bn(conv3_1_3x3)\n",
        "        conv3_1_3x3_bnxx = self.conv3_1_3x3_relu(conv3_1_3x3_bn)\n",
        "        conv3_1_1x1_increase = self.conv3_1_1x1_increase(conv3_1_3x3_bnxx)\n",
        "        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)\n",
        "        conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)\n",
        "        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)\n",
        "        conv3_1 = torch.add(conv3_1_1x1_proj_bn, 1, conv3_1_1x1_increase_bn)\n",
        "        conv3_1x = self.conv3_1_relu(conv3_1)\n",
        "        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(conv3_1x)\n",
        "        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)\n",
        "        conv3_2_1x1_reduce_bnxx = self.conv3_2_1x1_reduce_relu(conv3_2_1x1_reduce_bn)\n",
        "        conv3_2_3x3 = self.conv3_2_3x3(conv3_2_1x1_reduce_bnxx)\n",
        "        conv3_2_3x3_bn = self.conv3_2_3x3_bn(conv3_2_3x3)\n",
        "        conv3_2_3x3_bnxx = self.conv3_2_3x3_relu(conv3_2_3x3_bn)\n",
        "        conv3_2_1x1_increase = self.conv3_2_1x1_increase(conv3_2_3x3_bnxx)\n",
        "        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)\n",
        "        conv3_2 = torch.add(conv3_1x, 1, conv3_2_1x1_increase_bn)\n",
        "        conv3_2x = self.conv3_2_relu(conv3_2)\n",
        "        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(conv3_2x)\n",
        "        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)\n",
        "        conv3_3_1x1_reduce_bnxx = self.conv3_3_1x1_reduce_relu(conv3_3_1x1_reduce_bn)\n",
        "        conv3_3_3x3 = self.conv3_3_3x3(conv3_3_1x1_reduce_bnxx)\n",
        "        conv3_3_3x3_bn = self.conv3_3_3x3_bn(conv3_3_3x3)\n",
        "        conv3_3_3x3_bnxx = self.conv3_3_3x3_relu(conv3_3_3x3_bn)\n",
        "        conv3_3_1x1_increase = self.conv3_3_1x1_increase(conv3_3_3x3_bnxx)\n",
        "        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)\n",
        "        conv3_3 = torch.add(conv3_2x, 1, conv3_3_1x1_increase_bn)\n",
        "        conv3_3x = self.conv3_3_relu(conv3_3)\n",
        "        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(conv3_3x)\n",
        "        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)\n",
        "        conv3_4_1x1_reduce_bnxx = self.conv3_4_1x1_reduce_relu(conv3_4_1x1_reduce_bn)\n",
        "        conv3_4_3x3 = self.conv3_4_3x3(conv3_4_1x1_reduce_bnxx)\n",
        "        conv3_4_3x3_bn = self.conv3_4_3x3_bn(conv3_4_3x3)\n",
        "        conv3_4_3x3_bnxx = self.conv3_4_3x3_relu(conv3_4_3x3_bn)\n",
        "        conv3_4_1x1_increase = self.conv3_4_1x1_increase(conv3_4_3x3_bnxx)\n",
        "        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)\n",
        "        conv3_4 = torch.add(conv3_3x, 1, conv3_4_1x1_increase_bn)\n",
        "        conv3_4x = self.conv3_4_relu(conv3_4)\n",
        "        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(conv3_4x)\n",
        "        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)\n",
        "        conv4_1_1x1_reduce_bnxx = self.conv4_1_1x1_reduce_relu(conv4_1_1x1_reduce_bn)\n",
        "        conv4_1_3x3 = self.conv4_1_3x3(conv4_1_1x1_reduce_bnxx)\n",
        "        conv4_1_3x3_bn = self.conv4_1_3x3_bn(conv4_1_3x3)\n",
        "        conv4_1_3x3_bnxx = self.conv4_1_3x3_relu(conv4_1_3x3_bn)\n",
        "        conv4_1_1x1_increase = self.conv4_1_1x1_increase(conv4_1_3x3_bnxx)\n",
        "        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)\n",
        "        conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)\n",
        "        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)\n",
        "        conv4_1 = torch.add(conv4_1_1x1_proj_bn, 1, conv4_1_1x1_increase_bn)\n",
        "        conv4_1x = self.conv4_1_relu(conv4_1)\n",
        "        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(conv4_1x)\n",
        "        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)\n",
        "        conv4_2_1x1_reduce_bnxx = self.conv4_2_1x1_reduce_relu(conv4_2_1x1_reduce_bn)\n",
        "        conv4_2_3x3 = self.conv4_2_3x3(conv4_2_1x1_reduce_bnxx)\n",
        "        conv4_2_3x3_bn = self.conv4_2_3x3_bn(conv4_2_3x3)\n",
        "        conv4_2_3x3_bnxx = self.conv4_2_3x3_relu(conv4_2_3x3_bn)\n",
        "        conv4_2_1x1_increase = self.conv4_2_1x1_increase(conv4_2_3x3_bnxx)\n",
        "        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)\n",
        "        conv4_2 = torch.add(conv4_1x, 1, conv4_2_1x1_increase_bn)\n",
        "        conv4_2x = self.conv4_2_relu(conv4_2)\n",
        "        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(conv4_2x)\n",
        "        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)\n",
        "        conv4_3_1x1_reduce_bnxx = self.conv4_3_1x1_reduce_relu(conv4_3_1x1_reduce_bn)\n",
        "        conv4_3_3x3 = self.conv4_3_3x3(conv4_3_1x1_reduce_bnxx)\n",
        "        conv4_3_3x3_bn = self.conv4_3_3x3_bn(conv4_3_3x3)\n",
        "        conv4_3_3x3_bnxx = self.conv4_3_3x3_relu(conv4_3_3x3_bn)\n",
        "        conv4_3_1x1_increase = self.conv4_3_1x1_increase(conv4_3_3x3_bnxx)\n",
        "        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)\n",
        "        conv4_3 = torch.add(conv4_2x, 1, conv4_3_1x1_increase_bn)\n",
        "        conv4_3x = self.conv4_3_relu(conv4_3)\n",
        "        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(conv4_3x)\n",
        "        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)\n",
        "        conv4_4_1x1_reduce_bnxx = self.conv4_4_1x1_reduce_relu(conv4_4_1x1_reduce_bn)\n",
        "        conv4_4_3x3 = self.conv4_4_3x3(conv4_4_1x1_reduce_bnxx)\n",
        "        conv4_4_3x3_bn = self.conv4_4_3x3_bn(conv4_4_3x3)\n",
        "        conv4_4_3x3_bnxx = self.conv4_4_3x3_relu(conv4_4_3x3_bn)\n",
        "        conv4_4_1x1_increase = self.conv4_4_1x1_increase(conv4_4_3x3_bnxx)\n",
        "        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)\n",
        "        conv4_4 = torch.add(conv4_3x, 1, conv4_4_1x1_increase_bn)\n",
        "        conv4_4x = self.conv4_4_relu(conv4_4)\n",
        "        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(conv4_4x)\n",
        "        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)\n",
        "        conv4_5_1x1_reduce_bnxx = self.conv4_5_1x1_reduce_relu(conv4_5_1x1_reduce_bn)\n",
        "        conv4_5_3x3 = self.conv4_5_3x3(conv4_5_1x1_reduce_bnxx)\n",
        "        conv4_5_3x3_bn = self.conv4_5_3x3_bn(conv4_5_3x3)\n",
        "        conv4_5_3x3_bnxx = self.conv4_5_3x3_relu(conv4_5_3x3_bn)\n",
        "        conv4_5_1x1_increase = self.conv4_5_1x1_increase(conv4_5_3x3_bnxx)\n",
        "        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)\n",
        "        conv4_5 = torch.add(conv4_4x, 1, conv4_5_1x1_increase_bn)\n",
        "        conv4_5x = self.conv4_5_relu(conv4_5)\n",
        "        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(conv4_5x)\n",
        "        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)\n",
        "        conv4_6_1x1_reduce_bnxx = self.conv4_6_1x1_reduce_relu(conv4_6_1x1_reduce_bn)\n",
        "        conv4_6_3x3 = self.conv4_6_3x3(conv4_6_1x1_reduce_bnxx)\n",
        "        conv4_6_3x3_bn = self.conv4_6_3x3_bn(conv4_6_3x3)\n",
        "        conv4_6_3x3_bnxx = self.conv4_6_3x3_relu(conv4_6_3x3_bn)\n",
        "        conv4_6_1x1_increase = self.conv4_6_1x1_increase(conv4_6_3x3_bnxx)\n",
        "        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)\n",
        "        conv4_6 = torch.add(conv4_5x, 1, conv4_6_1x1_increase_bn)\n",
        "        conv4_6x = self.conv4_6_relu(conv4_6)\n",
        "        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(conv4_6x)\n",
        "        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)\n",
        "        conv5_1_1x1_reduce_bnxx = self.conv5_1_1x1_reduce_relu(conv5_1_1x1_reduce_bn)\n",
        "        conv5_1_3x3 = self.conv5_1_3x3(conv5_1_1x1_reduce_bnxx)\n",
        "        conv5_1_3x3_bn = self.conv5_1_3x3_bn(conv5_1_3x3)\n",
        "        conv5_1_3x3_bnxx = self.conv5_1_3x3_relu(conv5_1_3x3_bn)\n",
        "        conv5_1_1x1_increase = self.conv5_1_1x1_increase(conv5_1_3x3_bnxx)\n",
        "        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)\n",
        "        conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)\n",
        "        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)\n",
        "        conv5_1 = torch.add(conv5_1_1x1_proj_bn, 1, conv5_1_1x1_increase_bn)\n",
        "        conv5_1x = self.conv5_1_relu(conv5_1)\n",
        "        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(conv5_1x)\n",
        "        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)\n",
        "        conv5_2_1x1_reduce_bnxx = self.conv5_2_1x1_reduce_relu(conv5_2_1x1_reduce_bn)\n",
        "        conv5_2_3x3 = self.conv5_2_3x3(conv5_2_1x1_reduce_bnxx)\n",
        "        conv5_2_3x3_bn = self.conv5_2_3x3_bn(conv5_2_3x3)\n",
        "        conv5_2_3x3_bnxx = self.conv5_2_3x3_relu(conv5_2_3x3_bn)\n",
        "        conv5_2_1x1_increase = self.conv5_2_1x1_increase(conv5_2_3x3_bnxx)\n",
        "        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)\n",
        "        conv5_2 = torch.add(conv5_1x, 1, conv5_2_1x1_increase_bn)\n",
        "        conv5_2x = self.conv5_2_relu(conv5_2)\n",
        "        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(conv5_2x)\n",
        "        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)\n",
        "        conv5_3_1x1_reduce_bnxx = self.conv5_3_1x1_reduce_relu(conv5_3_1x1_reduce_bn)\n",
        "        conv5_3_3x3 = self.conv5_3_3x3(conv5_3_1x1_reduce_bnxx)\n",
        "        conv5_3_3x3_bn = self.conv5_3_3x3_bn(conv5_3_3x3)\n",
        "        conv5_3_3x3_bnxx = self.conv5_3_3x3_relu(conv5_3_3x3_bn)\n",
        "        conv5_3_1x1_increase = self.conv5_3_1x1_increase(conv5_3_3x3_bnxx)\n",
        "        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)\n",
        "        conv5_3 = torch.add(conv5_2x, 1, conv5_3_1x1_increase_bn)\n",
        "        conv5_3x = self.conv5_3_relu(conv5_3)\n",
        "        pool5_7x7_s1 = self.pool5_7x7_s1(conv5_3x)\n",
        "        classifier_preflatten = self.classifier(pool5_7x7_s1)\n",
        "        classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)\n",
        "        #return classifier, pool5_7x7_s1 　出力を変更しておかないと次元が合わないと言われる\n",
        "        return classifier\n",
        "\n",
        "def resnet50_ft_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Resnet50_ft_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7sJV06qPNsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process(data_dir):\n",
        "    # 入力画像の前処理をするクラス\n",
        "    # 訓練時と推論時で処理が異なる\n",
        "\n",
        "    \"\"\"\n",
        "        画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "        画像のサイズをリサイズし、色を標準化する。\n",
        "        訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        resize : int\n",
        "            リサイズ先の画像の大きさ。\n",
        "        mean : (R, G, B)\n",
        "            各色チャネルの平均値。\n",
        "        std : (R, G, B)\n",
        "            各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    data_transforms = {\n",
        "        'train': transforms.Compose([\n",
        "            transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ]),\n",
        "    }\n",
        "\n",
        "    data_dir = data_dir\n",
        "    n_samples = len(data_dir)\n",
        "\n",
        "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                              data_transforms[x])\n",
        "                      for x in ['train', 'val']}\n",
        "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                                shuffle=True, num_workers=4)\n",
        "                  for x in ['train', 'val']}\n",
        "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "    class_names = image_datasets['train'].classes\n",
        "\n",
        "\n",
        "    print(class_names)\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_train:\"+str(len(os.listdir(path=data_dir + '/train/'+class_names[k]))))\n",
        "        k+=1\n",
        "    k=0\n",
        "    for i in class_names:\n",
        "        print(class_names[k]+\"_val:\"+str(len(os.listdir(path=data_dir + '/val/'+class_names[k]))))\n",
        "        k+=1\n",
        "\n",
        "    print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "    print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "    \n",
        "    return image_datasets, dataloaders, dataset_sizes, class_names, device\n",
        "\n",
        "\n",
        "#少数の画像を可視化\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "def getBatch(dataloaders):    \n",
        "    # Get a batch of training data\n",
        "    inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "    # Make a grid from batch\n",
        "    out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "    #imshow(out, title=[class_names[x] for x in classes])\n",
        "    return(inputs, classes)\n",
        "\n",
        "#Defining early stopping class\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "#Train models\n",
        "def train_model(model, criterion, optimizer, patience, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # to track the training loss as the model trains\n",
        "    train_loss = []\n",
        "    # to track the validation loss as the model trains\n",
        "    valid_loss = []\n",
        "\n",
        "\n",
        "    # initialize the early_stopping object\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase] \n",
        "            \n",
        "            # record train_loss and valid_loss\n",
        "            if phase == 'train':\n",
        "                train_loss.append(epoch_loss)\n",
        "            if phase == 'val':\n",
        "                valid_loss.append(epoch_loss)\n",
        "            #print(train_loss)\n",
        "            #print(valid_loss)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      \n",
        "      # early_stopping needs the validation loss to check if it has decresed, \n",
        "      # and if it has, it will make a checkpoint of the current model\n",
        "        if phase == 'val':    \n",
        "            early_stopping(epoch_loss, model)\n",
        "                \n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "        print()\n",
        "\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_loss, valid_loss\n",
        "\n",
        "\n",
        "#Visualize model\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "\n",
        "def training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50):\n",
        "    model_ft, train_loss, valid_loss = train_model(model_ft, criterion, optimizer_ft,  patience=patience, num_epochs=num_epochs)\n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "    \"\"\"\n",
        "    #probalilityを計算する\n",
        "    pred_prob = torch.topk(nn.Softmax(dim=1)(output), 1)[0]\n",
        "    pred_class = torch.topk(nn.Softmax(dim=1)(output), 1)[1]\n",
        "    if pred_class == 1:\n",
        "        pred_prob = pred_prob\n",
        "    elif pred_class == 0:\n",
        "        pred_prob = 1- pred_prob\n",
        "    return(model_pred, pred_prob)  #class_nameの番号で出力される\n",
        "    \"\"\"\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\"\"\"\n",
        "・True positive (TN)\n",
        "・False positive (FP)\n",
        "・True negative (TN)\n",
        "・False negative (FN)\n",
        "Accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "Precision = TP/(FP + TP) ※positive predictive value\n",
        "Recall = TP/(TP + FN)　※sensitivity\n",
        "Specificity = TN/(FP + TN)\n",
        "F_value = (2RecallPrecision)/(Recall+Precision)\n",
        "\"\"\"\n",
        "\n",
        "def evaluation(model_ft, testset_dir):\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #testデータセット内のファイル名を取得\n",
        "    image_path = glob.glob(testset_dir + \"/*/*\")\n",
        "    #random.shuffle(image_path)  #表示順をランダムにする\n",
        "    print('number of images: ' +str(len(image_path)))\n",
        "\n",
        "\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1     \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    return TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob\n",
        "\n",
        "\n",
        "def make_csv(roc_label_list):\n",
        "    #csvのdata tableを作成\n",
        "    pd.set_option('display.max_rows', 800)  # 省略なしで表示\n",
        "    #columns1 = [\"EfficientNet_32\", \"EfficientNet_64\", \"EfficientNet_128\", \"EfficientNet_256\", \"EfficientNet_512\", \"EfficientNet_558\"]\n",
        "    roc_label_list.extend([\"avg\", \"std\"])\n",
        "    index1 = [\"TP\",\"TN\",\"FP\",\"FN\",\"Accuracy\",\"Positive predictive value\",\"sensitity\",\"specificity\",\"F-value\",\"roc_auc\"]\n",
        "    df = pd.DataFrame(index=index1, columns=roc_label_list)\n",
        "    return df\n",
        "\n",
        "def write_csv(df, col, TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc):\n",
        "    df.iloc[0:10, col] = TP, TN, FP, FN, accuracy, precision, recall, specificity, f_value,roc_auc \n",
        "    #print(df)\n",
        "\n",
        "    # CSVとして出力\n",
        "    #df2.to_csv(\"/content/drive/My Drive/Grav_bootcamp/Posttrain_model_eval_result.csv\",encoding=\"shift_jis\")\n",
        "    return df\n",
        "\n",
        "def Draw_roc_curve(label_list_list, model_pred_prob_list, sample_num_list, num_curves):\n",
        "\n",
        "#グラフの外形を作成\n",
        "    fig = plt.figure(figsize=(8.0, 6.0))\n",
        "    lw = 2\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    ycolor = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"w\"]      # 各プロットの色\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    k=0\n",
        "    for j in range(num_curves):\n",
        "        y_score = []\n",
        "        y_true = []\n",
        "\n",
        "        for i in label_list_list[k]:\n",
        "            if i == 'cont':\n",
        "                  y_true.append(0)\n",
        "            elif i == 'grav':\n",
        "                  y_true.append(1)\n",
        "            \n",
        "        #それぞれの画像における陽性の確率についてリストを作成\n",
        "        y_score = model_pred_prob_list[k]\n",
        "\n",
        "        fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=ycolor[k],lw=lw, label= str(roc_label_list[k])+':ROC curve (area = %0.2f)' % roc_auc)\n",
        "            \n",
        "        k+=1\n",
        "\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "def calculate_auc(label_list, model_pred_prob):\n",
        "    y_true, y_score = [], []\n",
        "    for i in label_list:\n",
        "        if i == 'cont':\n",
        "              y_true.append(0)\n",
        "        elif i == 'grav':\n",
        "              y_true.append(1)\n",
        "            \n",
        "    #それぞれの画像における陽性の確率についてリストを作成\n",
        "    y_score = model_pred_prob\n",
        "    fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(\"roc_auc: \" +str(roc_auc))\n",
        "    return(roc_auc, y_true, y_score)\n",
        "\n",
        "def calcurate_ave_std(df, fold):\n",
        "    for i in range(5):\n",
        "        df.iloc[i,fold] = df[i,0:5].mean \n",
        "\n",
        "def convnet():\n",
        "    #Pretrained dataと結びつける\n",
        "    model_ft = Resnet50_ft_dag()\n",
        "    model_ft.load_state_dict(torch.load('/content/drive/My Drive/Grav_bootcamp/resnet50_ft_dag.pth'))\n",
        "    #最終結合層のリセットと付け替え(全結合層を2つに)\n",
        "    #model_ft.classifier = nn.Conv2d(2048, len(class_names), kernel_size=[1,1], stride=(1,1), bias = False)\n",
        "    model_ft.classifier = nn.Linear(2048, 2)\n",
        "    model_ft.classifier = nn.Sequential(*([Flatten()] + list(model_ft.children())[-1:])) #Flattenを挿入\n",
        "\n",
        "\n",
        "    #GPU使用\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    #損失関数を定義\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    #https://blog.knjcode.com/adabound-memo/\n",
        "    #https://pypi.org/project/torch-optimizer/\n",
        "    optimizer_ft = optim.AdaBound(\n",
        "        model_ft.parameters(),\n",
        "        lr= 1e-3,\n",
        "        betas= (0.9, 0.999),\n",
        "        final_lr = 0.1,\n",
        "        gamma=1e-3,\n",
        "        eps= 1e-8,\n",
        "        weight_decay=0,\n",
        "        amsbound=False,\n",
        "    )\n",
        "    return (model_ft, criterion, optimizer_ft)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGfUwQXv6Jc",
        "colab_type": "text"
      },
      "source": [
        "#**まとめて解析**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM2VMXltwBs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a395290-1068-44cf-93d4-21ca68f9b2af"
      },
      "source": [
        "# 出力名を記入\n",
        "out_name = \"ResNet50_VGGFace2_558\"\n",
        "\n",
        "#create data_dir_list\n",
        "data_dir = '/content/drive/My Drive/crossvalidation/558'\n",
        "fold = len(os.listdir(data_dir))\n",
        "print(str(fold)+'-fold cross validation')\n",
        "\n",
        "\n",
        "data_dir_list = [0]*fold\n",
        "\n",
        "for i in range(fold):\n",
        "    data_dir_list[i] = data_dir + '/' + str(i)\n",
        "    print(data_dir_list[i])\n",
        "\n",
        "#create roc_label_list\n",
        "roc_label_list = [0]*fold\n",
        "roc_label_list = list(range(fold))\n",
        "#print(roc_label_list)\n",
        "\n",
        "\n",
        "\n",
        "df = make_csv(roc_label_list)\n",
        "\n",
        "label_list_list, model_pred_prob_list, Y_TRUE, Y_SCORE = [],[],[],[]\n",
        "\n",
        "#print(data_dir_list)\n",
        "#print(roc_label_list)\n",
        "\n",
        "for i, t in enumerate(zip(data_dir_list, roc_label_list)):\n",
        "\n",
        "    image_datasets, dataloaders, dataset_sizes, class_names, device = pre_process(t[0]) #path\n",
        "    inputs, classes = getBatch(dataloaders)\n",
        "    model_ft, criterion, optimizer_ft = convnet()\n",
        "    training(model_ft, criterion, optimizer_ft,  patience=15, num_epochs=50)  \n",
        "    TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, label_list, model_pred_prob = evaluation(model_ft, '/content/drive/My Drive/Grav_bootcamp/Posttrain_250px')\n",
        "    roc_auc, y_true, y_score = calculate_auc(label_list, model_pred_prob)\n",
        "    Y_TRUE.append(y_true)\n",
        "    Y_SCORE.append(y_score)\n",
        "    df = write_csv(df, i,TP,TN,FP,FN, accuracy, precision, recall, specificity, f_value, roc_auc) #numberをcsvの行として指定\n",
        "\n",
        "    label_list_list.append(label_list)\n",
        "    model_pred_prob_list.append(model_pred_prob)\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "\n",
        "#Draw ROC curve\n",
        "fig = Draw_roc_curve(label_list_list, model_pred_prob_list, roc_label_list, len(label_list_list))\n",
        "\n",
        "pd.set_option('display.max_columns', 100)\n",
        "#それぞれの項目の平均を計算しcsvに追記する\n",
        "df.iloc[0:4,fold], df.iloc[9,fold]   = df.mean(axis=1)[0:4], df.mean(axis=1)[9] \n",
        "df.iloc[0:10,fold+1] = df.std(axis=1)[0:10]\n",
        "TP,TN,FP,FN = df.mean(axis=1)[0:4]\n",
        "df.iloc[4:9,fold] = calculateAccuracy (TP, TN, FP, FN)\n",
        "print(df)\n",
        "\n",
        "# CSVとして出力\n",
        "df.to_csv(\"/content/drive/My Drive/Grav_bootcamp/crossvalidation_csv/crossvalidation_\" + out_name + \".csv\",encoding=\"shift_jis\")\n",
        "\n",
        "#ROC_curveを保存\n",
        "fig.savefig(\"/content/drive/My Drive/Grav_bootcamp/crossvalidation_ROCfigure/crossvalidation_\" + out_name +\".png\")\n",
        "\n",
        "#Save ROC data\n",
        "with open(\"/content/drive/My Drive/Grav_bootcamp/crossvalidation_ROCdata/ROCdata_\"+out_name+\".csv\", 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    for i, t in enumerate(zip(Y_TRUE, Y_SCORE)):\n",
        "        writer.writerow([t[0],t[1]])\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5-fold cross validation\n",
            "/content/drive/My Drive/crossvalidation/558/0\n",
            "/content/drive/My Drive/crossvalidation/558/1\n",
            "/content/drive/My Drive/crossvalidation/558/2\n",
            "/content/drive/My Drive/crossvalidation/558/3\n",
            "/content/drive/My Drive/crossvalidation/558/4\n",
            "['cont', 'grav']\n",
            "cont_train:223\n",
            "grav_train:223\n",
            "cont_val:56\n",
            "grav_val:56\n",
            "training data set_total：446\n",
            "validating data set_total：112\n",
            "Epoch 0/49\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:186: UserWarning: This overload of add is deprecated:\n",
            "\tadd(Tensor input, Number alpha, Tensor other, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd(Tensor input, Tensor other, *, Number alpha, Tensor out) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train Loss: 1.3617 Acc: 0.5583\n",
            "val Loss: 0.9592 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 0.959235).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.6267 Acc: 0.6816\n",
            "val Loss: 0.8021 Acc: 0.5357\n",
            "Validation loss decreased (0.959235 --> 0.802081).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.6033 Acc: 0.7332\n",
            "val Loss: 0.8451 Acc: 0.5714\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.6067 Acc: 0.7399\n",
            "val Loss: 0.9017 Acc: 0.6071\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.4973 Acc: 0.7735\n",
            "val Loss: 0.6257 Acc: 0.6429\n",
            "Validation loss decreased (0.802081 --> 0.625743).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.4668 Acc: 0.7870\n",
            "val Loss: 3.0396 Acc: 0.4911\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.4234 Acc: 0.8049\n",
            "val Loss: 1.4513 Acc: 0.5179\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.4146 Acc: 0.8094\n",
            "val Loss: 0.9767 Acc: 0.6339\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.4116 Acc: 0.8117\n",
            "val Loss: 1.3597 Acc: 0.5625\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.4327 Acc: 0.8161\n",
            "val Loss: 0.5957 Acc: 0.6964\n",
            "Validation loss decreased (0.625743 --> 0.595658).  Saving model ...\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.3790 Acc: 0.8363\n",
            "val Loss: 0.7419 Acc: 0.6071\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.4235 Acc: 0.8206\n",
            "val Loss: 1.2247 Acc: 0.5893\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.4426 Acc: 0.8072\n",
            "val Loss: 0.8530 Acc: 0.6518\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.3705 Acc: 0.8408\n",
            "val Loss: 3.8002 Acc: 0.5268\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.3843 Acc: 0.8363\n",
            "val Loss: 1.5719 Acc: 0.5179\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.3560 Acc: 0.8430\n",
            "val Loss: 0.6898 Acc: 0.6875\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.3722 Acc: 0.8475\n",
            "val Loss: 0.6849 Acc: 0.6696\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.3443 Acc: 0.8498\n",
            "val Loss: 1.0033 Acc: 0.6518\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.3264 Acc: 0.8632\n",
            "val Loss: 1.2315 Acc: 0.5357\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.3651 Acc: 0.8587\n",
            "val Loss: 0.6423 Acc: 0.7500\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.4247 Acc: 0.8094\n",
            "val Loss: 5.0583 Acc: 0.5000\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.3706 Acc: 0.8161\n",
            "val Loss: 1.3333 Acc: 0.6071\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.3486 Acc: 0.8498\n",
            "val Loss: 0.5924 Acc: 0.7321\n",
            "Validation loss decreased (0.595658 --> 0.592444).  Saving model ...\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.3130 Acc: 0.8789\n",
            "val Loss: 1.0322 Acc: 0.6161\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.2992 Acc: 0.8632\n",
            "val Loss: 0.5785 Acc: 0.6786\n",
            "Validation loss decreased (0.592444 --> 0.578518).  Saving model ...\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.2808 Acc: 0.8744\n",
            "val Loss: 1.3480 Acc: 0.6071\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.2881 Acc: 0.8812\n",
            "val Loss: 0.7365 Acc: 0.7054\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.2134 Acc: 0.9215\n",
            "val Loss: 0.8348 Acc: 0.6339\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.2836 Acc: 0.8700\n",
            "val Loss: 2.3459 Acc: 0.6875\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.2978 Acc: 0.8655\n",
            "val Loss: 0.6907 Acc: 0.6875\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.2390 Acc: 0.9036\n",
            "val Loss: 1.5209 Acc: 0.6339\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.3102 Acc: 0.8498\n",
            "val Loss: 1.1100 Acc: 0.6161\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.2481 Acc: 0.8857\n",
            "val Loss: 1.0659 Acc: 0.6875\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.2534 Acc: 0.8924\n",
            "val Loss: 0.6315 Acc: 0.7232\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.2009 Acc: 0.9260\n",
            "val Loss: 1.8275 Acc: 0.5893\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.2844 Acc: 0.8722\n",
            "val Loss: 2.3254 Acc: 0.5536\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.2728 Acc: 0.8767\n",
            "val Loss: 1.1200 Acc: 0.5536\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.2453 Acc: 0.8901\n",
            "val Loss: 0.7294 Acc: 0.6875\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.2574 Acc: 0.8879\n",
            "val Loss: 1.8678 Acc: 0.5536\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "train Loss: 0.3762 Acc: 0.8386\n",
            "val Loss: 1.6834 Acc: 0.5536\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 6m 7s\n",
            "Best val Acc: 0.750000\n",
            "number of images: 108\n",
            "32 22 50 4\n",
            "Accuracy: 0.7592592592592593\n",
            "Precision (positive predictive value): 0.8888888888888888\n",
            "Recall (sensitivity): 0.5925925925925926\n",
            "Specificity: 0.9259259259259259\n",
            "F_value: 0.711111111111111\n",
            "roc_auc: 0.8165294924554184\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:223\n",
            "grav_train:223\n",
            "cont_val:56\n",
            "grav_val:56\n",
            "training data set_total：446\n",
            "validating data set_total：112\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.4844 Acc: 0.5426\n",
            "val Loss: 6.8209 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 6.820935).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.7037 Acc: 0.6413\n",
            "val Loss: 1.2831 Acc: 0.4732\n",
            "Validation loss decreased (6.820935 --> 1.283056).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.5639 Acc: 0.6996\n",
            "val Loss: 0.6135 Acc: 0.6696\n",
            "Validation loss decreased (1.283056 --> 0.613518).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.5697 Acc: 0.7466\n",
            "val Loss: 3.4181 Acc: 0.5804\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.6106 Acc: 0.6839\n",
            "val Loss: 0.6051 Acc: 0.7589\n",
            "Validation loss decreased (0.613518 --> 0.605097).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.5593 Acc: 0.7332\n",
            "val Loss: 0.9757 Acc: 0.5268\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.5391 Acc: 0.7354\n",
            "val Loss: 0.6296 Acc: 0.6875\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.4611 Acc: 0.7892\n",
            "val Loss: 2.3095 Acc: 0.5000\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.5248 Acc: 0.7287\n",
            "val Loss: 0.7157 Acc: 0.5804\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.4328 Acc: 0.8072\n",
            "val Loss: 0.5505 Acc: 0.7589\n",
            "Validation loss decreased (0.605097 --> 0.550475).  Saving model ...\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.4079 Acc: 0.8161\n",
            "val Loss: 1.9142 Acc: 0.5089\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.4002 Acc: 0.8117\n",
            "val Loss: 0.5534 Acc: 0.7500\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.4155 Acc: 0.8296\n",
            "val Loss: 0.6349 Acc: 0.6964\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.4390 Acc: 0.7937\n",
            "val Loss: 0.5443 Acc: 0.7500\n",
            "Validation loss decreased (0.550475 --> 0.544319).  Saving model ...\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.4559 Acc: 0.7848\n",
            "val Loss: 4.4299 Acc: 0.5357\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.5304 Acc: 0.7377\n",
            "val Loss: 0.4657 Acc: 0.7679\n",
            "Validation loss decreased (0.544319 --> 0.465695).  Saving model ...\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.4166 Acc: 0.7982\n",
            "val Loss: 0.6697 Acc: 0.6607\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.3648 Acc: 0.8206\n",
            "val Loss: 0.6524 Acc: 0.6250\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.3564 Acc: 0.8408\n",
            "val Loss: 0.5549 Acc: 0.7589\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.3255 Acc: 0.8565\n",
            "val Loss: 1.2216 Acc: 0.6696\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.3729 Acc: 0.8318\n",
            "val Loss: 0.7483 Acc: 0.6696\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.3897 Acc: 0.8139\n",
            "val Loss: 0.4896 Acc: 0.7411\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.3240 Acc: 0.8543\n",
            "val Loss: 0.4307 Acc: 0.7768\n",
            "Validation loss decreased (0.465695 --> 0.430661).  Saving model ...\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.2938 Acc: 0.8812\n",
            "val Loss: 0.4682 Acc: 0.7768\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.2427 Acc: 0.8901\n",
            "val Loss: 0.4819 Acc: 0.7589\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.2924 Acc: 0.8789\n",
            "val Loss: 0.4591 Acc: 0.8571\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.3037 Acc: 0.8767\n",
            "val Loss: 0.5658 Acc: 0.7768\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.3286 Acc: 0.8677\n",
            "val Loss: 0.5420 Acc: 0.7500\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.3289 Acc: 0.8632\n",
            "val Loss: 0.6084 Acc: 0.7411\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.3176 Acc: 0.8655\n",
            "val Loss: 0.7087 Acc: 0.7232\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.3747 Acc: 0.8408\n",
            "val Loss: 0.5543 Acc: 0.7232\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.3547 Acc: 0.8386\n",
            "val Loss: 0.5086 Acc: 0.7411\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.3094 Acc: 0.8789\n",
            "val Loss: 0.5026 Acc: 0.7679\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.2867 Acc: 0.8969\n",
            "val Loss: 0.5008 Acc: 0.7857\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.2884 Acc: 0.8655\n",
            "val Loss: 0.5255 Acc: 0.7589\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.2968 Acc: 0.8700\n",
            "val Loss: 0.5165 Acc: 0.7143\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.2651 Acc: 0.9081\n",
            "val Loss: 0.8798 Acc: 0.6518\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.2215 Acc: 0.9036\n",
            "val Loss: 0.6003 Acc: 0.7411\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 5m 25s\n",
            "Best val Acc: 0.857143\n",
            "number of images: 108\n",
            "42 12 49 5\n",
            "Accuracy: 0.8425925925925926\n",
            "Precision (positive predictive value): 0.8936170212765957\n",
            "Recall (sensitivity): 0.7777777777777778\n",
            "Specificity: 0.9074074074074074\n",
            "F_value: 0.8316831683168316\n",
            "roc_auc: 0.9029492455418381\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:223\n",
            "grav_train:223\n",
            "cont_val:56\n",
            "grav_val:56\n",
            "training data set_total：446\n",
            "validating data set_total：112\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.3944 Acc: 0.5673\n",
            "val Loss: 1.0652 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 1.065236).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.5595 Acc: 0.7332\n",
            "val Loss: 0.6318 Acc: 0.6339\n",
            "Validation loss decreased (1.065236 --> 0.631772).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.5996 Acc: 0.7489\n",
            "val Loss: 0.8265 Acc: 0.6607\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.5951 Acc: 0.7354\n",
            "val Loss: 0.6941 Acc: 0.7232\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.5073 Acc: 0.7713\n",
            "val Loss: 0.9105 Acc: 0.5982\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.4725 Acc: 0.7915\n",
            "val Loss: 0.8469 Acc: 0.5893\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.4217 Acc: 0.8004\n",
            "val Loss: 0.6401 Acc: 0.5893\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.3848 Acc: 0.8296\n",
            "val Loss: 0.5126 Acc: 0.7411\n",
            "Validation loss decreased (0.631772 --> 0.512561).  Saving model ...\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.4520 Acc: 0.7803\n",
            "val Loss: 0.6156 Acc: 0.7411\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.4424 Acc: 0.8072\n",
            "val Loss: 0.6599 Acc: 0.6250\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.4472 Acc: 0.8139\n",
            "val Loss: 0.5566 Acc: 0.6964\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.3901 Acc: 0.8004\n",
            "val Loss: 2.6102 Acc: 0.5357\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.4305 Acc: 0.8027\n",
            "val Loss: 2.2339 Acc: 0.6161\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.3932 Acc: 0.8318\n",
            "val Loss: 0.5791 Acc: 0.6875\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.3945 Acc: 0.8296\n",
            "val Loss: 0.6922 Acc: 0.6071\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.3996 Acc: 0.8004\n",
            "val Loss: 0.6088 Acc: 0.6696\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.3222 Acc: 0.8587\n",
            "val Loss: 1.0683 Acc: 0.6071\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.3580 Acc: 0.8408\n",
            "val Loss: 0.7858 Acc: 0.5714\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.4302 Acc: 0.8229\n",
            "val Loss: 0.6507 Acc: 0.6786\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.3733 Acc: 0.8520\n",
            "val Loss: 0.5205 Acc: 0.7589\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.2880 Acc: 0.8700\n",
            "val Loss: 0.7471 Acc: 0.6875\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.3193 Acc: 0.8677\n",
            "val Loss: 0.6463 Acc: 0.6607\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.2824 Acc: 0.8789\n",
            "val Loss: 0.5666 Acc: 0.7232\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 3m 17s\n",
            "Best val Acc: 0.758929\n",
            "number of images: 108\n",
            "42 12 52 2\n",
            "Accuracy: 0.8703703703703703\n",
            "Precision (positive predictive value): 0.9545454545454546\n",
            "Recall (sensitivity): 0.7777777777777778\n",
            "Specificity: 0.9629629629629629\n",
            "F_value: 0.8571428571428572\n",
            "roc_auc: 0.9231824417009602\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:223\n",
            "grav_train:223\n",
            "cont_val:56\n",
            "grav_val:56\n",
            "training data set_total：446\n",
            "validating data set_total：112\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.7023 Acc: 0.5000\n",
            "val Loss: 3.0740 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 3.074025).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 0.7215 Acc: 0.6143\n",
            "val Loss: 1.4678 Acc: 0.5000\n",
            "Validation loss decreased (3.074025 --> 1.467838).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.6727 Acc: 0.6143\n",
            "val Loss: 0.6681 Acc: 0.6518\n",
            "Validation loss decreased (1.467838 --> 0.668085).  Saving model ...\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.5712 Acc: 0.7242\n",
            "val Loss: 0.8678 Acc: 0.6339\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.5723 Acc: 0.7309\n",
            "val Loss: 0.5863 Acc: 0.6696\n",
            "Validation loss decreased (0.668085 --> 0.586254).  Saving model ...\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.5105 Acc: 0.7668\n",
            "val Loss: 5.7232 Acc: 0.5000\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.6210 Acc: 0.6928\n",
            "val Loss: 0.8681 Acc: 0.6429\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.4782 Acc: 0.7623\n",
            "val Loss: 0.4541 Acc: 0.7857\n",
            "Validation loss decreased (0.586254 --> 0.454106).  Saving model ...\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.3895 Acc: 0.8251\n",
            "val Loss: 1.1314 Acc: 0.5893\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.5673 Acc: 0.6951\n",
            "val Loss: 0.6437 Acc: 0.7232\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.4811 Acc: 0.7534\n",
            "val Loss: 0.4972 Acc: 0.8125\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.4763 Acc: 0.8004\n",
            "val Loss: 0.6121 Acc: 0.7232\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.4554 Acc: 0.8072\n",
            "val Loss: 0.4379 Acc: 0.8036\n",
            "Validation loss decreased (0.454106 --> 0.437858).  Saving model ...\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.4419 Acc: 0.7915\n",
            "val Loss: 0.5820 Acc: 0.6429\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.3998 Acc: 0.8251\n",
            "val Loss: 1.1048 Acc: 0.6429\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.3819 Acc: 0.8475\n",
            "val Loss: 0.5339 Acc: 0.7143\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.3587 Acc: 0.8565\n",
            "val Loss: 1.0724 Acc: 0.6161\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.3101 Acc: 0.8834\n",
            "val Loss: 0.7463 Acc: 0.7500\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.3322 Acc: 0.8520\n",
            "val Loss: 0.6786 Acc: 0.7500\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.3436 Acc: 0.8543\n",
            "val Loss: 0.4974 Acc: 0.7411\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.2733 Acc: 0.8924\n",
            "val Loss: 0.9166 Acc: 0.6964\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.3822 Acc: 0.8453\n",
            "val Loss: 0.6290 Acc: 0.7143\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.3031 Acc: 0.8744\n",
            "val Loss: 0.6882 Acc: 0.7500\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "train Loss: 0.3333 Acc: 0.8453\n",
            "val Loss: 0.3775 Acc: 0.8304\n",
            "Validation loss decreased (0.437858 --> 0.377491).  Saving model ...\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "train Loss: 0.2161 Acc: 0.9148\n",
            "val Loss: 0.4900 Acc: 0.8125\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "train Loss: 0.3170 Acc: 0.8744\n",
            "val Loss: 1.8195 Acc: 0.6250\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "train Loss: 0.3684 Acc: 0.8498\n",
            "val Loss: 0.8826 Acc: 0.5982\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "train Loss: 0.2625 Acc: 0.8812\n",
            "val Loss: 0.4708 Acc: 0.8036\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "train Loss: 0.2306 Acc: 0.9058\n",
            "val Loss: 0.5475 Acc: 0.7679\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "train Loss: 0.4385 Acc: 0.8274\n",
            "val Loss: 2.2266 Acc: 0.5000\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "train Loss: 0.4272 Acc: 0.8139\n",
            "val Loss: 0.6108 Acc: 0.7679\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "train Loss: 0.3380 Acc: 0.8543\n",
            "val Loss: 0.6842 Acc: 0.6875\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "train Loss: 0.3401 Acc: 0.8498\n",
            "val Loss: 0.9350 Acc: 0.6518\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "train Loss: 0.2707 Acc: 0.8812\n",
            "val Loss: 0.5448 Acc: 0.8036\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "train Loss: 0.2206 Acc: 0.9058\n",
            "val Loss: 0.7678 Acc: 0.6964\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "train Loss: 0.2978 Acc: 0.8857\n",
            "val Loss: 0.5622 Acc: 0.7946\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "train Loss: 0.1856 Acc: 0.9238\n",
            "val Loss: 0.7653 Acc: 0.7143\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "train Loss: 0.2510 Acc: 0.8991\n",
            "val Loss: 1.3038 Acc: 0.6696\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "train Loss: 0.3620 Acc: 0.8565\n",
            "val Loss: 0.5776 Acc: 0.7946\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 5m 28s\n",
            "Best val Acc: 0.830357\n",
            "number of images: 108\n",
            "37 17 49 5\n",
            "Accuracy: 0.7962962962962963\n",
            "Precision (positive predictive value): 0.8809523809523809\n",
            "Recall (sensitivity): 0.6851851851851852\n",
            "Specificity: 0.9074074074074074\n",
            "F_value: 0.7708333333333335\n",
            "roc_auc: 0.8950617283950618\n",
            "\n",
            "\n",
            "['cont', 'grav']\n",
            "cont_train:224\n",
            "grav_train:224\n",
            "cont_val:55\n",
            "grav_val:55\n",
            "training data set_total：448\n",
            "validating data set_total：110\n",
            "Epoch 0/49\n",
            "----------\n",
            "train Loss: 1.6398 Acc: 0.5134\n",
            "val Loss: 8.3148 Acc: 0.5000\n",
            "Validation loss decreased (inf --> 8.314809).  Saving model ...\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "train Loss: 1.0205 Acc: 0.4732\n",
            "val Loss: 0.6933 Acc: 0.4727\n",
            "Validation loss decreased (8.314809 --> 0.693254).  Saving model ...\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "train Loss: 0.7373 Acc: 0.5201\n",
            "val Loss: 1.2209 Acc: 0.4727\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "train Loss: 0.7873 Acc: 0.4888\n",
            "val Loss: 0.7107 Acc: 0.4455\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "train Loss: 0.7554 Acc: 0.5000\n",
            "val Loss: 0.7566 Acc: 0.5091\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "train Loss: 0.8631 Acc: 0.5067\n",
            "val Loss: 1.0999 Acc: 0.5182\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "train Loss: 0.7902 Acc: 0.4955\n",
            "val Loss: 1.3532 Acc: 0.4545\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "train Loss: 0.7070 Acc: 0.4866\n",
            "val Loss: 0.6826 Acc: 0.4727\n",
            "Validation loss decreased (0.693254 --> 0.682644).  Saving model ...\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "train Loss: 0.7618 Acc: 0.4933\n",
            "val Loss: 0.6897 Acc: 0.5818\n",
            "EarlyStopping counter: 1 out of 15\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "train Loss: 0.7284 Acc: 0.5000\n",
            "val Loss: 0.7116 Acc: 0.4909\n",
            "EarlyStopping counter: 2 out of 15\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "train Loss: 0.6981 Acc: 0.5022\n",
            "val Loss: 0.7320 Acc: 0.4818\n",
            "EarlyStopping counter: 3 out of 15\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "train Loss: 0.7016 Acc: 0.5067\n",
            "val Loss: 0.7042 Acc: 0.5000\n",
            "EarlyStopping counter: 4 out of 15\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "train Loss: 0.7000 Acc: 0.5045\n",
            "val Loss: 0.6936 Acc: 0.5000\n",
            "EarlyStopping counter: 5 out of 15\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "train Loss: 0.7084 Acc: 0.4710\n",
            "val Loss: 0.6902 Acc: 0.5091\n",
            "EarlyStopping counter: 6 out of 15\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "train Loss: 0.6997 Acc: 0.5067\n",
            "val Loss: 0.6861 Acc: 0.5000\n",
            "EarlyStopping counter: 7 out of 15\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "train Loss: 0.7025 Acc: 0.5156\n",
            "val Loss: 0.6928 Acc: 0.5000\n",
            "EarlyStopping counter: 8 out of 15\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "train Loss: 0.6880 Acc: 0.5335\n",
            "val Loss: 0.6834 Acc: 0.5182\n",
            "EarlyStopping counter: 9 out of 15\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "train Loss: 0.6894 Acc: 0.5536\n",
            "val Loss: 0.6938 Acc: 0.5000\n",
            "EarlyStopping counter: 10 out of 15\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "train Loss: 0.7021 Acc: 0.5179\n",
            "val Loss: 0.6917 Acc: 0.5182\n",
            "EarlyStopping counter: 11 out of 15\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "train Loss: 0.6889 Acc: 0.5156\n",
            "val Loss: 0.6975 Acc: 0.5000\n",
            "EarlyStopping counter: 12 out of 15\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "train Loss: 0.7005 Acc: 0.5022\n",
            "val Loss: 0.6873 Acc: 0.5091\n",
            "EarlyStopping counter: 13 out of 15\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "train Loss: 0.6929 Acc: 0.5089\n",
            "val Loss: 0.6895 Acc: 0.5182\n",
            "EarlyStopping counter: 14 out of 15\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "train Loss: 0.6936 Acc: 0.5223\n",
            "val Loss: 0.6914 Acc: 0.5000\n",
            "EarlyStopping counter: 15 out of 15\n",
            "Early stopping\n",
            "Training complete in 3m 18s\n",
            "Best val Acc: 0.581818\n",
            "number of images: 108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "46 8 11 43\n",
            "Accuracy: 0.5277777777777778\n",
            "Precision (positive predictive value): 0.5168539325842697\n",
            "Recall (sensitivity): 0.8518518518518519\n",
            "Specificity: 0.2037037037037037\n",
            "F_value: 0.6433566433566433\n",
            "roc_auc: 0.651920438957476\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                  0         1         2         3         4  \\\n",
            "TP                               32        42        42        37        46   \n",
            "TN                               50        49        52        49        11   \n",
            "FP                                4         5         2         5        43   \n",
            "FN                               22        12        12        17         8   \n",
            "Accuracy                   0.759259  0.842593   0.87037  0.796296  0.527778   \n",
            "Positive predictive value  0.888889  0.893617  0.954545  0.880952  0.516854   \n",
            "sensitity                  0.592593  0.777778  0.777778  0.685185  0.851852   \n",
            "specificity                0.925926  0.907407  0.962963  0.907407  0.203704   \n",
            "F-value                    0.711111  0.831683  0.857143  0.770833  0.643357   \n",
            "roc_auc                    0.816529  0.902949  0.923182  0.895062   0.65192   \n",
            "\n",
            "                                avg        std  \n",
            "TP                             39.8    4.83322  \n",
            "TN                             42.2    15.6384  \n",
            "FP                             11.8    15.6384  \n",
            "FN                             14.2    4.83322  \n",
            "Accuracy                   0.743852    0.13624  \n",
            "Positive predictive value  0.738122   0.175811  \n",
            "sensitity                   0.73017   0.100069  \n",
            "specificity                0.756701   0.323783  \n",
            "F-value                    0.734125  0.0875335  \n",
            "roc_auc                    0.837929  0.0998169  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPO-b2cAT0yF",
        "colab_type": "text"
      },
      "source": [
        "#**ネットワークの保存と読み込み**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMYyFQ6ATzyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ネットワークの保存\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c744XUtfT6xW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73a923de-e875-4e71-b4b5-1ae266557981"
      },
      "source": [
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Grav_bootcamp/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "torch.save(model_ft.state_dict(), PATH)\n",
        "model_ft.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}