{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled35.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNYZ/p/7za2/7I5Ii1ZB9na",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Assessment1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpC0Fk9lUn2",
        "colab_type": "text"
      },
      "source": [
        "#**Assessment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9B59fSXlT5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHO1RjNom0Pr",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-yVbbvSm3Ay",
        "colab_type": "code",
        "outputId": "9b74dd6c-4b8e-4e84-b6cd-98bad70705ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "#ModelNameをリストにする\n",
        "ModelName_list = []\n",
        "ModelName = ''\n",
        "model_pred_prob = []\n",
        "\n",
        "def checkModelName(ModelName, ModelName_list):\n",
        "    if ModelName in ModelName_list:\n",
        "        raise Exception(\"This model has been already loaded\")\n",
        "    else:\n",
        "        ModelName_list.append(ModelName)\n",
        "\n",
        "\n",
        "# 入力画像の前処理をするクラス\n",
        "# 訓練時と推論時で処理が異なる\n",
        "\n",
        "\"\"\"\n",
        "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "\"\"\"\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/My Drive/Deep_learning/gravcont_seed_1234'\n",
        "n_samples = len(data_dir)\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "print(class_names)\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_train:\"+str(len(os.listdir(path='/content/drive/My Drive/Deep_learning/gravcont_seed_1234/train/'+class_names[k]))))\n",
        "    k+=1\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_val:\"+str(len(os.listdir(path='/content/drive/My Drive/Deep_learning/gravcont_seed_1234/val/'+class_names[k]))))\n",
        "    k+=1\n",
        "\n",
        "print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "\n"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cont', 'grav']\n",
            "cont_train:256\n",
            "grav_train:252\n",
            "cont_val:65\n",
            "grav_val:63\n",
            "training data set_total：508\n",
            "validating data set_total：128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zLnknjNnqNU",
        "colab_type": "text"
      },
      "source": [
        "#**Calculate Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doQbOyGHpJYU",
        "colab_type": "code",
        "outputId": "2c39b74b-ac2d-4d73-a67c-cc901d1ff528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "##########Calculate Accuracy#############\n",
        "#valフォルダ内のファイル名を取得\n",
        "image_path = glob.glob(\"/content/drive/My Drive/Deep_learning/gravcont_seed_1234/val/*/*\")\n",
        "random.shuffle(image_path)  #表示順をランダムにする\n",
        "print('number of images: ' +str(len(image_path)))\n",
        "#print(image_path) \n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    \n",
        "    if ModelName == 'Attention_branch_Network_ImageNet':\n",
        "        output = _, output, _ = model_ft(image_tensor)\n",
        "    else:\n",
        "        output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (image_path):\n",
        "\n",
        "    #ここからがメイン\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    \n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "\n",
        "    #return(accuracy, precision, recall, specificity, f_value)\n",
        "    return model_pred_prob\n",
        "\n"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of images: 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk-s71bSlLMJ",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_VGGFace2のネットワーク**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twcn29TKk7kM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "c79c7bb3-6904-4462-af26-838478b7ba5f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "ModelName = 'ResNet50_VGGFace2'\n",
        "checkModelName(ModelName, ModelName_list)\n",
        "\n",
        "class Resnet50_ft_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Resnet50_ft_dag, self).__init__()\n",
        "        self.meta = {'mean': [131.0912, 103.8827, 91.4953],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv1_relu_7x7_s2 = nn.ReLU()\n",
        "        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
        "        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_3x3_relu = nn.ReLU()\n",
        "        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_3x3_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_3x3_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_3x3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_3x3_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_3x3_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_3x3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_3x3_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_3x3_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_3x3_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_3x3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_3x3_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_3x3_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_3x3_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_3x3_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_3x3_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_relu = nn.ReLU()\n",
        "        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
        "        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))\n",
        "\n",
        "    def forward(self, data):\n",
        "        conv1_7x7_s2 = self.conv1_7x7_s2(data)\n",
        "        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)\n",
        "        conv1_7x7_s2_bnxx = self.conv1_relu_7x7_s2(conv1_7x7_s2_bn)\n",
        "        pool1_3x3_s2 = self.pool1_3x3_s2(conv1_7x7_s2_bnxx)\n",
        "        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(pool1_3x3_s2)\n",
        "        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)\n",
        "        conv2_1_1x1_reduce_bnxx = self.conv2_1_1x1_reduce_relu(conv2_1_1x1_reduce_bn)\n",
        "        conv2_1_3x3 = self.conv2_1_3x3(conv2_1_1x1_reduce_bnxx)\n",
        "        conv2_1_3x3_bn = self.conv2_1_3x3_bn(conv2_1_3x3)\n",
        "        conv2_1_3x3_bnxx = self.conv2_1_3x3_relu(conv2_1_3x3_bn)\n",
        "        conv2_1_1x1_increase = self.conv2_1_1x1_increase(conv2_1_3x3_bnxx)\n",
        "        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)\n",
        "        conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)\n",
        "        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)\n",
        "        conv2_1 = torch.add(conv2_1_1x1_proj_bn, 1, conv2_1_1x1_increase_bn)\n",
        "        conv2_1x = self.conv2_1_relu(conv2_1)\n",
        "        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(conv2_1x)\n",
        "        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)\n",
        "        conv2_2_1x1_reduce_bnxx = self.conv2_2_1x1_reduce_relu(conv2_2_1x1_reduce_bn)\n",
        "        conv2_2_3x3 = self.conv2_2_3x3(conv2_2_1x1_reduce_bnxx)\n",
        "        conv2_2_3x3_bn = self.conv2_2_3x3_bn(conv2_2_3x3)\n",
        "        conv2_2_3x3_bnxx = self.conv2_2_3x3_relu(conv2_2_3x3_bn)\n",
        "        conv2_2_1x1_increase = self.conv2_2_1x1_increase(conv2_2_3x3_bnxx)\n",
        "        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)\n",
        "        conv2_2 = torch.add(conv2_1x, 1, conv2_2_1x1_increase_bn)\n",
        "        conv2_2x = self.conv2_2_relu(conv2_2)\n",
        "        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(conv2_2x)\n",
        "        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)\n",
        "        conv2_3_1x1_reduce_bnxx = self.conv2_3_1x1_reduce_relu(conv2_3_1x1_reduce_bn)\n",
        "        conv2_3_3x3 = self.conv2_3_3x3(conv2_3_1x1_reduce_bnxx)\n",
        "        conv2_3_3x3_bn = self.conv2_3_3x3_bn(conv2_3_3x3)\n",
        "        conv2_3_3x3_bnxx = self.conv2_3_3x3_relu(conv2_3_3x3_bn)\n",
        "        conv2_3_1x1_increase = self.conv2_3_1x1_increase(conv2_3_3x3_bnxx)\n",
        "        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)\n",
        "        conv2_3 = torch.add(conv2_2x, 1, conv2_3_1x1_increase_bn)\n",
        "        conv2_3x = self.conv2_3_relu(conv2_3)\n",
        "        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(conv2_3x)\n",
        "        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)\n",
        "        conv3_1_1x1_reduce_bnxx = self.conv3_1_1x1_reduce_relu(conv3_1_1x1_reduce_bn)\n",
        "        conv3_1_3x3 = self.conv3_1_3x3(conv3_1_1x1_reduce_bnxx)\n",
        "        conv3_1_3x3_bn = self.conv3_1_3x3_bn(conv3_1_3x3)\n",
        "        conv3_1_3x3_bnxx = self.conv3_1_3x3_relu(conv3_1_3x3_bn)\n",
        "        conv3_1_1x1_increase = self.conv3_1_1x1_increase(conv3_1_3x3_bnxx)\n",
        "        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)\n",
        "        conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)\n",
        "        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)\n",
        "        conv3_1 = torch.add(conv3_1_1x1_proj_bn, 1, conv3_1_1x1_increase_bn)\n",
        "        conv3_1x = self.conv3_1_relu(conv3_1)\n",
        "        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(conv3_1x)\n",
        "        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)\n",
        "        conv3_2_1x1_reduce_bnxx = self.conv3_2_1x1_reduce_relu(conv3_2_1x1_reduce_bn)\n",
        "        conv3_2_3x3 = self.conv3_2_3x3(conv3_2_1x1_reduce_bnxx)\n",
        "        conv3_2_3x3_bn = self.conv3_2_3x3_bn(conv3_2_3x3)\n",
        "        conv3_2_3x3_bnxx = self.conv3_2_3x3_relu(conv3_2_3x3_bn)\n",
        "        conv3_2_1x1_increase = self.conv3_2_1x1_increase(conv3_2_3x3_bnxx)\n",
        "        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)\n",
        "        conv3_2 = torch.add(conv3_1x, 1, conv3_2_1x1_increase_bn)\n",
        "        conv3_2x = self.conv3_2_relu(conv3_2)\n",
        "        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(conv3_2x)\n",
        "        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)\n",
        "        conv3_3_1x1_reduce_bnxx = self.conv3_3_1x1_reduce_relu(conv3_3_1x1_reduce_bn)\n",
        "        conv3_3_3x3 = self.conv3_3_3x3(conv3_3_1x1_reduce_bnxx)\n",
        "        conv3_3_3x3_bn = self.conv3_3_3x3_bn(conv3_3_3x3)\n",
        "        conv3_3_3x3_bnxx = self.conv3_3_3x3_relu(conv3_3_3x3_bn)\n",
        "        conv3_3_1x1_increase = self.conv3_3_1x1_increase(conv3_3_3x3_bnxx)\n",
        "        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)\n",
        "        conv3_3 = torch.add(conv3_2x, 1, conv3_3_1x1_increase_bn)\n",
        "        conv3_3x = self.conv3_3_relu(conv3_3)\n",
        "        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(conv3_3x)\n",
        "        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)\n",
        "        conv3_4_1x1_reduce_bnxx = self.conv3_4_1x1_reduce_relu(conv3_4_1x1_reduce_bn)\n",
        "        conv3_4_3x3 = self.conv3_4_3x3(conv3_4_1x1_reduce_bnxx)\n",
        "        conv3_4_3x3_bn = self.conv3_4_3x3_bn(conv3_4_3x3)\n",
        "        conv3_4_3x3_bnxx = self.conv3_4_3x3_relu(conv3_4_3x3_bn)\n",
        "        conv3_4_1x1_increase = self.conv3_4_1x1_increase(conv3_4_3x3_bnxx)\n",
        "        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)\n",
        "        conv3_4 = torch.add(conv3_3x, 1, conv3_4_1x1_increase_bn)\n",
        "        conv3_4x = self.conv3_4_relu(conv3_4)\n",
        "        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(conv3_4x)\n",
        "        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)\n",
        "        conv4_1_1x1_reduce_bnxx = self.conv4_1_1x1_reduce_relu(conv4_1_1x1_reduce_bn)\n",
        "        conv4_1_3x3 = self.conv4_1_3x3(conv4_1_1x1_reduce_bnxx)\n",
        "        conv4_1_3x3_bn = self.conv4_1_3x3_bn(conv4_1_3x3)\n",
        "        conv4_1_3x3_bnxx = self.conv4_1_3x3_relu(conv4_1_3x3_bn)\n",
        "        conv4_1_1x1_increase = self.conv4_1_1x1_increase(conv4_1_3x3_bnxx)\n",
        "        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)\n",
        "        conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)\n",
        "        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)\n",
        "        conv4_1 = torch.add(conv4_1_1x1_proj_bn, 1, conv4_1_1x1_increase_bn)\n",
        "        conv4_1x = self.conv4_1_relu(conv4_1)\n",
        "        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(conv4_1x)\n",
        "        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)\n",
        "        conv4_2_1x1_reduce_bnxx = self.conv4_2_1x1_reduce_relu(conv4_2_1x1_reduce_bn)\n",
        "        conv4_2_3x3 = self.conv4_2_3x3(conv4_2_1x1_reduce_bnxx)\n",
        "        conv4_2_3x3_bn = self.conv4_2_3x3_bn(conv4_2_3x3)\n",
        "        conv4_2_3x3_bnxx = self.conv4_2_3x3_relu(conv4_2_3x3_bn)\n",
        "        conv4_2_1x1_increase = self.conv4_2_1x1_increase(conv4_2_3x3_bnxx)\n",
        "        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)\n",
        "        conv4_2 = torch.add(conv4_1x, 1, conv4_2_1x1_increase_bn)\n",
        "        conv4_2x = self.conv4_2_relu(conv4_2)\n",
        "        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(conv4_2x)\n",
        "        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)\n",
        "        conv4_3_1x1_reduce_bnxx = self.conv4_3_1x1_reduce_relu(conv4_3_1x1_reduce_bn)\n",
        "        conv4_3_3x3 = self.conv4_3_3x3(conv4_3_1x1_reduce_bnxx)\n",
        "        conv4_3_3x3_bn = self.conv4_3_3x3_bn(conv4_3_3x3)\n",
        "        conv4_3_3x3_bnxx = self.conv4_3_3x3_relu(conv4_3_3x3_bn)\n",
        "        conv4_3_1x1_increase = self.conv4_3_1x1_increase(conv4_3_3x3_bnxx)\n",
        "        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)\n",
        "        conv4_3 = torch.add(conv4_2x, 1, conv4_3_1x1_increase_bn)\n",
        "        conv4_3x = self.conv4_3_relu(conv4_3)\n",
        "        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(conv4_3x)\n",
        "        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)\n",
        "        conv4_4_1x1_reduce_bnxx = self.conv4_4_1x1_reduce_relu(conv4_4_1x1_reduce_bn)\n",
        "        conv4_4_3x3 = self.conv4_4_3x3(conv4_4_1x1_reduce_bnxx)\n",
        "        conv4_4_3x3_bn = self.conv4_4_3x3_bn(conv4_4_3x3)\n",
        "        conv4_4_3x3_bnxx = self.conv4_4_3x3_relu(conv4_4_3x3_bn)\n",
        "        conv4_4_1x1_increase = self.conv4_4_1x1_increase(conv4_4_3x3_bnxx)\n",
        "        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)\n",
        "        conv4_4 = torch.add(conv4_3x, 1, conv4_4_1x1_increase_bn)\n",
        "        conv4_4x = self.conv4_4_relu(conv4_4)\n",
        "        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(conv4_4x)\n",
        "        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)\n",
        "        conv4_5_1x1_reduce_bnxx = self.conv4_5_1x1_reduce_relu(conv4_5_1x1_reduce_bn)\n",
        "        conv4_5_3x3 = self.conv4_5_3x3(conv4_5_1x1_reduce_bnxx)\n",
        "        conv4_5_3x3_bn = self.conv4_5_3x3_bn(conv4_5_3x3)\n",
        "        conv4_5_3x3_bnxx = self.conv4_5_3x3_relu(conv4_5_3x3_bn)\n",
        "        conv4_5_1x1_increase = self.conv4_5_1x1_increase(conv4_5_3x3_bnxx)\n",
        "        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)\n",
        "        conv4_5 = torch.add(conv4_4x, 1, conv4_5_1x1_increase_bn)\n",
        "        conv4_5x = self.conv4_5_relu(conv4_5)\n",
        "        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(conv4_5x)\n",
        "        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)\n",
        "        conv4_6_1x1_reduce_bnxx = self.conv4_6_1x1_reduce_relu(conv4_6_1x1_reduce_bn)\n",
        "        conv4_6_3x3 = self.conv4_6_3x3(conv4_6_1x1_reduce_bnxx)\n",
        "        conv4_6_3x3_bn = self.conv4_6_3x3_bn(conv4_6_3x3)\n",
        "        conv4_6_3x3_bnxx = self.conv4_6_3x3_relu(conv4_6_3x3_bn)\n",
        "        conv4_6_1x1_increase = self.conv4_6_1x1_increase(conv4_6_3x3_bnxx)\n",
        "        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)\n",
        "        conv4_6 = torch.add(conv4_5x, 1, conv4_6_1x1_increase_bn)\n",
        "        conv4_6x = self.conv4_6_relu(conv4_6)\n",
        "        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(conv4_6x)\n",
        "        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)\n",
        "        conv5_1_1x1_reduce_bnxx = self.conv5_1_1x1_reduce_relu(conv5_1_1x1_reduce_bn)\n",
        "        conv5_1_3x3 = self.conv5_1_3x3(conv5_1_1x1_reduce_bnxx)\n",
        "        conv5_1_3x3_bn = self.conv5_1_3x3_bn(conv5_1_3x3)\n",
        "        conv5_1_3x3_bnxx = self.conv5_1_3x3_relu(conv5_1_3x3_bn)\n",
        "        conv5_1_1x1_increase = self.conv5_1_1x1_increase(conv5_1_3x3_bnxx)\n",
        "        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)\n",
        "        conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)\n",
        "        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)\n",
        "        conv5_1 = torch.add(conv5_1_1x1_proj_bn, 1, conv5_1_1x1_increase_bn)\n",
        "        conv5_1x = self.conv5_1_relu(conv5_1)\n",
        "        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(conv5_1x)\n",
        "        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)\n",
        "        conv5_2_1x1_reduce_bnxx = self.conv5_2_1x1_reduce_relu(conv5_2_1x1_reduce_bn)\n",
        "        conv5_2_3x3 = self.conv5_2_3x3(conv5_2_1x1_reduce_bnxx)\n",
        "        conv5_2_3x3_bn = self.conv5_2_3x3_bn(conv5_2_3x3)\n",
        "        conv5_2_3x3_bnxx = self.conv5_2_3x3_relu(conv5_2_3x3_bn)\n",
        "        conv5_2_1x1_increase = self.conv5_2_1x1_increase(conv5_2_3x3_bnxx)\n",
        "        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)\n",
        "        conv5_2 = torch.add(conv5_1x, 1, conv5_2_1x1_increase_bn)\n",
        "        conv5_2x = self.conv5_2_relu(conv5_2)\n",
        "        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(conv5_2x)\n",
        "        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)\n",
        "        conv5_3_1x1_reduce_bnxx = self.conv5_3_1x1_reduce_relu(conv5_3_1x1_reduce_bn)\n",
        "        conv5_3_3x3 = self.conv5_3_3x3(conv5_3_1x1_reduce_bnxx)\n",
        "        conv5_3_3x3_bn = self.conv5_3_3x3_bn(conv5_3_3x3)\n",
        "        conv5_3_3x3_bnxx = self.conv5_3_3x3_relu(conv5_3_3x3_bn)\n",
        "        conv5_3_1x1_increase = self.conv5_3_1x1_increase(conv5_3_3x3_bnxx)\n",
        "        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)\n",
        "        conv5_3 = torch.add(conv5_2x, 1, conv5_3_1x1_increase_bn)\n",
        "        conv5_3x = self.conv5_3_relu(conv5_3)\n",
        "        pool5_7x7_s1 = self.pool5_7x7_s1(conv5_3x)\n",
        "        classifier_preflatten = self.classifier(pool5_7x7_s1)\n",
        "        classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)\n",
        "        #return classifier, pool5_7x7_s1 　出力を変更しておかないと次元が合わないと言われる\n",
        "        return classifier\n",
        "\n",
        "def resnet50_ft_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Resnet50_ft_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "#モデルのロード\n",
        "model_ft = Resnet50_ft_dag()\n",
        "\n",
        "#最終結合層のリセットと付け替え(全結合層を2つに)\n",
        "model_ft.classifier = nn.Linear(2048, 2)\n",
        "model_ft.classifier = nn.Sequential(*([Flatten()] + list(model_ft.children())[-1:])) #Flattenを挿入\n",
        "\n",
        "#重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_VGGFace2_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "model_pred_prob.append(calculateAccuracy(image_path))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50 15 54 9\n",
            "Accuracy: 0.8125\n",
            "Precision (positive predictive value): 0.847457627118644\n",
            "Recall (sensitivity): 0.7692307692307693\n",
            "Specificity: 0.8571428571428571\n",
            "F_value: 0.8064516129032259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72UkHpANnFjw",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNwSFAOfnEtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "cfb76598-4b1d-465a-df47-2111951885d6"
      },
      "source": [
        "ModelName = 'ResNet50_ImageNet'\n",
        "checkModelName(ModelName, ModelName_list)\n",
        "\n",
        "model_ft = models.resnet50(pretrained=False)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# 重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_ImageNet_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "model_pred_prob.append(calculateAccuracy(image_path))"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47 18 36 27\n",
            "Accuracy: 0.6484375\n",
            "Precision (positive predictive value): 0.6351351351351351\n",
            "Recall (sensitivity): 0.7230769230769231\n",
            "Specificity: 0.5714285714285714\n",
            "F_value: 0.6762589928057553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H64GGa2JmT9D",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_nonPretrained**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVKpRFdAmUDR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "ebcaa5ed-9872-409c-d0ad-82347b38f300"
      },
      "source": [
        "ModelName = 'ResNet50_nonPretrained'\n",
        "checkModelName(ModelName, ModelName_list)\n",
        "\n",
        "model_ft = models.resnet50(pretrained=False)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# 重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_ImageNet_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "model_pred_prob.append(calculateAccuracy(image_path))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47 18 36 27\n",
            "Accuracy: 0.6484375\n",
            "Precision (positive predictive value): 0.6351351351351351\n",
            "Recall (sensitivity): 0.7230769230769231\n",
            "Specificity: 0.5714285714285714\n",
            "F_value: 0.6762589928057553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWhxHmDPns3R",
        "colab_type": "text"
      },
      "source": [
        "#**EfficientNet_b4_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syACduJCntAB",
        "colab_type": "code",
        "outputId": "52d2d36a-9e4b-4994-8c4e-2620f009df4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "ModelName = 'EfficientNet_b4_ImageNet'\n",
        "checkModelName(ModelName, ModelName_list)    \n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "model_ft = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "num_ftrs = model_ft._fc.in_features\n",
        "model_ft._fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "model_pred_prob.append(calculateAccuracy(image_path))"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.4)\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "55 10 52 11\n",
            "Accuracy: 0.8359375\n",
            "Precision (positive predictive value): 0.8333333333333334\n",
            "Recall (sensitivity): 0.8461538461538461\n",
            "Specificity: 0.8253968253968254\n",
            "F_value: 0.8396946564885497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymGrWxHfntKI",
        "colab_type": "text"
      },
      "source": [
        "#**Attention_branch_Network_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gESMORA2ntRo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d17cee08-ce85-43f4-ee5b-2c93eac67073"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "ModelName = 'Attention_branch_Network_ImageNet'\n",
        "checkModelName(ModelName, ModelName_list)   \n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "\n",
        "class ResNet_ARN(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet_ARN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], down_size=True)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, down_size=True)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, down_size=True)\n",
        "\n",
        "        self.att_layer4 = self._make_layer(block, 512, layers[3], stride=1, down_size=False)\n",
        "        self.bn_att = nn.BatchNorm2d(512 * block.expansion)\n",
        "        self.att_conv   = nn.Conv2d(512 * block.expansion, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.bn_att2 = nn.BatchNorm2d(num_classes)\n",
        "        self.att_conv2  = nn.Conv2d(num_classes, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.att_conv3  = nn.Conv2d(num_classes, 1, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn_att3 = nn.BatchNorm2d(1)\n",
        "        self.att_gap = nn.AvgPool2d(14)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, down_size=True)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, down_size=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "\n",
        "        if down_size:\n",
        "            self.inplanes = planes * block.expansion\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "        else:\n",
        "            inplanes = planes * block.expansion\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        fe = x\n",
        "\n",
        "        ax = self.bn_att(self.att_layer4(x))\n",
        "        ax = self.relu(self.bn_att2(self.att_conv(ax)))\n",
        "        bs, cs, ys, xs = ax.shape\n",
        "        self.att = self.sigmoid(self.bn_att3(self.att_conv3(ax)))        \n",
        "        # self.att = self.att.view(bs, 1, ys, xs)\n",
        "        ax = self.att_conv2(ax)\n",
        "        ax = self.att_gap(ax)\n",
        "        ax = ax.view(ax.size(0), -1)\n",
        "\n",
        "        rx = x * self.att\n",
        "        rx = rx + x\n",
        "        per = rx\n",
        "        rx = self.layer4(rx)\n",
        "        rx = self.avgpool(rx)\n",
        "        rx = rx.view(rx.size(0), -1)\n",
        "        rx = self.fc(rx)\n",
        "\n",
        "        return ax, rx, [self.att, fe, per]\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "def resnetARN50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_ARN(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "#モデルのロード\n",
        "model_ft = resnetARN50().to(device)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#attention branch networkの最終出力を2つにする\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/gravcont_att_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/gravcont_att_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "model_pred_prob.append(calculateAccuracy(image_path))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-02f497710941>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;31m#Prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m \u001b[0mmodel_pred_prob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculateAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-123-7fb5cc733bfd>\u001b[0m in \u001b[0;36mcalculateAccuracy\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     93\u001b[0m           \u001b[0mimage_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#画像の名前とラベルを取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m           \u001b[0mimage_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#予測のための画像下処理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m           \u001b[0mmodel_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#予測結果を出力\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m           \u001b[0;31m#print('Image: '+ image_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m           \u001b[0;31m#print('Label: '+ label)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-123-7fb5cc733bfd>\u001b[0m in \u001b[0;36mimage_eval\u001b[0;34m(image_tensor, label)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mModelName\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Attention_branch_Network_ImageNet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-130-02f497710941>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mrx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mrx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mrx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_addmm"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVay7Id8oIPK",
        "colab_type": "text"
      },
      "source": [
        "#**Drawing ROC curve** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q1bF1hpg5Qd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "336dde02-6043-4065-af1f-995c2056f13e"
      },
      "source": [
        "print(model_pred_prob)\n",
        "len(model_pred_prob)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.999891996383667, 0.9999786615371704, 0.8180245757102966, 0.2631845474243164, 0.18998384475708008, 0.9999922513961792, 0.8118093013763428, 0.5611247420310974, 0.9550123810768127, 0.463189959526062, 0.9981873631477356, 0.03722041845321655, 0.4843406677246094, 0.08021163940429688, 0.020694434642791748, 0.14328551292419434, 0.9997441172599792, 0.34198999404907227, 0.1644439697265625, 0.9912379384040833, 0.2542804479598999, 0.9965883493423462, 0.9705369472503662, 0.7397899627685547, 0.3626706004142761, 0.9999887943267822, 0.991655707359314, 0.5520241260528564, 0.9947236776351929, 0.044655799865722656, 0.034631550312042236, 0.7642732262611389, 0.9987905621528625, 0.9790109992027283, 0.1825065016746521, 0.7727088928222656, 0.7029680013656616, 0.04016178846359253, 0.08155953884124756, 0.15973007678985596, 0.035286784172058105, 0.08215898275375366, 0.21190625429153442, 0.9998177886009216, 0.9597899317741394, 0.017274916172027588, 0.15833890438079834, 0.9993981122970581, 0.999994158744812, 0.9997712969779968, 0.25128233432769775, 0.3338892459869385, 0.9999620914459229, 0.9771180152893066, 0.10501933097839355, 0.13279128074645996, 0.9950986504554749, 0.9999979734420776, 0.9999029636383057, 0.12338149547576904, 0.04878133535385132, 0.5824019312858582, 0.8634684681892395, 0.9950608611106873, 0.07400882244110107, 0.9998389482498169, 0.9790650606155396, 0.38154661655426025, 0.10793709754943848, 0.11153936386108398, 0.9988924860954285, 0.08007705211639404, 0.23866605758666992, 0.04064488410949707, 0.07347387075424194, 0.7264547348022461, 0.7178724408149719, 0.12976133823394775, 0.9999529123306274, 0.5534926056861877, 0.5122610926628113, 0.115531325340271, 0.054137587547302246, 0.034967899322509766, 0.8475646376609802, 0.7494966387748718, 0.50101238489151, 0.7320812940597534, 0.24226748943328857, 0.9999929666519165, 0.980217695236206, 1.0, 0.9999799728393555, 0.10690063238143921, 0.8987223505973816, 0.5207807421684265, 0.999922513961792, 0.11166650056838989, 0.7021567821502686, 0.036663711071014404, 0.4335808753967285, 0.9999843835830688, 0.4647541046142578, 0.026246607303619385, 0.9909512996673584, 0.4994310140609741, 0.24655413627624512, 0.4469411373138428, 0.8928737044334412, 0.027871131896972656, 0.8348885774612427, 0.018199443817138672, 0.20673328638076782, 0.9045776128768921, 0.9997963309288025, 0.7218027114868164, 0.009984791278839111, 0.9829356670379639, 0.9999953508377075, 0.0979154109954834, 0.42002350091934204, 0.24956220388412476, 0.9411635994911194, 0.9996811151504517, 0.617763340473175, 0.9999992847442627, 0.9999947547912598, 0.056265175342559814], [0.5978227853775024, 0.45511138439178467, 0.4514177441596985, 0.5167866945266724, 0.7105836868286133, 0.9007282257080078, 0.4444500803947449, 0.5486435294151306, 0.9676823616027832, 0.4580063819885254, 0.4485820531845093, 0.5729544162750244, 0.4386497735977173, 0.5330690741539001, 0.4416743516921997, 0.6661463379859924, 0.8560894131660461, 0.47137463092803955, 0.45038676261901855, 0.5129293203353882, 0.448797345161438, 0.45731836557388306, 0.8563628792762756, 0.46268218755722046, 0.4467776417732239, 0.6131135821342468, 0.4601612091064453, 0.46317386627197266, 0.9902614951133728, 0.4700145721435547, 0.8911248445510864, 0.43456220626831055, 0.2614743113517761, 0.6834436058998108, 0.447370707988739, 0.5009387731552124, 0.9977260231971741, 0.6278179883956909, 0.4540392756462097, 0.4443066120147705, 0.7772385478019714, 0.4819785952568054, 0.49471449851989746, 0.5550156235694885, 0.3400474786758423, 0.4005424380302429, 0.5119511485099792, 0.6010584831237793, 0.8997153043746948, 0.8418664932250977, 0.44690996408462524, 0.4366557002067566, 0.4686068296432495, 0.5527889132499695, 0.46048790216445923, 0.47970861196517944, 0.4872545003890991, 0.6279266476631165, 0.6342640519142151, 0.45760560035705566, 0.4470701813697815, 0.4445747137069702, 0.9579796195030212, 0.6348122358322144, 0.457197904586792, 0.45561647415161133, 0.5045126080513, 0.4883279800415039, 0.45748597383499146, 0.4547216296195984, 0.4337611198425293, 0.5043448209762573, 0.4392452836036682, 0.4513764977455139, 0.45215898752212524, 0.2621484398841858, 0.4367274045944214, 0.4600644111633301, 0.477481484413147, 0.5756432414054871, 0.9800602197647095, 0.8785755038261414, 0.4963163137435913, 0.46735960245132446, 0.49674248695373535, 0.457591712474823, 0.4445977210998535, 0.4455602765083313, 0.6509571671485901, 0.7793661952018738, 0.5030667781829834, 0.665060818195343, 0.6300921440124512, 0.5098046064376831, 0.3746839761734009, 0.523269772529602, 0.6890971064567566, 0.9984369874000549, 0.4510357975959778, 0.4575575590133667, 0.472176730632782, 0.46496206521987915, 0.588966965675354, 0.44307589530944824, 0.46251553297042847, 0.44409239292144775, 0.43959057331085205, 0.6545975804328918, 0.45255887508392334, 0.4673295021057129, 0.44579291343688965, 0.3950538635253906, 0.4404844045639038, 0.6870983242988586, 0.6061873435974121, 0.4452696442604065, 0.4485463500022888, 0.9778621196746826, 0.9863038659095764, 0.44945383071899414, 0.4913640022277832, 0.486394464969635, 0.7328001260757446, 0.6959050893783569, 0.6175581812858582, 0.8883811235427856, 0.42616045475006104, 0.4723024368286133], [0.9999662637710571, 0.9958206415176392, 0.0006622672080993652, 0.8678992390632629, 0.0007329583168029785, 0.999346911907196, 0.9479318261146545, 0.05251765251159668, 0.9714308977127075, 0.0007756948471069336, 0.987168550491333, 0.037167370319366455, 0.25491178035736084, 0.005273997783660889, 0.006403923034667969, 0.2648872137069702, 0.9990183115005493, 0.0502665638923645, 0.006535828113555908, 0.14230036735534668, 0.5668217539787292, 0.9984018206596375, 0.9919238686561584, 0.8732227087020874, 0.00036978721618652344, 0.9997565150260925, 0.92750483751297, 0.37741565704345703, 0.9996860027313232, 0.00044339895248413086, 0.0006186962127685547, 0.9756995439529419, 0.9857271313667297, 0.9948262572288513, 0.5740213394165039, 0.3965492844581604, 0.6500130295753479, 6.365776062011719e-05, 0.0036216378211975098, 0.2557086944580078, 0.001439511775970459, 0.5837361812591553, 0.003927350044250488, 0.9876694083213806, 0.9933193922042847, 0.0017742514610290527, 0.035999596118927, 0.9999672174453735, 0.9682961702346802, 0.893115222454071, 0.009379982948303223, 0.9654383659362793, 0.9947071671485901, 0.9608086347579956, 0.8137933611869812, 0.05095958709716797, 0.9391413927078247, 0.9987878203392029, 0.9996174573898315, 0.0001442432403564453, 0.0002949833869934082, 0.018832623958587646, 0.7031481266021729, 0.9974523186683655, 0.039182424545288086, 0.9928106665611267, 0.9997701048851013, 0.01166313886642456, 0.00047969818115234375, 0.0011022090911865234, 0.9999167919158936, 0.03855395317077637, 0.001295924186706543, 4.3392181396484375e-05, 0.1640024185180664, 0.026497721672058105, 0.03431934118270874, 0.0575408935546875, 0.9989601373672485, 0.25779926776885986, 0.07819390296936035, 0.030348539352416992, 0.021691501140594482, 0.0034273862838745117, 0.025782287120819092, 0.47985851764678955, 0.007214009761810303, 0.7960048317909241, 0.005148470401763916, 0.9918732047080994, 0.9884399175643921, 0.999996542930603, 0.9996063113212585, 0.0016132593154907227, 0.9949856996536255, 0.6909025311470032, 0.9994410872459412, 0.1462746262550354, 0.09144806861877441, 0.056277692317962646, 0.6457259654998779, 0.9996297359466553, 0.020458638668060303, 4.947185516357422e-05, 0.9997363686561584, 0.9679014086723328, 0.7866513133049011, 0.00605165958404541, 0.8373453617095947, 0.13252681493759155, 0.10499650239944458, 0.00048291683197021484, 0.006132781505584717, 0.9590820074081421, 0.9942471981048584, 0.015684068202972412, 0.00014698505401611328, 0.6945499777793884, 0.9994408488273621, 0.002390146255493164, 0.2320229411125183, 0.19384384155273438, 0.8968398571014404, 0.9998812675476074, 0.9187160730361938, 0.9999723434448242, 0.9992615580558777, 0.23124456405639648]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_c2zVMKoIVK",
        "colab_type": "code",
        "outputId": "5e3e1b0c-630b-447d-ca73-400cf12f320b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_score = []\n",
        "y_true = []\n",
        "\n",
        "k=0\n",
        "for i in label_list:\n",
        "    if label_list[k] == 'cont':\n",
        "          y_true.append(0)\n",
        "    elif label_list[k] == 'grav':\n",
        "          y_true.append(1)\n",
        "    k+=1\n",
        "\n",
        "\n",
        "#健康な状態を「0」、病気を「1」としてラベルよりリストを作成\n",
        "y_true = y_true\n",
        "#それぞれの画像における陽性の確率についてリストを作成\n",
        "y_score = model_pred_prob\n",
        "\n",
        "#print(y_true)\n",
        "#print(y_score)\n",
        "\n",
        "fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#print(fpr)\n",
        "#print(tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gom3bMGgSXVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_score = []\n",
        "y_true = []\n",
        "\n",
        "k=0\n",
        "for i in label_list:\n",
        "    if label_list[k] == 'cont':\n",
        "          y_true.append(0)\n",
        "    elif label_list[k] == 'grav':\n",
        "          y_true.append(1)\n",
        "    k+=1\n",
        "\n",
        "\n",
        "#健康な状態を「0」、病気を「1」としてラベルよりリストを作成\n",
        "y_true = y_true\n",
        "#それぞれの画像における陽性の確率についてリストを作成\n",
        "y_score = model_pred_prob\n",
        "\n",
        "#print(y_true)\n",
        "#print(y_score)\n",
        "\n",
        "fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#print(fpr)\n",
        "#print(tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}