{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled35.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWZLMZluNy8mkUIoRKHkyW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Assessment1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpC0Fk9lUn2",
        "colab_type": "text"
      },
      "source": [
        "#**Assessment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9B59fSXlT5f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "8e54c2c0-0c94-4217-c0dc-f2061a92b9d5"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  1234\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHO1RjNom0Pr",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-yVbbvSm3Ay",
        "colab_type": "code",
        "outputId": "778c2b56-d313-43a2-9d17-f74af320b98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "#ModelNameをリストにする\n",
        "ModelName_list = []\n",
        "ModelName = ''\n",
        "model_pred_prob = []\n",
        "\n",
        "def checkModelName(ModelName, ModelName_list):\n",
        "    if ModelName in ModelName_list:\n",
        "        pass\n",
        "        #raise Exception(\"This model has been already loaded\")\n",
        "    else:\n",
        "        ModelName_list.append(ModelName)\n",
        "\n",
        "\n",
        "# 入力画像の前処理をするクラス\n",
        "# 訓練時と推論時で処理が異なる\n",
        "\n",
        "\"\"\"\n",
        "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "\"\"\"\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/My Drive/Deep_learning/gravcont_seed_1234'\n",
        "n_samples = len(data_dir)\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "print(class_names)\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_train:\"+str(len(os.listdir(path='/content/drive/My Drive/Deep_learning/gravcont_seed_1234/train/'+class_names[k]))))\n",
        "    k+=1\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_val:\"+str(len(os.listdir(path='/content/drive/My Drive/Deep_learning/gravcont_seed_1234/val/'+class_names[k]))))\n",
        "    k+=1\n",
        "\n",
        "print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "\n"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cont', 'grav']\n",
            "cont_train:256\n",
            "grav_train:252\n",
            "cont_val:65\n",
            "grav_val:63\n",
            "training data set_total：508\n",
            "validating data set_total：128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zLnknjNnqNU",
        "colab_type": "text"
      },
      "source": [
        "#**Calculate Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doQbOyGHpJYU",
        "colab_type": "code",
        "outputId": "97d4e38a-54d5-4beb-dc96-23b41012b23e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "##########Calculate Accuracy#############\n",
        "#valフォルダ内のファイル名を取得\n",
        "image_path = glob.glob(\"/content/drive/My Drive/Deep_learning/gravcont_seed_1234/val/*/*\")\n",
        "random.shuffle(image_path)  #表示順をランダムにする\n",
        "print('number of images: ' +str(len(image_path)))\n",
        "#print(image_path) \n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    \n",
        "    if ModelName == 'Attention_branch_Network_ImageNet':\n",
        "        _, output, _ = model_ft(image_tensor)\n",
        "    else:\n",
        "        output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #評価モードにする\n",
        "    model_ft.eval()\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (image_path):\n",
        "\n",
        "    #ここからがメイン\n",
        "    TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "    image_name_list = []\n",
        "    label_list = []\n",
        "    model_pred_list = []\n",
        "    hum_pred_list = []\n",
        "\n",
        "    model_pred_class = []\n",
        "    model_pred_prob = []\n",
        "\n",
        "    for i in image_path:\n",
        "          image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "          image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "          model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力 \n",
        "\n",
        "          #print('Image: '+ image_name)\n",
        "          #print('Label: '+ label)\n",
        "          #print('Pred: '+ model_pred)\n",
        "          #showImage(i)  #画像を表示\n",
        "          #print() #空白行を入れる\n",
        "          time.sleep(0.1)\n",
        "\n",
        "          image_name_list.append(image_name)\n",
        "          label_list.append(label)\n",
        "          model_pred_list.append(model_pred)\n",
        "\n",
        "          model_pred_class.append(int(pred))\n",
        "          model_pred_prob.append(float(prob))\n",
        "\n",
        "          if label == class_names[0]:\n",
        "              if model_pred == class_names[0]:\n",
        "                  TP += 1\n",
        "              else:\n",
        "                  FN += 1\n",
        "          elif label == class_names[1]:\n",
        "              if model_pred == class_names[1]:\n",
        "                  TN += 1\n",
        "              else:\n",
        "                  FP += 1\n",
        "          \n",
        "\n",
        "    print(TP, FN, TN, FP)\n",
        "\n",
        "    #Accuracyを計算\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    \n",
        "    print('Accuracy: ' + str(accuracy))\n",
        "    print('Precision (positive predictive value): ' + str(precision))\n",
        "    print('Recall (sensitivity): ' + str(recall))\n",
        "    print('Specificity: ' + str(specificity))\n",
        "    print('F_value: ' + str(f_value))\n",
        "\n",
        "    #print(model_pred_class)\n",
        "    #print(model_pred_prob)\n",
        "\n",
        "    #return(accuracy, precision, recall, specificity, f_value)\n",
        "    return model_pred_prob\n",
        "\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of images: 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk-s71bSlLMJ",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_VGGFace2のネットワーク**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twcn29TKk7kM",
        "colab_type": "code",
        "outputId": "3f568e70-ca01-490c-ab94-3e2769fd26cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "ModelName = 'ResNet50_VGGFace2'\n",
        "checkModelName(ModelName, ModelName_list)\n",
        "\n",
        "class Resnet50_ft_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Resnet50_ft_dag, self).__init__()\n",
        "        self.meta = {'mean': [131.0912, 103.8827, 91.4953],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv1_relu_7x7_s2 = nn.ReLU()\n",
        "        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
        "        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_3x3_relu = nn.ReLU()\n",
        "        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_3x3_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_3x3_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_3x3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_3x3_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_3x3_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_3x3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_3x3_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_3x3_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_3x3_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_3x3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_3x3_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_3x3_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_3x3_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_3x3_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_3x3_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_relu = nn.ReLU()\n",
        "        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
        "        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))\n",
        "\n",
        "    def forward(self, data):\n",
        "        conv1_7x7_s2 = self.conv1_7x7_s2(data)\n",
        "        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)\n",
        "        conv1_7x7_s2_bnxx = self.conv1_relu_7x7_s2(conv1_7x7_s2_bn)\n",
        "        pool1_3x3_s2 = self.pool1_3x3_s2(conv1_7x7_s2_bnxx)\n",
        "        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(pool1_3x3_s2)\n",
        "        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)\n",
        "        conv2_1_1x1_reduce_bnxx = self.conv2_1_1x1_reduce_relu(conv2_1_1x1_reduce_bn)\n",
        "        conv2_1_3x3 = self.conv2_1_3x3(conv2_1_1x1_reduce_bnxx)\n",
        "        conv2_1_3x3_bn = self.conv2_1_3x3_bn(conv2_1_3x3)\n",
        "        conv2_1_3x3_bnxx = self.conv2_1_3x3_relu(conv2_1_3x3_bn)\n",
        "        conv2_1_1x1_increase = self.conv2_1_1x1_increase(conv2_1_3x3_bnxx)\n",
        "        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)\n",
        "        conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)\n",
        "        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)\n",
        "        conv2_1 = torch.add(conv2_1_1x1_proj_bn, 1, conv2_1_1x1_increase_bn)\n",
        "        conv2_1x = self.conv2_1_relu(conv2_1)\n",
        "        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(conv2_1x)\n",
        "        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)\n",
        "        conv2_2_1x1_reduce_bnxx = self.conv2_2_1x1_reduce_relu(conv2_2_1x1_reduce_bn)\n",
        "        conv2_2_3x3 = self.conv2_2_3x3(conv2_2_1x1_reduce_bnxx)\n",
        "        conv2_2_3x3_bn = self.conv2_2_3x3_bn(conv2_2_3x3)\n",
        "        conv2_2_3x3_bnxx = self.conv2_2_3x3_relu(conv2_2_3x3_bn)\n",
        "        conv2_2_1x1_increase = self.conv2_2_1x1_increase(conv2_2_3x3_bnxx)\n",
        "        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)\n",
        "        conv2_2 = torch.add(conv2_1x, 1, conv2_2_1x1_increase_bn)\n",
        "        conv2_2x = self.conv2_2_relu(conv2_2)\n",
        "        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(conv2_2x)\n",
        "        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)\n",
        "        conv2_3_1x1_reduce_bnxx = self.conv2_3_1x1_reduce_relu(conv2_3_1x1_reduce_bn)\n",
        "        conv2_3_3x3 = self.conv2_3_3x3(conv2_3_1x1_reduce_bnxx)\n",
        "        conv2_3_3x3_bn = self.conv2_3_3x3_bn(conv2_3_3x3)\n",
        "        conv2_3_3x3_bnxx = self.conv2_3_3x3_relu(conv2_3_3x3_bn)\n",
        "        conv2_3_1x1_increase = self.conv2_3_1x1_increase(conv2_3_3x3_bnxx)\n",
        "        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)\n",
        "        conv2_3 = torch.add(conv2_2x, 1, conv2_3_1x1_increase_bn)\n",
        "        conv2_3x = self.conv2_3_relu(conv2_3)\n",
        "        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(conv2_3x)\n",
        "        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)\n",
        "        conv3_1_1x1_reduce_bnxx = self.conv3_1_1x1_reduce_relu(conv3_1_1x1_reduce_bn)\n",
        "        conv3_1_3x3 = self.conv3_1_3x3(conv3_1_1x1_reduce_bnxx)\n",
        "        conv3_1_3x3_bn = self.conv3_1_3x3_bn(conv3_1_3x3)\n",
        "        conv3_1_3x3_bnxx = self.conv3_1_3x3_relu(conv3_1_3x3_bn)\n",
        "        conv3_1_1x1_increase = self.conv3_1_1x1_increase(conv3_1_3x3_bnxx)\n",
        "        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)\n",
        "        conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)\n",
        "        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)\n",
        "        conv3_1 = torch.add(conv3_1_1x1_proj_bn, 1, conv3_1_1x1_increase_bn)\n",
        "        conv3_1x = self.conv3_1_relu(conv3_1)\n",
        "        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(conv3_1x)\n",
        "        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)\n",
        "        conv3_2_1x1_reduce_bnxx = self.conv3_2_1x1_reduce_relu(conv3_2_1x1_reduce_bn)\n",
        "        conv3_2_3x3 = self.conv3_2_3x3(conv3_2_1x1_reduce_bnxx)\n",
        "        conv3_2_3x3_bn = self.conv3_2_3x3_bn(conv3_2_3x3)\n",
        "        conv3_2_3x3_bnxx = self.conv3_2_3x3_relu(conv3_2_3x3_bn)\n",
        "        conv3_2_1x1_increase = self.conv3_2_1x1_increase(conv3_2_3x3_bnxx)\n",
        "        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)\n",
        "        conv3_2 = torch.add(conv3_1x, 1, conv3_2_1x1_increase_bn)\n",
        "        conv3_2x = self.conv3_2_relu(conv3_2)\n",
        "        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(conv3_2x)\n",
        "        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)\n",
        "        conv3_3_1x1_reduce_bnxx = self.conv3_3_1x1_reduce_relu(conv3_3_1x1_reduce_bn)\n",
        "        conv3_3_3x3 = self.conv3_3_3x3(conv3_3_1x1_reduce_bnxx)\n",
        "        conv3_3_3x3_bn = self.conv3_3_3x3_bn(conv3_3_3x3)\n",
        "        conv3_3_3x3_bnxx = self.conv3_3_3x3_relu(conv3_3_3x3_bn)\n",
        "        conv3_3_1x1_increase = self.conv3_3_1x1_increase(conv3_3_3x3_bnxx)\n",
        "        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)\n",
        "        conv3_3 = torch.add(conv3_2x, 1, conv3_3_1x1_increase_bn)\n",
        "        conv3_3x = self.conv3_3_relu(conv3_3)\n",
        "        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(conv3_3x)\n",
        "        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)\n",
        "        conv3_4_1x1_reduce_bnxx = self.conv3_4_1x1_reduce_relu(conv3_4_1x1_reduce_bn)\n",
        "        conv3_4_3x3 = self.conv3_4_3x3(conv3_4_1x1_reduce_bnxx)\n",
        "        conv3_4_3x3_bn = self.conv3_4_3x3_bn(conv3_4_3x3)\n",
        "        conv3_4_3x3_bnxx = self.conv3_4_3x3_relu(conv3_4_3x3_bn)\n",
        "        conv3_4_1x1_increase = self.conv3_4_1x1_increase(conv3_4_3x3_bnxx)\n",
        "        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)\n",
        "        conv3_4 = torch.add(conv3_3x, 1, conv3_4_1x1_increase_bn)\n",
        "        conv3_4x = self.conv3_4_relu(conv3_4)\n",
        "        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(conv3_4x)\n",
        "        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)\n",
        "        conv4_1_1x1_reduce_bnxx = self.conv4_1_1x1_reduce_relu(conv4_1_1x1_reduce_bn)\n",
        "        conv4_1_3x3 = self.conv4_1_3x3(conv4_1_1x1_reduce_bnxx)\n",
        "        conv4_1_3x3_bn = self.conv4_1_3x3_bn(conv4_1_3x3)\n",
        "        conv4_1_3x3_bnxx = self.conv4_1_3x3_relu(conv4_1_3x3_bn)\n",
        "        conv4_1_1x1_increase = self.conv4_1_1x1_increase(conv4_1_3x3_bnxx)\n",
        "        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)\n",
        "        conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)\n",
        "        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)\n",
        "        conv4_1 = torch.add(conv4_1_1x1_proj_bn, 1, conv4_1_1x1_increase_bn)\n",
        "        conv4_1x = self.conv4_1_relu(conv4_1)\n",
        "        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(conv4_1x)\n",
        "        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)\n",
        "        conv4_2_1x1_reduce_bnxx = self.conv4_2_1x1_reduce_relu(conv4_2_1x1_reduce_bn)\n",
        "        conv4_2_3x3 = self.conv4_2_3x3(conv4_2_1x1_reduce_bnxx)\n",
        "        conv4_2_3x3_bn = self.conv4_2_3x3_bn(conv4_2_3x3)\n",
        "        conv4_2_3x3_bnxx = self.conv4_2_3x3_relu(conv4_2_3x3_bn)\n",
        "        conv4_2_1x1_increase = self.conv4_2_1x1_increase(conv4_2_3x3_bnxx)\n",
        "        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)\n",
        "        conv4_2 = torch.add(conv4_1x, 1, conv4_2_1x1_increase_bn)\n",
        "        conv4_2x = self.conv4_2_relu(conv4_2)\n",
        "        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(conv4_2x)\n",
        "        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)\n",
        "        conv4_3_1x1_reduce_bnxx = self.conv4_3_1x1_reduce_relu(conv4_3_1x1_reduce_bn)\n",
        "        conv4_3_3x3 = self.conv4_3_3x3(conv4_3_1x1_reduce_bnxx)\n",
        "        conv4_3_3x3_bn = self.conv4_3_3x3_bn(conv4_3_3x3)\n",
        "        conv4_3_3x3_bnxx = self.conv4_3_3x3_relu(conv4_3_3x3_bn)\n",
        "        conv4_3_1x1_increase = self.conv4_3_1x1_increase(conv4_3_3x3_bnxx)\n",
        "        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)\n",
        "        conv4_3 = torch.add(conv4_2x, 1, conv4_3_1x1_increase_bn)\n",
        "        conv4_3x = self.conv4_3_relu(conv4_3)\n",
        "        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(conv4_3x)\n",
        "        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)\n",
        "        conv4_4_1x1_reduce_bnxx = self.conv4_4_1x1_reduce_relu(conv4_4_1x1_reduce_bn)\n",
        "        conv4_4_3x3 = self.conv4_4_3x3(conv4_4_1x1_reduce_bnxx)\n",
        "        conv4_4_3x3_bn = self.conv4_4_3x3_bn(conv4_4_3x3)\n",
        "        conv4_4_3x3_bnxx = self.conv4_4_3x3_relu(conv4_4_3x3_bn)\n",
        "        conv4_4_1x1_increase = self.conv4_4_1x1_increase(conv4_4_3x3_bnxx)\n",
        "        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)\n",
        "        conv4_4 = torch.add(conv4_3x, 1, conv4_4_1x1_increase_bn)\n",
        "        conv4_4x = self.conv4_4_relu(conv4_4)\n",
        "        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(conv4_4x)\n",
        "        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)\n",
        "        conv4_5_1x1_reduce_bnxx = self.conv4_5_1x1_reduce_relu(conv4_5_1x1_reduce_bn)\n",
        "        conv4_5_3x3 = self.conv4_5_3x3(conv4_5_1x1_reduce_bnxx)\n",
        "        conv4_5_3x3_bn = self.conv4_5_3x3_bn(conv4_5_3x3)\n",
        "        conv4_5_3x3_bnxx = self.conv4_5_3x3_relu(conv4_5_3x3_bn)\n",
        "        conv4_5_1x1_increase = self.conv4_5_1x1_increase(conv4_5_3x3_bnxx)\n",
        "        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)\n",
        "        conv4_5 = torch.add(conv4_4x, 1, conv4_5_1x1_increase_bn)\n",
        "        conv4_5x = self.conv4_5_relu(conv4_5)\n",
        "        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(conv4_5x)\n",
        "        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)\n",
        "        conv4_6_1x1_reduce_bnxx = self.conv4_6_1x1_reduce_relu(conv4_6_1x1_reduce_bn)\n",
        "        conv4_6_3x3 = self.conv4_6_3x3(conv4_6_1x1_reduce_bnxx)\n",
        "        conv4_6_3x3_bn = self.conv4_6_3x3_bn(conv4_6_3x3)\n",
        "        conv4_6_3x3_bnxx = self.conv4_6_3x3_relu(conv4_6_3x3_bn)\n",
        "        conv4_6_1x1_increase = self.conv4_6_1x1_increase(conv4_6_3x3_bnxx)\n",
        "        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)\n",
        "        conv4_6 = torch.add(conv4_5x, 1, conv4_6_1x1_increase_bn)\n",
        "        conv4_6x = self.conv4_6_relu(conv4_6)\n",
        "        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(conv4_6x)\n",
        "        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)\n",
        "        conv5_1_1x1_reduce_bnxx = self.conv5_1_1x1_reduce_relu(conv5_1_1x1_reduce_bn)\n",
        "        conv5_1_3x3 = self.conv5_1_3x3(conv5_1_1x1_reduce_bnxx)\n",
        "        conv5_1_3x3_bn = self.conv5_1_3x3_bn(conv5_1_3x3)\n",
        "        conv5_1_3x3_bnxx = self.conv5_1_3x3_relu(conv5_1_3x3_bn)\n",
        "        conv5_1_1x1_increase = self.conv5_1_1x1_increase(conv5_1_3x3_bnxx)\n",
        "        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)\n",
        "        conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)\n",
        "        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)\n",
        "        conv5_1 = torch.add(conv5_1_1x1_proj_bn, 1, conv5_1_1x1_increase_bn)\n",
        "        conv5_1x = self.conv5_1_relu(conv5_1)\n",
        "        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(conv5_1x)\n",
        "        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)\n",
        "        conv5_2_1x1_reduce_bnxx = self.conv5_2_1x1_reduce_relu(conv5_2_1x1_reduce_bn)\n",
        "        conv5_2_3x3 = self.conv5_2_3x3(conv5_2_1x1_reduce_bnxx)\n",
        "        conv5_2_3x3_bn = self.conv5_2_3x3_bn(conv5_2_3x3)\n",
        "        conv5_2_3x3_bnxx = self.conv5_2_3x3_relu(conv5_2_3x3_bn)\n",
        "        conv5_2_1x1_increase = self.conv5_2_1x1_increase(conv5_2_3x3_bnxx)\n",
        "        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)\n",
        "        conv5_2 = torch.add(conv5_1x, 1, conv5_2_1x1_increase_bn)\n",
        "        conv5_2x = self.conv5_2_relu(conv5_2)\n",
        "        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(conv5_2x)\n",
        "        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)\n",
        "        conv5_3_1x1_reduce_bnxx = self.conv5_3_1x1_reduce_relu(conv5_3_1x1_reduce_bn)\n",
        "        conv5_3_3x3 = self.conv5_3_3x3(conv5_3_1x1_reduce_bnxx)\n",
        "        conv5_3_3x3_bn = self.conv5_3_3x3_bn(conv5_3_3x3)\n",
        "        conv5_3_3x3_bnxx = self.conv5_3_3x3_relu(conv5_3_3x3_bn)\n",
        "        conv5_3_1x1_increase = self.conv5_3_1x1_increase(conv5_3_3x3_bnxx)\n",
        "        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)\n",
        "        conv5_3 = torch.add(conv5_2x, 1, conv5_3_1x1_increase_bn)\n",
        "        conv5_3x = self.conv5_3_relu(conv5_3)\n",
        "        pool5_7x7_s1 = self.pool5_7x7_s1(conv5_3x)\n",
        "        classifier_preflatten = self.classifier(pool5_7x7_s1)\n",
        "        classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)\n",
        "        #return classifier, pool5_7x7_s1 　出力を変更しておかないと次元が合わないと言われる\n",
        "        return classifier\n",
        "\n",
        "def resnet50_ft_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Resnet50_ft_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "#モデルのロード\n",
        "model_ft = Resnet50_ft_dag()\n",
        "\n",
        "#最終結合層のリセットと付け替え(全結合層を2つに)\n",
        "model_ft.classifier = nn.Linear(2048, 2)\n",
        "model_ft.classifier = nn.Sequential(*([Flatten()] + list(model_ft.children())[-1:])) #Flattenを挿入\n",
        "\n",
        "#重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_VGGFace2_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "model_pred_prob.append(calculateAccuracy(image_path))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50 15 54 9\n",
            "Accuracy: 0.8125\n",
            "Precision (positive predictive value): 0.847457627118644\n",
            "Recall (sensitivity): 0.7692307692307693\n",
            "Specificity: 0.8571428571428571\n",
            "F_value: 0.8064516129032259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72UkHpANnFjw",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNwSFAOfnEtq",
        "colab_type": "code",
        "outputId": "b40e28d1-f8a7-4a38-c483-73978b583b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "ModelName = 'ResNet50_ImageNet'\n",
        "checkModelName(ModelName, ModelName_list)\n",
        "\n",
        "model_ft = models.resnet50(pretrained=False)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# 重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_ImageNet_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "model_pred_prob.append(calculateAccuracy(image_path))"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47 18 36 27\n",
            "Accuracy: 0.6484375\n",
            "Precision (positive predictive value): 0.6351351351351351\n",
            "Recall (sensitivity): 0.7230769230769231\n",
            "Specificity: 0.5714285714285714\n",
            "F_value: 0.6762589928057553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H64GGa2JmT9D",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_nonPretrained**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVKpRFdAmUDR",
        "colab_type": "code",
        "outputId": "3f5ee770-dde7-45df-f605-9c72e65f0665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "ModelName = 'ResNet50_nonPretrained'\n",
        "checkModelName(ModelName, ModelName_list)\n",
        "\n",
        "model_ft = models.resnet50(pretrained=False)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# 重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_ImageNet_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "model_pred_prob.append(calculateAccuracy(image_path))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47 18 36 27\n",
            "Accuracy: 0.6484375\n",
            "Precision (positive predictive value): 0.6351351351351351\n",
            "Recall (sensitivity): 0.7230769230769231\n",
            "Specificity: 0.5714285714285714\n",
            "F_value: 0.6762589928057553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWhxHmDPns3R",
        "colab_type": "text"
      },
      "source": [
        "#**EfficientNet_b4_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syACduJCntAB",
        "colab_type": "code",
        "outputId": "42ea06b2-1122-4cfe-ed39-f6dd4bddc7df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "ModelName = 'EfficientNet_b4_ImageNet'\n",
        "checkModelName(ModelName, ModelName_list)    \n",
        "\n",
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "model_ft = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "num_ftrs = model_ft._fc.in_features\n",
        "model_ft._fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "#Prediction\n",
        "model_pred_prob.append(calculateAccuracy(image_path))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.5.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.18.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Loaded pretrained weights for efficientnet-b4\n",
            "55 10 52 11\n",
            "Accuracy: 0.8359375\n",
            "Precision (positive predictive value): 0.8333333333333334\n",
            "Recall (sensitivity): 0.8461538461538461\n",
            "Specificity: 0.8253968253968254\n",
            "F_value: 0.8396946564885497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymGrWxHfntKI",
        "colab_type": "text"
      },
      "source": [
        "#**Attention_branch_Network_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gESMORA2ntRo",
        "colab_type": "code",
        "outputId": "4b957e0c-9cc5-4c6a-af98-16a4e83da8b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "ModelName = 'Attention_branch_Network_ImageNet'\n",
        "checkModelName(ModelName, ModelName_list)   \n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "\n",
        "class ResNet_ARN(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet_ARN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], down_size=True)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, down_size=True)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, down_size=True)\n",
        "\n",
        "        self.att_layer4 = self._make_layer(block, 512, layers[3], stride=1, down_size=False)\n",
        "        self.bn_att = nn.BatchNorm2d(512 * block.expansion)\n",
        "        self.att_conv   = nn.Conv2d(512 * block.expansion, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.bn_att2 = nn.BatchNorm2d(num_classes)\n",
        "        self.att_conv2  = nn.Conv2d(num_classes, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.att_conv3  = nn.Conv2d(num_classes, 1, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn_att3 = nn.BatchNorm2d(1)\n",
        "        self.att_gap = nn.AvgPool2d(14)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, down_size=True)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, down_size=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "\n",
        "        if down_size:\n",
        "            self.inplanes = planes * block.expansion\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "        else:\n",
        "            inplanes = planes * block.expansion\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        fe = x\n",
        "\n",
        "        ax = self.bn_att(self.att_layer4(x))\n",
        "        ax = self.relu(self.bn_att2(self.att_conv(ax)))\n",
        "        bs, cs, ys, xs = ax.shape\n",
        "        self.att = self.sigmoid(self.bn_att3(self.att_conv3(ax)))        \n",
        "        # self.att = self.att.view(bs, 1, ys, xs)\n",
        "        ax = self.att_conv2(ax)\n",
        "        ax = self.att_gap(ax)\n",
        "        ax = ax.view(ax.size(0), -1)\n",
        "\n",
        "        rx = x * self.att\n",
        "        rx = rx + x\n",
        "        per = rx\n",
        "        rx = self.layer4(rx)\n",
        "        rx = self.avgpool(rx)\n",
        "        rx = rx.view(rx.size(0), -1)\n",
        "        rx = self.fc(rx)\n",
        "\n",
        "        return ax, rx, [self.att, fe, per]\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "def resnetARN50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_ARN(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "#モデルのロード\n",
        "model_ft = resnetARN50().to(device)\n",
        "\n",
        "#attention branch networkの最終出力を2つにする\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/gravcont_att_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/gravcont_att_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "model_ft.to(device)\n",
        "\n",
        "#Prediction\n",
        "model_pred_prob.append(calculateAccuracy(image_path))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49 16 52 11\n",
            "Accuracy: 0.7890625\n",
            "Precision (positive predictive value): 0.8166666666666667\n",
            "Recall (sensitivity): 0.7538461538461538\n",
            "Specificity: 0.8253968253968254\n",
            "F_value: 0.784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVay7Id8oIPK",
        "colab_type": "text"
      },
      "source": [
        "#**Drawing ROC curve** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q1bF1hpg5Qd",
        "colab_type": "code",
        "outputId": "bd63c8b7-dc1b-4c95-d73d-697dfbcd8d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "print(ModelName_list)\n",
        "print(y_true)\n",
        "print(len(y_true))\n",
        "print(model_pred_prob)\n",
        "len(model_pred_prob[0])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Attention_branch_Network_ImageNet']\n",
            "[1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n",
            "128\n",
            "[[0.9999804496765137, 0.973350465297699, 0.5492003560066223, 0.8715779185295105, 0.448685884475708, 0.9993699193000793, 0.5581653118133545, 0.3279690146446228, 0.8822904825210571, 0.016269803047180176, 0.5420143008232117, 0.02117025852203369, 0.43813782930374146, 0.006657660007476807, 0.006848454475402832, 0.09376770257949829, 0.9323363900184631, 0.04961514472961426, 0.6835908889770508, 0.8928790092468262, 0.5848947167396545, 0.9062340259552002, 0.8197617530822754, 0.8190549612045288, 0.03692948818206787, 0.9743345379829407, 0.7241644263267517, 0.08534902334213257, 0.9674075841903687, 0.061676204204559326, 0.009530961513519287, 0.16312569379806519, 0.8831198811531067, 0.6817405819892883, 0.5565565228462219, 0.48967891931533813, 0.8163159489631653, 0.09485447406768799, 0.15869605541229248, 0.1450669765472412, 0.04912972450256348, 0.18716275691986084, 0.17334693670272827, 0.9585177302360535, 0.9299746155738831, 0.0020933151245117188, 0.11759328842163086, 0.9449374079704285, 0.9981837868690491, 0.8065115213394165, 0.12119263410568237, 0.5283310413360596, 0.913486123085022, 0.844965398311615, 0.6509225368499756, 0.1711687445640564, 0.2416958212852478, 1.0, 0.9999998807907104, 0.10933226346969604, 0.03191089630126953, 0.8133178353309631, 0.568537175655365, 0.9132652282714844, 0.04287010431289673, 0.9013834595680237, 0.9998331069946289, 0.059578657150268555, 0.02027946710586548, 0.22890013456344604, 0.9296479821205139, 0.0519370436668396, 0.5264374017715454, 0.07066148519515991, 0.033984601497650146, 0.19886451959609985, 0.20580220222473145, 0.27207982540130615, 0.9998475313186646, 0.13870441913604736, 0.3164275884628296, 0.07900810241699219, 0.03488600254058838, 0.22335082292556763, 0.5773994326591492, 0.5260176658630371, 0.3239080309867859, 0.5028619766235352, 0.7048995494842529, 0.9238229393959045, 0.8477585315704346, 0.9999985694885254, 0.9867516756057739, 0.10157102346420288, 0.8791587948799133, 0.0284348726272583, 0.999737560749054, 0.020971357822418213, 0.4972720146179199, 0.10639292001724243, 0.7645732164382935, 0.868098795413971, 0.3434271216392517, 0.047308146953582764, 0.951073944568634, 0.7368251085281372, 0.26131898164749146, 0.10411667823791504, 0.8157558441162109, 0.20170456171035767, 0.5408828258514404, 0.016292154788970947, 0.523574709892273, 0.6878512501716614, 0.9984000325202942, 0.033384859561920166, 0.004872679710388184, 0.43840110301971436, 0.991766631603241, 0.03621870279312134, 0.6284942030906677, 0.6723499298095703, 0.8080409169197083, 0.9955253005027771, 0.825607180595398, 0.9997435212135315, 0.9999746084213257, 0.0636715292930603]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_c2zVMKoIVK",
        "colab_type": "code",
        "outputId": "1024563d-7e20-4f17-aaa7-8213fd542dcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_score = []\n",
        "y_true = []\n",
        "\n",
        "k=0\n",
        "for i in label_list:\n",
        "    if label_list[k] == 'cont':\n",
        "          y_true.append(0)\n",
        "    elif label_list[k] == 'grav':\n",
        "          y_true.append(1)\n",
        "    k+=1\n",
        "\n",
        "\n",
        "#健康な状態を「0」、病気を「1」としてラベルよりリストを作成\n",
        "y_true = y_true\n",
        "#それぞれの画像における陽性の確率についてリストを作成\n",
        "#for構文でグラフを上書きしていく予定\n",
        "y_score = model_pred_prob[0]\n",
        "\n",
        "#print(y_true)\n",
        "#print(y_score)\n",
        "\n",
        "fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#print(fpr)\n",
        "#print(tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gom3bMGgSXVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_score = []\n",
        "y_true = []\n",
        "\n",
        "k=0\n",
        "for i in label_list:\n",
        "    if label_list[k] == 'cont':\n",
        "          y_true.append(0)\n",
        "    elif label_list[k] == 'grav':\n",
        "          y_true.append(1)\n",
        "    k+=1\n",
        "\n",
        "\n",
        "#健康な状態を「0」、病気を「1」としてラベルよりリストを作成\n",
        "y_true = y_true\n",
        "#それぞれの画像における陽性の確率についてリストを作成\n",
        "y_score = model_pred_prob\n",
        "\n",
        "#print(y_true)\n",
        "#print(y_score)\n",
        "\n",
        "fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#print(fpr)\n",
        "#print(tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}