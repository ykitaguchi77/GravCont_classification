{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled35.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOmHQIC57tl8XBS9iSlZO1V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/GravCont_classification_colab/blob/master/Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpC0Fk9lUn2",
        "colab_type": "text"
      },
      "source": [
        "#**Assessment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9B59fSXlT5f",
        "colab_type": "code",
        "outputId": "b742c593-3ccf-4de3-e5e4-11d56f379950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.utils.data as data\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "#Advanced Pytorchから\n",
        "import glob\n",
        "import os.path as osp\n",
        "import random\n",
        "import json\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "\n",
        "#サポートパッチのインポート\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set random seem for reproducibility\n",
        "manualSeed = 1234\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "\n",
        "torch.torch.backends.cudnn.benchmark = True\n",
        "torch.torch.backends.cudnn.enabled = True\n",
        "\n",
        "\n",
        "'''\n",
        "grav: 甲状腺眼症\n",
        "cont: コントロール\n",
        "黒の空白を挿入することにより225px*225pxの画像を生成、EfficientNetを用いて転移学習\n",
        "－－－－－－－－－－－－－－\n",
        "データの構造\n",
        "gravcont.zip ------grav\n",
        "               |---cont\n",
        "'''                                     \n",
        "\n",
        "#google driveをcolabolatoryにマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  1234\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHO1RjNom0Pr",
        "colab_type": "text"
      },
      "source": [
        "#**モジュール群**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-yVbbvSm3Ay",
        "colab_type": "code",
        "outputId": "b64d1f4f-6d68-4cdc-9fe8-1eb0b2379512",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# 入力画像の前処理をするクラス\n",
        "# 訓練時と推論時で処理が異なる\n",
        "\n",
        "\"\"\"\n",
        "    画像の前処理クラス。訓練時、検証時で異なる動作をする。\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "    訓練時はRandomResizedCropとRandomHorizontalFlipでデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "\"\"\"\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224, scale=(0.75,1.0)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = '/content/drive/My Drive/Deep_learning/gravcont_seed_1234'\n",
        "n_samples = len(data_dir)\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=20,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "print(class_names)\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_train:\"+str(len(os.listdir(path='/content/drive/My Drive/Deep_learning/gravcont_seed_1234/train/'+class_names[k]))))\n",
        "    k+=1\n",
        "k=0\n",
        "for i in class_names:\n",
        "    print(class_names[k]+\"_val:\"+str(len(os.listdir(path='/content/drive/My Drive/Deep_learning/gravcont_seed_1234/val/'+class_names[k]))))\n",
        "    k+=1\n",
        "\n",
        "print(\"training data set_total：\"+ str(len(image_datasets['train'])))\n",
        "print(\"validating data set_total：\"+str(len(image_datasets['val'])))\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['cont', 'grav']\n",
            "cont_train:256\n",
            "grav_train:252\n",
            "cont_val:65\n",
            "grav_val:63\n",
            "training data set_total：508\n",
            "validating data set_total：128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk-s71bSlLMJ",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_VGGFace2のネットワーク**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twcn29TKk7kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Resnet50_ft_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Resnet50_ft_dag, self).__init__()\n",
        "        self.meta = {'mean': [131.0912, 103.8827, 91.4953],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=[7, 7], stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv1_relu_7x7_s2 = nn.ReLU()\n",
        "        self.pool1_3x3_s2 = nn.MaxPool2d(kernel_size=[3, 3], stride=[2, 2], padding=(0, 0), dilation=1, ceil_mode=True)\n",
        "        self.conv2_1_1x1_reduce = nn.Conv2d(64, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_1_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_1_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_3x3_relu = nn.ReLU()\n",
        "        self.conv2_1_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_1x1_proj = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_1_1x1_proj_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_1_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_2_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_2_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_3x3_relu = nn.ReLU()\n",
        "        self.conv2_2_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_2_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_2_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_reduce = nn.Conv2d(256, 64, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_reduce_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv2_3_3x3 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv2_3_3x3_bn = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_3x3_relu = nn.ReLU()\n",
        "        self.conv2_3_1x1_increase = nn.Conv2d(64, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv2_3_1x1_increase_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv2_3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_reduce = nn.Conv2d(256, 128, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_1_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_1_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_3x3_relu = nn.ReLU()\n",
        "        self.conv3_1_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_1_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_1x1_proj = nn.Conv2d(256, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv3_1_1x1_proj_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_1_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_2_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_2_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_3x3_relu = nn.ReLU()\n",
        "        self.conv3_2_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_2_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_2_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_3_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_3_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_3x3_relu = nn.ReLU()\n",
        "        self.conv3_3_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_3_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_reduce = nn.Conv2d(512, 128, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_reduce_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv3_4_3x3 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv3_4_3x3_bn = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_3x3_relu = nn.ReLU()\n",
        "        self.conv3_4_1x1_increase = nn.Conv2d(128, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv3_4_1x1_increase_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv3_4_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_reduce = nn.Conv2d(512, 256, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_1_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_1_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_3x3_relu = nn.ReLU()\n",
        "        self.conv4_1_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_1_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_1x1_proj = nn.Conv2d(512, 1024, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv4_1_1x1_proj_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_1_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_2_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_2_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_3x3_relu = nn.ReLU()\n",
        "        self.conv4_2_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_2_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_2_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_3_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_3_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_3x3_relu = nn.ReLU()\n",
        "        self.conv4_3_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_3_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_4_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_4_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_3x3_relu = nn.ReLU()\n",
        "        self.conv4_4_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_4_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_4_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_5_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_5_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_3x3_relu = nn.ReLU()\n",
        "        self.conv4_5_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_5_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_5_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_reduce = nn.Conv2d(1024, 256, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_reduce_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv4_6_3x3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv4_6_3x3_bn = nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_3x3_relu = nn.ReLU()\n",
        "        self.conv4_6_1x1_increase = nn.Conv2d(256, 1024, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv4_6_1x1_increase_bn = nn.BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv4_6_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_reduce = nn.Conv2d(1024, 512, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_1_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_1_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_3x3_relu = nn.ReLU()\n",
        "        self.conv5_1_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_1_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_1x1_proj = nn.Conv2d(1024, 2048, kernel_size=[1, 1], stride=(2, 2), bias=False)\n",
        "        self.conv5_1_1x1_proj_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_1_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_2_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_2_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_3x3_relu = nn.ReLU()\n",
        "        self.conv5_2_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_2_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_2_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_reduce = nn.Conv2d(2048, 512, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_reduce_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_1x1_reduce_relu = nn.ReLU()\n",
        "        self.conv5_3_3x3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1), bias=False)\n",
        "        self.conv5_3_3x3_bn = nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_3x3_relu = nn.ReLU()\n",
        "        self.conv5_3_1x1_increase = nn.Conv2d(512, 2048, kernel_size=[1, 1], stride=(1, 1), bias=False)\n",
        "        self.conv5_3_1x1_increase_bn = nn.BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.conv5_3_relu = nn.ReLU()\n",
        "        self.pool5_7x7_s1 = nn.AvgPool2d(kernel_size=[7, 7], stride=[1, 1], padding=0)\n",
        "        self.classifier = nn.Conv2d(2048, 8631, kernel_size=[1, 1], stride=(1, 1))\n",
        "\n",
        "    def forward(self, data):\n",
        "        conv1_7x7_s2 = self.conv1_7x7_s2(data)\n",
        "        conv1_7x7_s2_bn = self.conv1_7x7_s2_bn(conv1_7x7_s2)\n",
        "        conv1_7x7_s2_bnxx = self.conv1_relu_7x7_s2(conv1_7x7_s2_bn)\n",
        "        pool1_3x3_s2 = self.pool1_3x3_s2(conv1_7x7_s2_bnxx)\n",
        "        conv2_1_1x1_reduce = self.conv2_1_1x1_reduce(pool1_3x3_s2)\n",
        "        conv2_1_1x1_reduce_bn = self.conv2_1_1x1_reduce_bn(conv2_1_1x1_reduce)\n",
        "        conv2_1_1x1_reduce_bnxx = self.conv2_1_1x1_reduce_relu(conv2_1_1x1_reduce_bn)\n",
        "        conv2_1_3x3 = self.conv2_1_3x3(conv2_1_1x1_reduce_bnxx)\n",
        "        conv2_1_3x3_bn = self.conv2_1_3x3_bn(conv2_1_3x3)\n",
        "        conv2_1_3x3_bnxx = self.conv2_1_3x3_relu(conv2_1_3x3_bn)\n",
        "        conv2_1_1x1_increase = self.conv2_1_1x1_increase(conv2_1_3x3_bnxx)\n",
        "        conv2_1_1x1_increase_bn = self.conv2_1_1x1_increase_bn(conv2_1_1x1_increase)\n",
        "        conv2_1_1x1_proj = self.conv2_1_1x1_proj(pool1_3x3_s2)\n",
        "        conv2_1_1x1_proj_bn = self.conv2_1_1x1_proj_bn(conv2_1_1x1_proj)\n",
        "        conv2_1 = torch.add(conv2_1_1x1_proj_bn, 1, conv2_1_1x1_increase_bn)\n",
        "        conv2_1x = self.conv2_1_relu(conv2_1)\n",
        "        conv2_2_1x1_reduce = self.conv2_2_1x1_reduce(conv2_1x)\n",
        "        conv2_2_1x1_reduce_bn = self.conv2_2_1x1_reduce_bn(conv2_2_1x1_reduce)\n",
        "        conv2_2_1x1_reduce_bnxx = self.conv2_2_1x1_reduce_relu(conv2_2_1x1_reduce_bn)\n",
        "        conv2_2_3x3 = self.conv2_2_3x3(conv2_2_1x1_reduce_bnxx)\n",
        "        conv2_2_3x3_bn = self.conv2_2_3x3_bn(conv2_2_3x3)\n",
        "        conv2_2_3x3_bnxx = self.conv2_2_3x3_relu(conv2_2_3x3_bn)\n",
        "        conv2_2_1x1_increase = self.conv2_2_1x1_increase(conv2_2_3x3_bnxx)\n",
        "        conv2_2_1x1_increase_bn = self.conv2_2_1x1_increase_bn(conv2_2_1x1_increase)\n",
        "        conv2_2 = torch.add(conv2_1x, 1, conv2_2_1x1_increase_bn)\n",
        "        conv2_2x = self.conv2_2_relu(conv2_2)\n",
        "        conv2_3_1x1_reduce = self.conv2_3_1x1_reduce(conv2_2x)\n",
        "        conv2_3_1x1_reduce_bn = self.conv2_3_1x1_reduce_bn(conv2_3_1x1_reduce)\n",
        "        conv2_3_1x1_reduce_bnxx = self.conv2_3_1x1_reduce_relu(conv2_3_1x1_reduce_bn)\n",
        "        conv2_3_3x3 = self.conv2_3_3x3(conv2_3_1x1_reduce_bnxx)\n",
        "        conv2_3_3x3_bn = self.conv2_3_3x3_bn(conv2_3_3x3)\n",
        "        conv2_3_3x3_bnxx = self.conv2_3_3x3_relu(conv2_3_3x3_bn)\n",
        "        conv2_3_1x1_increase = self.conv2_3_1x1_increase(conv2_3_3x3_bnxx)\n",
        "        conv2_3_1x1_increase_bn = self.conv2_3_1x1_increase_bn(conv2_3_1x1_increase)\n",
        "        conv2_3 = torch.add(conv2_2x, 1, conv2_3_1x1_increase_bn)\n",
        "        conv2_3x = self.conv2_3_relu(conv2_3)\n",
        "        conv3_1_1x1_reduce = self.conv3_1_1x1_reduce(conv2_3x)\n",
        "        conv3_1_1x1_reduce_bn = self.conv3_1_1x1_reduce_bn(conv3_1_1x1_reduce)\n",
        "        conv3_1_1x1_reduce_bnxx = self.conv3_1_1x1_reduce_relu(conv3_1_1x1_reduce_bn)\n",
        "        conv3_1_3x3 = self.conv3_1_3x3(conv3_1_1x1_reduce_bnxx)\n",
        "        conv3_1_3x3_bn = self.conv3_1_3x3_bn(conv3_1_3x3)\n",
        "        conv3_1_3x3_bnxx = self.conv3_1_3x3_relu(conv3_1_3x3_bn)\n",
        "        conv3_1_1x1_increase = self.conv3_1_1x1_increase(conv3_1_3x3_bnxx)\n",
        "        conv3_1_1x1_increase_bn = self.conv3_1_1x1_increase_bn(conv3_1_1x1_increase)\n",
        "        conv3_1_1x1_proj = self.conv3_1_1x1_proj(conv2_3x)\n",
        "        conv3_1_1x1_proj_bn = self.conv3_1_1x1_proj_bn(conv3_1_1x1_proj)\n",
        "        conv3_1 = torch.add(conv3_1_1x1_proj_bn, 1, conv3_1_1x1_increase_bn)\n",
        "        conv3_1x = self.conv3_1_relu(conv3_1)\n",
        "        conv3_2_1x1_reduce = self.conv3_2_1x1_reduce(conv3_1x)\n",
        "        conv3_2_1x1_reduce_bn = self.conv3_2_1x1_reduce_bn(conv3_2_1x1_reduce)\n",
        "        conv3_2_1x1_reduce_bnxx = self.conv3_2_1x1_reduce_relu(conv3_2_1x1_reduce_bn)\n",
        "        conv3_2_3x3 = self.conv3_2_3x3(conv3_2_1x1_reduce_bnxx)\n",
        "        conv3_2_3x3_bn = self.conv3_2_3x3_bn(conv3_2_3x3)\n",
        "        conv3_2_3x3_bnxx = self.conv3_2_3x3_relu(conv3_2_3x3_bn)\n",
        "        conv3_2_1x1_increase = self.conv3_2_1x1_increase(conv3_2_3x3_bnxx)\n",
        "        conv3_2_1x1_increase_bn = self.conv3_2_1x1_increase_bn(conv3_2_1x1_increase)\n",
        "        conv3_2 = torch.add(conv3_1x, 1, conv3_2_1x1_increase_bn)\n",
        "        conv3_2x = self.conv3_2_relu(conv3_2)\n",
        "        conv3_3_1x1_reduce = self.conv3_3_1x1_reduce(conv3_2x)\n",
        "        conv3_3_1x1_reduce_bn = self.conv3_3_1x1_reduce_bn(conv3_3_1x1_reduce)\n",
        "        conv3_3_1x1_reduce_bnxx = self.conv3_3_1x1_reduce_relu(conv3_3_1x1_reduce_bn)\n",
        "        conv3_3_3x3 = self.conv3_3_3x3(conv3_3_1x1_reduce_bnxx)\n",
        "        conv3_3_3x3_bn = self.conv3_3_3x3_bn(conv3_3_3x3)\n",
        "        conv3_3_3x3_bnxx = self.conv3_3_3x3_relu(conv3_3_3x3_bn)\n",
        "        conv3_3_1x1_increase = self.conv3_3_1x1_increase(conv3_3_3x3_bnxx)\n",
        "        conv3_3_1x1_increase_bn = self.conv3_3_1x1_increase_bn(conv3_3_1x1_increase)\n",
        "        conv3_3 = torch.add(conv3_2x, 1, conv3_3_1x1_increase_bn)\n",
        "        conv3_3x = self.conv3_3_relu(conv3_3)\n",
        "        conv3_4_1x1_reduce = self.conv3_4_1x1_reduce(conv3_3x)\n",
        "        conv3_4_1x1_reduce_bn = self.conv3_4_1x1_reduce_bn(conv3_4_1x1_reduce)\n",
        "        conv3_4_1x1_reduce_bnxx = self.conv3_4_1x1_reduce_relu(conv3_4_1x1_reduce_bn)\n",
        "        conv3_4_3x3 = self.conv3_4_3x3(conv3_4_1x1_reduce_bnxx)\n",
        "        conv3_4_3x3_bn = self.conv3_4_3x3_bn(conv3_4_3x3)\n",
        "        conv3_4_3x3_bnxx = self.conv3_4_3x3_relu(conv3_4_3x3_bn)\n",
        "        conv3_4_1x1_increase = self.conv3_4_1x1_increase(conv3_4_3x3_bnxx)\n",
        "        conv3_4_1x1_increase_bn = self.conv3_4_1x1_increase_bn(conv3_4_1x1_increase)\n",
        "        conv3_4 = torch.add(conv3_3x, 1, conv3_4_1x1_increase_bn)\n",
        "        conv3_4x = self.conv3_4_relu(conv3_4)\n",
        "        conv4_1_1x1_reduce = self.conv4_1_1x1_reduce(conv3_4x)\n",
        "        conv4_1_1x1_reduce_bn = self.conv4_1_1x1_reduce_bn(conv4_1_1x1_reduce)\n",
        "        conv4_1_1x1_reduce_bnxx = self.conv4_1_1x1_reduce_relu(conv4_1_1x1_reduce_bn)\n",
        "        conv4_1_3x3 = self.conv4_1_3x3(conv4_1_1x1_reduce_bnxx)\n",
        "        conv4_1_3x3_bn = self.conv4_1_3x3_bn(conv4_1_3x3)\n",
        "        conv4_1_3x3_bnxx = self.conv4_1_3x3_relu(conv4_1_3x3_bn)\n",
        "        conv4_1_1x1_increase = self.conv4_1_1x1_increase(conv4_1_3x3_bnxx)\n",
        "        conv4_1_1x1_increase_bn = self.conv4_1_1x1_increase_bn(conv4_1_1x1_increase)\n",
        "        conv4_1_1x1_proj = self.conv4_1_1x1_proj(conv3_4x)\n",
        "        conv4_1_1x1_proj_bn = self.conv4_1_1x1_proj_bn(conv4_1_1x1_proj)\n",
        "        conv4_1 = torch.add(conv4_1_1x1_proj_bn, 1, conv4_1_1x1_increase_bn)\n",
        "        conv4_1x = self.conv4_1_relu(conv4_1)\n",
        "        conv4_2_1x1_reduce = self.conv4_2_1x1_reduce(conv4_1x)\n",
        "        conv4_2_1x1_reduce_bn = self.conv4_2_1x1_reduce_bn(conv4_2_1x1_reduce)\n",
        "        conv4_2_1x1_reduce_bnxx = self.conv4_2_1x1_reduce_relu(conv4_2_1x1_reduce_bn)\n",
        "        conv4_2_3x3 = self.conv4_2_3x3(conv4_2_1x1_reduce_bnxx)\n",
        "        conv4_2_3x3_bn = self.conv4_2_3x3_bn(conv4_2_3x3)\n",
        "        conv4_2_3x3_bnxx = self.conv4_2_3x3_relu(conv4_2_3x3_bn)\n",
        "        conv4_2_1x1_increase = self.conv4_2_1x1_increase(conv4_2_3x3_bnxx)\n",
        "        conv4_2_1x1_increase_bn = self.conv4_2_1x1_increase_bn(conv4_2_1x1_increase)\n",
        "        conv4_2 = torch.add(conv4_1x, 1, conv4_2_1x1_increase_bn)\n",
        "        conv4_2x = self.conv4_2_relu(conv4_2)\n",
        "        conv4_3_1x1_reduce = self.conv4_3_1x1_reduce(conv4_2x)\n",
        "        conv4_3_1x1_reduce_bn = self.conv4_3_1x1_reduce_bn(conv4_3_1x1_reduce)\n",
        "        conv4_3_1x1_reduce_bnxx = self.conv4_3_1x1_reduce_relu(conv4_3_1x1_reduce_bn)\n",
        "        conv4_3_3x3 = self.conv4_3_3x3(conv4_3_1x1_reduce_bnxx)\n",
        "        conv4_3_3x3_bn = self.conv4_3_3x3_bn(conv4_3_3x3)\n",
        "        conv4_3_3x3_bnxx = self.conv4_3_3x3_relu(conv4_3_3x3_bn)\n",
        "        conv4_3_1x1_increase = self.conv4_3_1x1_increase(conv4_3_3x3_bnxx)\n",
        "        conv4_3_1x1_increase_bn = self.conv4_3_1x1_increase_bn(conv4_3_1x1_increase)\n",
        "        conv4_3 = torch.add(conv4_2x, 1, conv4_3_1x1_increase_bn)\n",
        "        conv4_3x = self.conv4_3_relu(conv4_3)\n",
        "        conv4_4_1x1_reduce = self.conv4_4_1x1_reduce(conv4_3x)\n",
        "        conv4_4_1x1_reduce_bn = self.conv4_4_1x1_reduce_bn(conv4_4_1x1_reduce)\n",
        "        conv4_4_1x1_reduce_bnxx = self.conv4_4_1x1_reduce_relu(conv4_4_1x1_reduce_bn)\n",
        "        conv4_4_3x3 = self.conv4_4_3x3(conv4_4_1x1_reduce_bnxx)\n",
        "        conv4_4_3x3_bn = self.conv4_4_3x3_bn(conv4_4_3x3)\n",
        "        conv4_4_3x3_bnxx = self.conv4_4_3x3_relu(conv4_4_3x3_bn)\n",
        "        conv4_4_1x1_increase = self.conv4_4_1x1_increase(conv4_4_3x3_bnxx)\n",
        "        conv4_4_1x1_increase_bn = self.conv4_4_1x1_increase_bn(conv4_4_1x1_increase)\n",
        "        conv4_4 = torch.add(conv4_3x, 1, conv4_4_1x1_increase_bn)\n",
        "        conv4_4x = self.conv4_4_relu(conv4_4)\n",
        "        conv4_5_1x1_reduce = self.conv4_5_1x1_reduce(conv4_4x)\n",
        "        conv4_5_1x1_reduce_bn = self.conv4_5_1x1_reduce_bn(conv4_5_1x1_reduce)\n",
        "        conv4_5_1x1_reduce_bnxx = self.conv4_5_1x1_reduce_relu(conv4_5_1x1_reduce_bn)\n",
        "        conv4_5_3x3 = self.conv4_5_3x3(conv4_5_1x1_reduce_bnxx)\n",
        "        conv4_5_3x3_bn = self.conv4_5_3x3_bn(conv4_5_3x3)\n",
        "        conv4_5_3x3_bnxx = self.conv4_5_3x3_relu(conv4_5_3x3_bn)\n",
        "        conv4_5_1x1_increase = self.conv4_5_1x1_increase(conv4_5_3x3_bnxx)\n",
        "        conv4_5_1x1_increase_bn = self.conv4_5_1x1_increase_bn(conv4_5_1x1_increase)\n",
        "        conv4_5 = torch.add(conv4_4x, 1, conv4_5_1x1_increase_bn)\n",
        "        conv4_5x = self.conv4_5_relu(conv4_5)\n",
        "        conv4_6_1x1_reduce = self.conv4_6_1x1_reduce(conv4_5x)\n",
        "        conv4_6_1x1_reduce_bn = self.conv4_6_1x1_reduce_bn(conv4_6_1x1_reduce)\n",
        "        conv4_6_1x1_reduce_bnxx = self.conv4_6_1x1_reduce_relu(conv4_6_1x1_reduce_bn)\n",
        "        conv4_6_3x3 = self.conv4_6_3x3(conv4_6_1x1_reduce_bnxx)\n",
        "        conv4_6_3x3_bn = self.conv4_6_3x3_bn(conv4_6_3x3)\n",
        "        conv4_6_3x3_bnxx = self.conv4_6_3x3_relu(conv4_6_3x3_bn)\n",
        "        conv4_6_1x1_increase = self.conv4_6_1x1_increase(conv4_6_3x3_bnxx)\n",
        "        conv4_6_1x1_increase_bn = self.conv4_6_1x1_increase_bn(conv4_6_1x1_increase)\n",
        "        conv4_6 = torch.add(conv4_5x, 1, conv4_6_1x1_increase_bn)\n",
        "        conv4_6x = self.conv4_6_relu(conv4_6)\n",
        "        conv5_1_1x1_reduce = self.conv5_1_1x1_reduce(conv4_6x)\n",
        "        conv5_1_1x1_reduce_bn = self.conv5_1_1x1_reduce_bn(conv5_1_1x1_reduce)\n",
        "        conv5_1_1x1_reduce_bnxx = self.conv5_1_1x1_reduce_relu(conv5_1_1x1_reduce_bn)\n",
        "        conv5_1_3x3 = self.conv5_1_3x3(conv5_1_1x1_reduce_bnxx)\n",
        "        conv5_1_3x3_bn = self.conv5_1_3x3_bn(conv5_1_3x3)\n",
        "        conv5_1_3x3_bnxx = self.conv5_1_3x3_relu(conv5_1_3x3_bn)\n",
        "        conv5_1_1x1_increase = self.conv5_1_1x1_increase(conv5_1_3x3_bnxx)\n",
        "        conv5_1_1x1_increase_bn = self.conv5_1_1x1_increase_bn(conv5_1_1x1_increase)\n",
        "        conv5_1_1x1_proj = self.conv5_1_1x1_proj(conv4_6x)\n",
        "        conv5_1_1x1_proj_bn = self.conv5_1_1x1_proj_bn(conv5_1_1x1_proj)\n",
        "        conv5_1 = torch.add(conv5_1_1x1_proj_bn, 1, conv5_1_1x1_increase_bn)\n",
        "        conv5_1x = self.conv5_1_relu(conv5_1)\n",
        "        conv5_2_1x1_reduce = self.conv5_2_1x1_reduce(conv5_1x)\n",
        "        conv5_2_1x1_reduce_bn = self.conv5_2_1x1_reduce_bn(conv5_2_1x1_reduce)\n",
        "        conv5_2_1x1_reduce_bnxx = self.conv5_2_1x1_reduce_relu(conv5_2_1x1_reduce_bn)\n",
        "        conv5_2_3x3 = self.conv5_2_3x3(conv5_2_1x1_reduce_bnxx)\n",
        "        conv5_2_3x3_bn = self.conv5_2_3x3_bn(conv5_2_3x3)\n",
        "        conv5_2_3x3_bnxx = self.conv5_2_3x3_relu(conv5_2_3x3_bn)\n",
        "        conv5_2_1x1_increase = self.conv5_2_1x1_increase(conv5_2_3x3_bnxx)\n",
        "        conv5_2_1x1_increase_bn = self.conv5_2_1x1_increase_bn(conv5_2_1x1_increase)\n",
        "        conv5_2 = torch.add(conv5_1x, 1, conv5_2_1x1_increase_bn)\n",
        "        conv5_2x = self.conv5_2_relu(conv5_2)\n",
        "        conv5_3_1x1_reduce = self.conv5_3_1x1_reduce(conv5_2x)\n",
        "        conv5_3_1x1_reduce_bn = self.conv5_3_1x1_reduce_bn(conv5_3_1x1_reduce)\n",
        "        conv5_3_1x1_reduce_bnxx = self.conv5_3_1x1_reduce_relu(conv5_3_1x1_reduce_bn)\n",
        "        conv5_3_3x3 = self.conv5_3_3x3(conv5_3_1x1_reduce_bnxx)\n",
        "        conv5_3_3x3_bn = self.conv5_3_3x3_bn(conv5_3_3x3)\n",
        "        conv5_3_3x3_bnxx = self.conv5_3_3x3_relu(conv5_3_3x3_bn)\n",
        "        conv5_3_1x1_increase = self.conv5_3_1x1_increase(conv5_3_3x3_bnxx)\n",
        "        conv5_3_1x1_increase_bn = self.conv5_3_1x1_increase_bn(conv5_3_1x1_increase)\n",
        "        conv5_3 = torch.add(conv5_2x, 1, conv5_3_1x1_increase_bn)\n",
        "        conv5_3x = self.conv5_3_relu(conv5_3)\n",
        "        pool5_7x7_s1 = self.pool5_7x7_s1(conv5_3x)\n",
        "        classifier_preflatten = self.classifier(pool5_7x7_s1)\n",
        "        classifier = classifier_preflatten.view(classifier_preflatten.size(0), -1)\n",
        "        #return classifier, pool5_7x7_s1 　出力を変更しておかないと次元が合わないと言われる\n",
        "        return classifier\n",
        "\n",
        "def resnet50_ft_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Resnet50_ft_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "#モデルのロード\n",
        "model_ft = Resnet50_ft_dag()\n",
        "\n",
        "#最終結合層のリセットと付け替え(全結合層を2つに)\n",
        "model_ft.classifier = nn.Linear(2048, 2)\n",
        "model_ft.classifier = nn.Sequential(*([Flatten()] + list(model_ft.children())[-1:])) #Flattenを挿入\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72UkHpANnFjw",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNwSFAOfnEtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ft = models.resnet50(pretrained=False)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# 重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_ImageNet_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H64GGa2JmT9D",
        "colab_type": "text"
      },
      "source": [
        "#**ResNet50_nonPretrained**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVKpRFdAmUDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_ft = models.resnet50(pretrained=False)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# 重みロード\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_Resnet50_ImageNet_seed_'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWhxHmDPns3R",
        "colab_type": "text"
      },
      "source": [
        "#**EfficientNet_b4_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syACduJCntAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install efficientnet_pytorch\n",
        "from efficientnet_pytorch import EfficientNet \n",
        "\n",
        "model_ft = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "num_ftrs = model_ft._fc.in_features\n",
        "model_ft._fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/GravCont_EfficientNet-b4_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#GPU使用\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymGrWxHfntKI",
        "colab_type": "text"
      },
      "source": [
        "#**Attention_branch_Network_ImageNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gESMORA2ntRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\n",
        "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
        "           'resnet152']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
        "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
        "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
        "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class Flatten(nn.Module): \n",
        "    \"\"\"One layer module that flattens its input.\"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "\n",
        "class ResNet_ARN(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(ResNet_ARN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], down_size=True)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, down_size=True)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, down_size=True)\n",
        "\n",
        "        self.att_layer4 = self._make_layer(block, 512, layers[3], stride=1, down_size=False)\n",
        "        self.bn_att = nn.BatchNorm2d(512 * block.expansion)\n",
        "        self.att_conv   = nn.Conv2d(512 * block.expansion, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.bn_att2 = nn.BatchNorm2d(num_classes)\n",
        "        self.att_conv2  = nn.Conv2d(num_classes, num_classes, kernel_size=1, padding=0,\n",
        "                               bias=False)\n",
        "        self.att_conv3  = nn.Conv2d(num_classes, 1, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn_att3 = nn.BatchNorm2d(1)\n",
        "        self.att_gap = nn.AvgPool2d(14)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, down_size=True)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, down_size=True):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "\n",
        "        if down_size:\n",
        "            self.inplanes = planes * block.expansion\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(self.inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "        else:\n",
        "            inplanes = planes * block.expansion\n",
        "            for i in range(1, blocks):\n",
        "                layers.append(block(inplanes, planes))\n",
        "\n",
        "            return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        fe = x\n",
        "\n",
        "        ax = self.bn_att(self.att_layer4(x))\n",
        "        ax = self.relu(self.bn_att2(self.att_conv(ax)))\n",
        "        bs, cs, ys, xs = ax.shape\n",
        "        self.att = self.sigmoid(self.bn_att3(self.att_conv3(ax)))        \n",
        "        # self.att = self.att.view(bs, 1, ys, xs)\n",
        "        ax = self.att_conv2(ax)\n",
        "        ax = self.att_gap(ax)\n",
        "        ax = ax.view(ax.size(0), -1)\n",
        "\n",
        "        rx = x * self.att\n",
        "        rx = rx + x\n",
        "        per = rx\n",
        "        rx = self.layer4(rx)\n",
        "        rx = self.avgpool(rx)\n",
        "        rx = rx.view(rx.size(0), -1)\n",
        "        rx = self.fc(rx)\n",
        "\n",
        "        return ax, rx, [self.att, fe, per]\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "def resnetARN50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ResNet_ARN(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "#モデルのロード\n",
        "model_ft = resnetARN50().to(device)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "#attention branch networkの最終出力を2つにする\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/gravcont_att_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#ネットワークの読み込み\n",
        "PATH = '/content/drive/My Drive/Deep_learning/gravcont_att_ImageNet_seed'+str(manualSeed)+'.pth'\n",
        "model_ft.load_state_dict(torch.load(PATH))\n",
        "\n",
        "#評価モードにする\n",
        "model_ft.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zLnknjNnqNU",
        "colab_type": "text"
      },
      "source": [
        "#**Calculate Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doQbOyGHpJYU",
        "colab_type": "code",
        "outputId": "7247a568-f932-42a8-dcdc-a52359751907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "#評価モードにする\n",
        "model_ft.eval()\n",
        "\n",
        "##########Calculate Accuracy#############\n",
        "#valフォルダ内のファイル名を取得\n",
        "image_path = glob.glob(\"/content/drive/My Drive/Deep_learning/gravcont_seed_1234/val/*/*\")\n",
        "random.shuffle(image_path)  #表示順をランダムにする\n",
        "print('number of images: ' +str(len(image_path)))\n",
        "#print(image_path) \n",
        "\n",
        "\n",
        "#対象のパスからラベルを抜き出して表示\n",
        "def getlabel(image_path):\n",
        "      image_name = os.path.basename(image_path)\n",
        "      label = os.path.basename(os.path.dirname(image_path))\n",
        "      return(image_name, label)\n",
        "\n",
        "'''\n",
        "#変形後の画像を表示\n",
        "def image_transform(image_path):\n",
        "\n",
        "    image=Image.open(image_path)\n",
        "\n",
        "    \n",
        "    #変形した画像を表示する\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224)])\n",
        "    image_transformed = transform(image)\n",
        "    plt.imshow(np.array(image_transformed))\n",
        "'''\n",
        "\n",
        "#評価のための画像下処理\n",
        "def image_transform(image_path):    \n",
        "    image=Image.open(image_path)\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "    image_tensor = transform(image)\n",
        "\n",
        "    #バッチサイズの次元を先頭に追加した4Dテンソルに変換\n",
        "    image_tensor.unsqueeze_(0)\n",
        "    #print(image_tensor.size())  # torch.Size([1, 3, 224, 224])\n",
        "    image_tensor = image_tensor.to(device) #model_ftをGPUに載せる\n",
        "\n",
        "    return(image_tensor)\n",
        "\n",
        "#モデルにした処理した画像を投入して予測結果を表示\n",
        "def image_eval(image_tensor, label):\n",
        "    output = model_ft(image_tensor)\n",
        "    #print(output.size())  # torch.Size([1, 1000])\n",
        "    #print(output)\n",
        "\n",
        "    #model_pred:クラス名前、prob:確率、pred:クラス番号\n",
        "    prob, pred = torch.topk(nn.Softmax(dim=1)(output), 1)\n",
        "    model_pred = class_names[pred]\n",
        "    \n",
        "    #甲状腺眼症のprobabilityを計算（classが0なら1から減算、classが1ならそのまま）\n",
        "    prob = abs(1-float(prob)-float(pred))\n",
        " \n",
        "    return model_pred, prob, pred\n",
        "\n",
        " \n",
        "\n",
        "def showImage(image_path):\n",
        "    #画像のインポート\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
        "    #画像のリサイズ\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "    resized_img = cv2.resize(img, (int(width*300/height), 300))\n",
        "    cv2_imshow(resized_img)\n",
        "\n",
        "def calculateAccuracy (TP, TN, FP, FN):\n",
        "    accuracy = (TP + TN)/ (TP + TN + FP + FN)\n",
        "    precision  = TP/(FP + TP)\n",
        "    recall = TP/(TP + FN)\n",
        "    specificity = TN/(FP + TN)\n",
        "    f_value = (2*recall*precision)/(recall+precision)\n",
        "    return(accuracy, precision, recall, specificity, f_value)\n",
        "\n",
        "\n",
        "#ここからがメイン\n",
        "TP, FP, TN, FN, TP, FP, TN, FN = [0,0,0,0,0,0,0,0]\n",
        "image_name_list = []\n",
        "label_list = []\n",
        "model_pred_list = []\n",
        "hum_pred_list = []\n",
        "\n",
        "model_pred_class = []\n",
        "model_pred_prob = []\n",
        "\n",
        "\n",
        "for i in image_path:\n",
        "      image_name, label = getlabel(i)  #画像の名前とラベルを取得\n",
        "      image_tensor = image_transform(i)  #予測のための画像下処理\n",
        "      model_pred, prob, pred = image_eval(image_tensor, label)  #予測結果を出力   \n",
        "      #print('Image: '+ image_name)\n",
        "      #print('Label: '+ label)\n",
        "      #print('Pred: '+ model_pred)\n",
        "      #showImage(i)  #画像を表示\n",
        "      #print() #空白行を入れる\n",
        "      time.sleep(0.1)\n",
        "\n",
        "      image_name_list.append(image_name)\n",
        "      label_list.append(label)\n",
        "      model_pred_list.append(model_pred)\n",
        "\n",
        "      model_pred_class.append(int(pred))\n",
        "      model_pred_prob.append(float(prob))\n",
        "\n",
        "      if label == class_names[0]:\n",
        "          if model_pred == class_names[0]:\n",
        "              TP += 1\n",
        "          else:\n",
        "              FN += 1\n",
        "      elif label == class_names[1]:\n",
        "          if model_pred == class_names[1]:\n",
        "              TN += 1\n",
        "          else:\n",
        "              FP += 1\n",
        "      \n",
        "\n",
        "print(TP, FN, TN, FP)\n",
        "\n",
        "#Accuracyを計算\n",
        "accuracy, precision, recall, specificity, f_value = calculateAccuracy (TP, TN, FP, FN)\n",
        "print('Accuracy: ' + str(accuracy))\n",
        "print('Precision (positive predictive value): ' + str(precision))\n",
        "print('Recall (sensitivity): ' + str(recall))\n",
        "print('Specificity: ' + str(specificity))\n",
        "print('F_value: ' + str(f_value))\n",
        "\n",
        "print(model_pred_class)\n",
        "print(model_pred_prob)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of images: 128\n",
            "64 1 62 1\n",
            "Accuracy: 0.984375\n",
            "Precision (positive predictive value): 0.9846153846153847\n",
            "Recall (sensitivity): 0.9846153846153847\n",
            "Specificity: 0.9841269841269841\n",
            "F_value: 0.9846153846153847\n",
            "[1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0]\n",
            "[0.9994055032730103, 0.9743983149528503, 5.328655242919922e-05, 0.9896971583366394, 0.9930518865585327, 0.0017971992492675781, 0.09083443880081177, 0.9989098310470581, 0.00011050701141357422, 0.9985221028327942, 0.0018569231033325195, 0.9964433312416077, 0.23788762092590332, 0.0030853748321533203, 0.9636949300765991, 0.0007186532020568848, 0.00011730194091796875, 0.0008053183555603027, 0.9439658522605896, 0.9967413544654846, 0.00012862682342529297, 0.9672859907150269, 0.007395744323730469, 0.978624165058136, 0.9964074492454529, 0.9997612833976746, 0.9664363861083984, 0.9684000015258789, 0.025732457637786865, 0.008471488952636719, 0.9958904385566711, 0.01186293363571167, 0.9991909861564636, 0.9258318543434143, 5.888938903808594e-05, 0.9968361854553223, 0.006919682025909424, 0.001222074031829834, 0.07390749454498291, 0.9949523210525513, 0.9279975891113281, 0.07172989845275879, 0.9998639822006226, 0.004101216793060303, 0.0018879175186157227, 0.011307060718536377, 0.9993873834609985, 0.9985370635986328, 0.9826700687408447, 0.01854264736175537, 0.9952667951583862, 0.7433353066444397, 0.9998329877853394, 0.0007014274597167969, 0.04974722862243652, 0.011648833751678467, 0.11121690273284912, 0.0013737678527832031, 0.14451462030410767, 0.9977741837501526, 0.9731857180595398, 0.9772285223007202, 0.9999912977218628, 0.0009664297103881836, 0.0008829236030578613, 0.9757615923881531, 0.9240230917930603, 0.8039237856864929, 0.20648419857025146, 0.9993721842765808, 0.9610428810119629, 0.0020543932914733887, 0.9950994849205017, 0.0025396347045898438, 0.0022272467613220215, 0.971190333366394, 0.00011849403381347656, 0.0005677342414855957, 0.0008314847946166992, 0.9974814057350159, 0.9985628724098206, 5.459785461425781e-05, 0.882114052772522, 0.043185293674468994, 0.00270003080368042, 0.6635631322860718, 0.0018569231033325195, 0.13071346282958984, 0.0006551146507263184, 0.00015354156494140625, 0.8743754029273987, 0.996379554271698, 0.0035557150840759277, 0.9802713990211487, 0.000425875186920166, 0.014802157878875732, 0.9999871253967285, 0.0031969547271728516, 0.9999783039093018, 0.06430715322494507, 0.9597386121749878, 0.0009494423866271973, 0.996877908706665, 0.06322872638702393, 0.0003762245178222656, 0.0006138086318969727, 0.9346752762794495, 0.029818177223205566, 0.9869939684867859, 0.001156449317932129, 0.00045114755630493164, 0.9968994855880737, 0.016907989978790283, 0.005568325519561768, 0.984980583190918, 0.999060332775116, 0.9992069602012634, 0.0002474188804626465, 0.001276552677154541, 0.9936843514442444, 0.005109310150146484, 0.9995965361595154, 0.9975067973136902, 0.9999874830245972, 0.005600631237030029, 0.9999779462814331, 0.9948911666870117, 0.02510511875152588]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVay7Id8oIPK",
        "colab_type": "text"
      },
      "source": [
        "#**Drawing ROC curve** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_c2zVMKoIVK",
        "colab_type": "code",
        "outputId": "3e2c118b-df1b-43da-bcae-8216806df129",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_score = []\n",
        "y_true = []\n",
        "\n",
        "k=0\n",
        "for i in label_list:\n",
        "    if label_list[k] == 'cont':\n",
        "          y_true.append(0)\n",
        "    elif label_list[k] == 'grav':\n",
        "          y_true.append(1)\n",
        "    k+=1\n",
        "\n",
        "\n",
        "#健康な状態を「0」、病気を「1」としてラベルよりリストを作成\n",
        "y_true = y_true\n",
        "#それぞれの画像における陽性の確率についてリストを作成\n",
        "y_score = model_pred_prob\n",
        "\n",
        "#print(y_true)\n",
        "#print(y_score)\n",
        "\n",
        "fpr, tpr,thred = roc_curve(y_true, y_score)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#print(fpr)\n",
        "#print(tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic example')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}